import streamlit as st
from dotenv import load_dotenv
from langchain_core.messages import HumanMessage, AIMessage
from RAG.rag_pipeline import get_retriever_from_source
from RAG.chain_builder import get_conversational_rag_chain, get_default_chain
from persona import generate_response_from_persona
from image_generator import generate_images_for_topic, generate_videos_for_topic
from elevenlabs_tts import TTS_ELEVENLABS_TEMPLATES, TTS_POLLY_VOICES
from generate_timed_segments import (
    generate_subtitle_from_script,
    generate_ass_subtitle,
    SUBTITLE_TEMPLATES,
    _auto_split_for_tempo,
    dedupe_adjacent_texts,   # ì“°ì‹œë©´ ìœ ì§€, ì•ˆì“°ë©´ ë¹¼ì…”ë„ ë©ë‹ˆë‹¤
)
from video_maker import (
    create_video_with_segments,
    create_video_from_videos,
    add_subtitles_to_video,
    create_dark_text_video
)
from ssml_converter import convert_lines_to_ssml_batch, breath_linebreaks_batch, koreanize_if_english
from deep_translator import GoogleTranslator
from file_handler import get_documents_from_files
from upload import upload_to_youtube
from best_subtitle_extractor import load_best_subtitles_documents
from text_scraper import get_links, clean_html_parallel, filter_noise
from langchain_core.documents import Document
import os
import requests
import re
import json
import hashlib as _hl
import pandas as pd
from io import BytesIO
import nest_asyncio
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_community.vectorstores import FAISS
import math 
from moviepy import AudioFileClip
nest_asyncio.apply()
load_dotenv()

VIDEO_TEMPLATE = "ì˜ìƒ(ì˜ì–´ë³´ì´ìŠ¤+í•œêµ­ì–´ìë§‰Â·ê°€ìš´ë°)"
# --- BGM ê¸°ë³¸ ê²½ë¡œ(ì‚¬ìš©ì ìš”ì²­: ê³ ì • ì‚¬ìš©) ---
DEFAULT_BGM = "assets/[BGM] í™í•© ë¹„íŠ¸ ì‹ ë‚˜ëŠ” ìŒì•…  ë¬´ë£Œë¸Œê¸ˆ  HYP-Show Me - HYP MUSIC - BGM Design.mp3"

# ---------- ìœ í‹¸ ----------
def _split_script_for_tts(script_text: str) -> list[str]:
    """
    ì „ì²´ ëŒ€ë³¸ì„ LLMì— í•œ ë²ˆ ì „ë‹¬í•´ ë¶„ì ˆëœ ë¼ì¸ ë°°ì—´ì„ ë°›ì•„ì˜¨ë‹¤.
    """
    text = script_text or ""
    lines = breath_linebreaks_batch(text)  # âœ… ë°°ì¹˜ìš© í•¨ìˆ˜ ì‚¬ìš©
    return [ln.strip() for ln in lines if ln.strip()]


# === ê¸°ì¡´ build_ssml_log_file ëŒ€ì²´ ===
def build_ssml_log_file(
    orig_lines: list[str],
    used_ssml_lines: list[str] | None = None,
    used_br_lines: list[str] | None = None,
):
    """
    ì›ë¬¸/SSML/ë¸Œë ˆìŠ¤ 3ì»¬ëŸ¼ ë¡œê·¸ë¥¼ íŒŒì¼ ë°”ì´íŠ¸ë¡œ ìƒì„± (LLM í˜¸ì¶œ ì—†ìŒ).
    - SSML/ë¸Œë ˆìŠ¤ëŠ” ì„¸ì…˜ì— ì €ì¥ëœ ë¼ì¸ë§Œ ì‚¬ìš©
    - ê°’ì´ ì—†ìœ¼ë©´ ë¹ˆ ì¹¸ìœ¼ë¡œ ë‘  (LLM ì¬í˜¸ì¶œ ê¸ˆì§€)
    ë°˜í™˜ê°’: (data_bytes, ext, mime)
    """
    rows = []
    for i, orig in enumerate(orig_lines, start=1):
        ssml = ""
        if used_ssml_lines and i-1 < len(used_ssml_lines):
            ssml = (used_ssml_lines[i-1] or "").strip()

        br = ""
        if used_br_lines and i-1 < len(used_br_lines):
            br = (used_br_lines[i-1] or "").strip()

        rows.append({"No": i, "ì›ë¬¸": orig, "SSML": ssml, "ì¤„ë°”ê¿ˆ": br})

    df = pd.DataFrame(rows, columns=["No", "ì›ë¬¸", "SSML", "ì¤„ë°”ê¿ˆ"])

    # 1) XLSX ì‹œë„ (xlsxwriter ìš°ì„  â†’ openpyxl)
    buf = BytesIO()
    engine = None
    for cand in ("xlsxwriter", "openpyxl"):
        try:
            importlib.import_module(cand)
            engine = cand
            break
        except Exception:
            pass

    if engine:
        with pd.ExcelWriter(buf, engine=engine) as writer:
            df.to_excel(writer, index=False)
        buf.seek(0)
        return buf.getvalue(), "xlsx", "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"

    # 2) í´ë°±: CSV (Excel í˜¸í™˜ ìœ„í•´ UTF-8-SIG)
    sbuf = StringIO()
    df.to_csv(sbuf, index=False)
    csv_bytes = sbuf.getvalue().encode("utf-8-sig")
    return csv_bytes, "csv", "text/csv"

def _log_ssml_preview(line: str, provider: str, voice_template: str, polly_voice_key: str, subtitle_lang: str):
    """
    ë‹¨ì¼ ë¼ì¸ SSML ë¯¸ë¦¬ë³´ê¸° ë¡œê·¸.
    ë°°ì¹˜ í•¨ìˆ˜ê°€ JSON ë°°ì—´ì„ ë°˜í™˜í•˜ë¯€ë¡œ, í•œ ì¤„ë§Œ ë„£ê³  [0]ë²ˆì§¸ ê²°ê³¼ë¥¼ êº¼ë‚¸ë‹¤.
    """
    try:
        ssml_list = convert_lines_to_ssml_batch([koreanize_if_english(line)])
        ssml = ssml_list[0] if ssml_list else ""
        st.write(f"ğŸ§ª [SSML ë¯¸ë¦¬ë³´ê¸°] {line}")
        st.code(ssml, language="xml")
    except Exception as e:
        st.write(f"âš ï¸ SSML ë¯¸ë¦¬ë³´ê¸° ìƒì„± ì‹¤íŒ¨: {e}")


FPS = 30

def _snap_to_fps(t, fps=FPS):
    return max(0.0, round(float(t) * fps) / fps)

def _merge_short_segments(segments, min_dur=0.8):
    """ë¬¸ì¥ êµ¬ê°„ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ ì•/ë’¤ì™€ ë¶™ì—¬ ì „í™˜ ê³¼ë‹¤ ë°©ì§€"""
    if not segments: return segments
    out = [segments[0].copy()]
    for s in segments[1:]:
        prev = out[-1]
        if (s["end"] - s["start"]) < min_dur:
            # ì´ì „ê³¼ ë¶™ì¼ ìˆ˜ ìˆìœ¼ë©´ ë¶™ì„
            if (prev["end"] >= s["start"] - 1e-6):
                prev["end"] = max(prev["end"], s["end"])
                prev["text"] = (prev.get("text","").rstrip()+" "+s.get("text","")).strip()
            else:
                out.append(s.copy())
        else:
            out.append(s.copy())
    return out

def build_sentence_video_segments(sentence_segments, dense_events, audio_path=None, fps=FPS):
    """
    ë¬¸ì¥(=sentence_segments) ê²½ê³„ë¡œë§Œ í™”ë©´ ì „í™˜.
    ê° ë¬¸ì¥ êµ¬ê°„ì˜ start/endëŠ” ê·¸ ë¬¸ì¥ì— ì†í•œ dense_eventsì˜ ì‹œì‘~ë ë²”ìœ„ë¥¼ ë®ë„ë¡ í™•ì¥.
    ë§ˆì§€ë§‰ endëŠ” ì˜¤ë””ì˜¤ ê¸¸ì´ì— ìŠ¤ëƒ…/ì—°ì¥.
    """
    # 1) denseë¥¼ ë¬¸ì¥ë³„ë¡œ íˆ¬ì˜í•´ ë²”ìœ„ ì¡ê¸°
    out = []
    for sent in sentence_segments:
        s0, e0 = float(sent["start"]), float(sent["end"])
        txt = sent.get("text","")
        # ì´ ë¬¸ì¥ì„ ë®ëŠ” dense ë²”ìœ„ë¡œ í™•ì¥
        starts = []
        ends = []
        for d in dense_events:
            if not (d["end"] <= s0 or d["start"] >= e0):  # overlap
                starts.append(d["start"])
                ends.append(d["end"])
        if starts and ends:
            s = min(starts)
            e = max(ends)
        else:
            s, e = s0, e0
        out.append({"start": s, "end": e, "text": txt})

    # 2) ì˜¤ë””ì˜¤ ê¸¸ì´ì— ë§ì¶° ë§ˆì§€ë§‰ ì„¸ê·¸ë¨¼íŠ¸ ì—°ì¥
    if audio_path and os.path.exists(audio_path) and out:
        with AudioFileClip(audio_path) as aud:
            aud_dur = float(aud.duration or 0.0)
        # í”„ë ˆì„ ê²©ìì— ë§ì¶° ì•½ê°„ ì—¬ìœ ë¥¼ ë‘ê³  ì—°ì¥
        tail = _snap_to_fps(aud_dur + 0.02, fps)
        if out[-1]["end"] < tail:
            out[-1]["end"] = tail

    # 3) í”„ë ˆì„ ìŠ¤ëƒ… & ë„ˆë¬´ ì§§ì€ ê²ƒ ë³‘í•©
    for s in out:
        s["start"] = _snap_to_fps(s["start"], fps)
        s["end"]   = _snap_to_fps(s["end"], fps)
        if s["end"] <= s["start"]:
            s["end"] = s["start"] + (1.0/fps)
    out = _merge_short_segments(out, min_dur=0.8)

    # 4) ì¸ì ‘ ê²¹ì¹¨ ì •ë¦¬
    out_sorted = sorted(out, key=lambda x: x["start"])
    for i in range(1, len(out_sorted)):
        out_sorted[i]["start"] = max(out_sorted[i]["start"], out_sorted[i-1]["end"])
        if out_sorted[i]["end"] <= out_sorted[i]["start"]:
            out_sorted[i]["end"] = out_sorted[i]["start"] + (1.0/fps)

    return out_sorted

import hashlib, os

def _fingerprint_video(path: str) -> str:
    """
    ê°™ì€ ì˜ìƒì´ íŒŒì¼ëª…ë§Œ ë‹¬ë¼ ë“¤ì–´ì™€ë„ ì¡ì•„ë‚´ê¸° ìœ„í•œ ê°€ë²¼ìš´ ì§€ë¬¸.
    - íŒŒì¼ëª…(ì¿¼ë¦¬ìŠ¤íŠ¸ë§ ì œì™¸) + ì•ë¶€ë¶„ 512KB MD5 í•´ì‹œ
    - ì½ê¸° ì‹¤íŒ¨ ì‹œ íŒŒì¼ëª…ë§Œ ì‚¬ìš©
    """
    base = os.path.basename(path).lower().split("?")[0]
    try:
        with open(path, "rb") as f:
            head = f.read(512 * 1024)
        return base + ":" + hashlib.md5(head).hexdigest()
    except Exception:
        return base

def dedupe_video_paths_keep_order(paths: list[str]) -> list[str]:
    seen, out = set(), []
    for p in paths:
        key = _fingerprint_video(p)
        if key in seen:
            continue
        seen.add(key)
        out.append(p)
    return out

def _strip_trailing_commas(s: str) -> str:
    return re.sub(r',+\s*$', '', s or '')

def ensure_min_frames(events, fps=30.0, min_frames=2):
    if not events: return events
    tick = 1.0 / float(fps)
    min_dur = tick * max(1, int(min_frames))
    out = []
    for i, e in enumerate(events):
        s = float(e["start"]); ed = float(e["end"])
        if ed - s < min_dur:
            ed = s + min_dur
            if i + 1 < len(events):
                ed = min(ed, float(events[i+1]["start"]) - 0.001)  # ì‚´ì§ ì—¬ìœ 
        out.append({**e, "start": round(s,3), "end": round(ed,3)})
    return out

def drop_or_fix_empty_text(events, merge_if_overlap_or_gap=0.06):
    if not events: return events
    NBSP = "\u00A0"; ASS_NL = r"\N"
    out = []
    for e in events:
        txt = (e.get("text") or "").strip()
        vis = txt.replace(NBSP, "").replace(ASS_NL, "").strip()
        if not vis:
            # ì™„ì „ ë¹ˆ í…ìŠ¤íŠ¸ë§Œ ì œê±°
            continue
        if out and out[-1]["text"] == txt:
            prev_end = float(out[-1]["end"]); cur_start = float(e["start"])
            gap = max(0.0, cur_start - prev_end)
            # ê²¹ì¹˜ê±°ë‚˜ gapì´ ì•„ì£¼ ì§§ì„ ë•Œë§Œ ë³‘í•©
            if gap <= merge_if_overlap_or_gap:
                out[-1]["end"] = max(out[-1]["end"], e["end"])
            else:
                out.append(e)
        else:
            out.append(e)
    return out

def sanitize_ass_text(s: str) -> str:
    """ASSì—ì„œ ë¬¸ì œë  ìˆ˜ ìˆëŠ” ì¤‘ê´„í˜¸ë¥¼ ì´ìŠ¤ì¼€ì´í”„(ìš°ë¦¬ëŠ” override íƒœê·¸ë¥¼ í…ìŠ¤íŠ¸ì— ë„£ì§€ ì•ŠìŒ)."""
    s = (s or "")
    s = s.replace("\\{", "\\{").replace("\\}", "\\}")  # idempotent
    s = s.replace("{", r"\{").replace("}", r"\}")
    return s

def prepare_text_for_ass(text: str, one_line_threshold=12, biline_target=14) -> str:
    if r"\N" in (text or ""):
        return sanitize_ass_text(text)
    t = bind_compounds(text)                 # ê²°í•© í‘œí˜„ ë³´í˜¸
    t = _protect_short_tail_nbsp(t)          # ë§ê¼¬ë¦¬ ë³´í˜¸
    t = lock_oneliner_if_short(t, one_line_threshold)
    t = smart_biline_break(t, biline_target) # í•„ìš”í•œ ê²½ìš°ë§Œ \N ê°•ì œ
    t = sanitize_ass_text(t)
    # ì™„ì „ ê³µë°± ë°©ì§€(ì •ë§ ë¹ˆ ê²½ìš°ëŠ” NBSP í•˜ë‚˜ë¼ë„ ë„£ì–´ í‘œì‹œ ê°•ì œ)
    if not t.strip().replace(NBSP, "").replace(ASS_NL, ""):
        t = NBSP
    return t

ASS_NL = r"\N"

def _visible_len(s: str) -> int:
    # ë˜í•‘ íŒë‹¨ìš© ê¸¸ì´(ê°œëµ). NBSPëŠ” ê³µë°± ì·¨ê¸‰.
    return len((s or "").replace(NBSP, " "))

def lock_oneliner_if_short(text: str, threshold: int = 12) -> str:
    if _visible_len(text) <= threshold:
        return (text or "").replace(" ", NBSP)
    return text

def smart_biline_break(text: str, target: int = 14) -> str:
    raw = (text or "").replace(NBSP, " ")
    if len(raw) <= target * 2:
        return text  # ìë™ ë˜í•‘ì— ë§¡ê¹€

    import re
    candidates = [m.start() for m in re.finditer(r"[ ,Â·/](?!$)", raw)]
    if not candidates:
        # ì¡°ì‚¬ ê²½ê³„
        candidates = [m.end() for m in re.finditer(r"[ì€ëŠ”ì´ê°€ì„ë¥¼ë„ë§Œì˜ì—](?!$)", raw)]

    mid = len(raw) // 2
    pos = None
    if candidates:
        pos = min(candidates, key=lambda i: abs(i - mid))
    else:
        pos = mid

    left = raw[:pos].rstrip()
    right = raw[pos:].lstrip()
    return (left + ASS_NL + right).replace(" ", " ")

NBSP = "\u00A0"

def bind_compounds(
    text: str,
    unit_words=None,        # ìˆ«ì ë’¤ì— ë¶™ëŠ” ë‹¨ìœ„/ì ‘ë¯¸
    counter_words=None,     # ë²ˆ/ìˆ˜/ëª…/ê°œ/ì¹¸/ì°¨ë¡€ ë“± ì¹´ìš´í„°
    bignum_prefixes=None,   # ìˆ˜ì‹­/ìˆ˜ë°±/ìˆ˜ì²œ/ìˆ˜ë§Œ/ìˆ˜ë°±ë§Œ/ìˆ˜ì–µ/ìˆ˜ì¡°...
    user_terms=None         # ì‚¬ìš©ìê°€ ë³´í˜¸í•˜ê³  ì‹¶ì€ êµ¬(ë‚±ë§ ë¬¶ìŒ)
) -> str:
    """
    ë¬¸ì¥ ë‚´ë¶€ì—ì„œ 'ëŠê¸°ë©´ ì–´ìƒ‰í•œ ê²°í•© í‘œí˜„'ì„ ìë™ ê°ì§€í•´ ê³µë°±ì„ NBSPë¡œ ë°”ê¿‰ë‹ˆë‹¤.
    (ì¤„ë°”ê¿ˆ ì•Œê³ ë¦¬ì¦˜ì´ NBSPë¥¼ ë¶„í•  ì§€ì ìœ¼ë¡œ ë³´ì§€ ì•Šì•„ ìì—°ìŠ¤ëŸ¬ìš´ ëŠê¹€ì„ ìœ ë„)

    - ìˆ«ì+ë‹¨ìœ„: 3ìˆ˜, 9ì , 1ë¶„, 30ì´ˆ, 1cm, 1ë§Œ 2ì²œ km, 10ì˜ 120ì œê³±
    - í°ìˆ˜+ë‹¨ìœ„: ìˆ˜ë°±ë§Œ ìˆ˜, ìˆ˜ì²œë§Œ ëª… ...
    - ì´ë¦„+ê°’: í€¸ 9ì , ë£© 5ì , í° 1ì 
    - ì–‘í™”: (ë‹¨ )?í•œ/ë‘/ì„¸/... + ë²ˆ/ìˆ˜/ëª…/ê°œ/ì¹¸/(ì—)
    - ì‚¬ìš©ì ì •ì˜ ìš©ì–´: user_terms=["í•œ ë²ˆì—","ìˆ˜ë°±ë§Œ ìˆ˜"] ë“±
    """
    if not text or text.isspace():
        return text

    unit_words = unit_words or [
        "ìˆ˜","ì ","ë¶„","ì´ˆ","ì¹¸","ë²ˆ","ê°€ì§€","ëª…","ê°œ","ë…„","ë°°","%",
        "km","m","cm","mm","kg","g","mg","â„ƒ","â„‰","Â°"
    ]
    counter_words = counter_words or ["ë²ˆ","ìˆ˜","ê°€ì§€","ëª…","ê°œ","ì¹¸","ì°¨ë¡€"]
    bignum_prefixes = bignum_prefixes or [
        "ìˆ˜ì‹­","ìˆ˜ë°±","ìˆ˜ì²œ","ìˆ˜ë§Œ","ìˆ˜ì‹­ë§Œ","ìˆ˜ë°±ë§Œ","ìˆ˜ì²œë§Œ","ìˆ˜ì–µ","ìˆ˜ì¡°"
    ]
    user_terms = user_terms or []

    t = text

    # 0) ì‚¬ìš©ì ì§€ì • ì–´êµ¬ ë³´í˜¸ (ê·¸ëŒ€ë¡œ ë„£ìœ¼ë©´ ê°€ì¥ ìœ ì—°)
    #    ì˜ˆ: ["í•œ ë²ˆì—", "ìˆ˜ë°±ë§Œ ìˆ˜", "ë‹¨ í•œ ìˆ˜"]
    if user_terms:
        # ê¸´ ì–´êµ¬ë¶€í„° ì¹˜í™˜(ë¶€ë¶„ ì¤‘ë³µ ë°©ì§€)
        for term in sorted(user_terms, key=len, reverse=True):
            safe = term.replace(" ", NBSP)
            # ë‹¨ì–´ ê²½ê³„ ë¬´ì‹œí•˜ê³  ê·¸ëŒ€ë¡œ ì°¾ì•„ ì¹˜í™˜
            t = t.replace(term, safe)

    # 1) 'ì´ë¦„ + ìˆ«ì + (ì |ìˆ˜|ì¹¸|ë¶„|ì´ˆ|%)' íŒ¨í„´ (í€¸ 9ì , ë£© 5ì , í° 1ì )
    #    ì´ë¦„ì€ í•œê¸€/ì˜ë¬¸ ë‹¨ì–´ í•œ ê°œë¡œ ê°€ì •
    name_val = re.compile(
        r"([ê°€-í£A-Za-z]+)\s+(\d+(?:\.\d+)?)\s*(ì |ìˆ˜|ì¹¸|ë¶„|ì´ˆ|%)"
    )
    def _name_val(m):
        return f"{m.group(1)}{NBSP}{m.group(2)}{NBSP}{m.group(3)}"
    t = name_val.sub(_name_val, t)

    # 2) 'ìˆ«ì(ë³µí•©) + ë‹¨ìœ„' íŒ¨í„´ (1ë§Œ 2ì²œ km, 3 ìˆ˜, 30 ì´ˆ, 1 cm ...)
    #    - '1ë§Œ 2ì²œ' ê°™ì´ ë‚´ë¶€ ê³µë°±ë„ NBSPë¡œ
    unit_alt = "|".join(map(re.escape, unit_words))
    num_unit = re.compile(
        rf"((?:\d+(?:\s*[ë§Œì²œë°±ì‹­])?)(?:\s*\d+)*)(?:\s*)({unit_alt})"
    )
    def _num_unit(m):
        left = m.group(1).replace(" ", NBSP)
        return f"{left}{NBSP}{m.group(2)}"
    t = num_unit.sub(_num_unit, t)

    # 3) 'í°ìˆ˜ ì ‘ë‘(ìˆ˜ë°±/ìˆ˜ì²œë§Œ/ìˆ˜ì–µ/ìˆ˜ì¡°...) + ë‹¨ìœ„' (ìˆ˜ë°±ë§Œ ìˆ˜, ìˆ˜ì²œë§Œ ëª…)
    big_alt = "|".join(map(re.escape, bignum_prefixes))
    big_unit = re.compile(rf"({big_alt})\s*({unit_alt})")
    t = big_unit.sub(lambda m: f"{m.group(1)}{NBSP}{m.group(2)}", t)

    # 4) ì§€ìˆ˜ í‘œê¸° '10ì˜ 120ì œê³±'
    expo = re.compile(r"(\d+)\s*ì˜\s*(\d+)\s*ì œê³±")
    t = expo.sub(lambda m: f"{m.group(1)}{NBSP}ì˜{NBSP}{m.group(2)}{NBSP}ì œê³±", t)

    # 5) ì–‘í™” í‘œí˜„ '(ë‹¨ )?í•œ/ë‘/ì„¸/... + ë²ˆ/ìˆ˜/ê°€ì§€/ëª…/ê°œ/ì¹¸ (+ì—)'
    quant_num = "(í•œ|ë‘|ì„¸|ë„¤|ë‹¤ì„¯|ì—¬ì„¯|ì¼ê³±|ì—¬ëŸ|ì•„í™‰|ì—´)"
    counter_alt = "|".join(map(re.escape, counter_words))
    quant = re.compile(rf"(ë‹¨\s+)?{quant_num}\s+({counter_alt})(ì—)?")
    def _quant(m):
        pre = (m.group(1) or "").replace(" ", NBSP)  # "ë‹¨ " -> "ë‹¨&nbsp;"
        core = f"{m.group(2)}{NBSP}{m.group(3)}"     # "í•œ ë²ˆ"
        tail = f"{NBSP}{m.group(4)}" if m.group(4) else ""
        return f"{pre}{core}{tail}"
    t = quant.sub(_quant, t)

    # 6) ê³µë°± ì •ë¦¬(ì´ì¤‘ ì´ìƒ -> ë‹¨ì¼), ë¬¸ë‘/ë¬¸ë¯¸ ê³µë°± ì œê±° (NBSPëŠ” ìœ ì§€)
    t = re.sub(r"[ \t]{2,}", " ", t).strip()
    return t

def build_image_paths_for_dense_segments(segments_for_video, persona_text: str):
    if "seen_photo_ids" not in st.session_state:
        st.session_state.seen_photo_ids = set()
    if "query_page_cursor_img" not in st.session_state:
        st.session_state.query_page_cursor_img = {}

    sentence_units = [s.get('text', '') for s in segments_for_video]
    per_sentence_queries = get_scene_keywords_batch(sentence_units, persona_text)
    for i, q in enumerate(per_sentence_queries, start=1):
        st.write(f"ğŸ§© ì´˜ì´˜ì¡°ê° {i} í‚¤ì›Œë“œ: {q}")

    def _img_search_once(q: str, idx: int, page: int):
        try:
            paths, ids = generate_images_for_topic(
                q, 1,
                start_index=idx,
                page=page,
                exclude_ids=st.session_state.seen_photo_ids,
                return_ids=True
            )
        except TypeError:
            paths = generate_images_for_topic(q, 1, start_index=idx)
            ids = []
        if paths:
            if ids:
                st.session_state.seen_photo_ids.update(ids)
            return _save_unique_image(paths[0], idx)
        return None

    def _normalize_scene_query(raw: str) -> str:
        import re
        if not raw: return ""
        s = raw.strip()
        s = re.sub(r'(?i)^(here are .*?:)\s*', '', s)
        s = re.sub(r'(?i)^(keywords?|í‚¤ì›Œë“œ)\s*:\s*', '', s)
        s = s.replace("\n", " ").replace("\r", " ")
        s = re.sub(r'["â€œâ€â€˜â€™\'`]+', '', s)
        s = re.sub(r'[^A-Za-z\uAC00-\uD7A30-9 ,\-./Â°%]+', ' ', s)
        s = re.sub(r'\s*,\s*', ',', s)
        s = re.sub(r'\s*\.\s*', '.', s)
        s = re.sub(r'\s{2,}', ' ', s).strip(' ,').strip()
        parts = [p.strip() for p in s.split(',') if p.strip()]
        if parts: s = ', '.join(parts[:3])
        return s[:90].rstrip(' ,')

    def _fetch_one_image(q: str, idx: int, page_tries: int = 4):
        base_pg = st.session_state.query_page_cursor_img.get(q, 1)
        for step in range(page_tries):
            pg = base_pg + step
            got = _img_search_once(q, idx, pg)
            if got:
                st.session_state.query_page_cursor_img[q] = pg + 1
                return got
        if "," in q:
            for piece in [p.strip() for p in q.split(",") if p.strip()]:
                base_pg2 = st.session_state.query_page_cursor_img.get(piece, 1)
                for step in range(page_tries):
                    pg = base_pg2 + step
                    got = _img_search_once(piece, idx, pg)
                    if got:
                        st.session_state.query_page_cursor_img[piece] = pg + 1
                        return got
        fb = _normalize_scene_query(q)
        if fb and fb != q:
            base_pg3 = st.session_state.query_page_cursor_img.get(fb, 1)
            for step in range(page_tries):
                pg = base_pg3 + step
                got = _img_search_once(fb, idx, pg)
                if got:
                    st.session_state.query_page_cursor_img[fb] = pg + 1
                    return got
        return None

    image_paths = []
    target_len = len(segments_for_video)
    for idx, q in enumerate(per_sentence_queries, start=1):
        st.write(f"ğŸ–¼ï¸ ì´˜ì´˜ì¡°ê° {idx} ê²€ìƒ‰: {q}")
        path = _fetch_one_image(q, idx, page_tries=4)
        image_paths.append(path)

    if len(image_paths) < target_len:
        last = image_paths[-1] if image_paths else None
        image_paths += [last] * (target_len - len(image_paths))
    elif len(image_paths) > target_len:
        image_paths = image_paths[:target_len]

    st.success(f"ì´ë¯¸ì§€ {sum(1 for p in image_paths if p)}ì¥ í™•ë³´ / ì´ {target_len}ì¡°ê°")
    return image_paths

def enforce_reading_speed_non_merging(events, min_cps=11.0, floor=0.60, ceiling=None, margin=0.02):
    """
    ìë§‰ì„ 'í•©ì¹˜ì§€' ì•Šê³ , ê°€ëŠ¥í•œ ë²”ìœ„ì—ì„œë§Œ endë¥¼ ëŠ˜ë ¤
    - ê¸€ììˆ˜/ì½ê¸°ì†ë„ ê¸°ë°˜ ìµœì†Œ ë…¸ì¶œì‹œê°„ ë³´ì¥
    - ë‹¤ìŒ cueì˜ ì‹œì‘ì€ ì¹¨ë²”í•˜ì§€ ì•ŠìŒ
    """
    if not events:
        return events
    out = []
    for i, e in enumerate(events):
        s  = float(e["start"])
        ed = float(e["end"])
        text = (e.get("text") or "").strip()
        need = max(floor, (len(text) / max(min_cps, 1e-6)) if text else floor)
        target_end = s + need
        # ë‹¤ìŒ cue ì‹œì‘ ì§ì „ê¹Œì§€ë§Œ í™•ì¥
        if i + 1 < len(events):
            next_s = float(events[i+1]["start"])
            ed = min(max(ed, target_end), next_s - margin)
        else:
            ed = max(ed, target_end)
        if ceiling is not None:
            ed = min(ed, s + float(ceiling))
        if ed < s + 0.02:
            ed = s + 0.02
        out.append({**e, "start": round(s, 3), "end": round(ed, 3)})
    return out

def _protect_short_tail_nbsp(text: str) -> str:
    """
    'ë³´ë³‘ ê°™ì£ ?' ê°™ì€ ê¼¬ë¦¬ê°€ ë‹¤ìŒ ì¤„ë¡œ ë–¨ì–´ì§€ì§€ ì•Šë„ë¡,
    ê¼¬ë¦¬ ì• ê³µë°±ì„ NBSPë¡œ ì¹˜í™˜.
    """
    NBSP = "\u00A0"
    # í•œ/ë‘ ë‹¨ì–´ ê¼¬ë¦¬ íŒ¨í„´ë“¤
    TAILS = [
        r"ê°™ì£ \?", r"ê·¸ë ‡ì£ \?", r"ê·¸ì£ \?", r"ê·¸ì£ ",
        r"ì´ì£ \?", r"ì´ì£ ", r"ì£ \?", r"ì£ ",
        r"ì…ë‹ˆë‹¤", r"ì˜ˆìš”", r"ì´ì—ìš”", r"ì´ë‹¤", r"ë‹¤$"
    ]
    pat = re.compile(r"\s+(?=(" + "|".join(TAILS) + r"))")
    return pat.sub(NBSP, (text or "").strip())

def apply_nbsp_tails(events):
    return [{**e, "text": _protect_short_tail_nbsp(e.get("text") or "")} for e in events]

def quantize_events(events, fps=24.0):
    """ìë§‰ ì‹œê°„ì„ ë¹„ë””ì˜¤ í”„ë ˆì„ ê²©ìì— ë§ì¶° ìŠ¤ëƒ…."""
    if not events: return events
    tick = 1.0 / float(fps)
    out, prev_end = [], None
    for e in events:
        s  = round(float(e["start"]) / tick) * tick
        ed = round(float(e["end"])   / tick) * tick
        if prev_end is not None and s < prev_end:
            s = prev_end
        if ed <= s:
            ed = s + tick
        out.append({**e, "start": round(s, 3), "end": round(ed, 3)})
        prev_end = ed
    return out

def clamp_no_overlap(events, margin=0.05):
    """
    ê° cueì˜ endê°€ ë°˜ë“œì‹œ ë‹¤ìŒ cue start - margin ì´í•˜ê°€ ë˜ë„ë¡ í´ë¨í”„.
    ë³‘í•©/í…ìŠ¤íŠ¸ ë³€ê²½ ì—†ìŒ. ì‹œê°„ë§Œ ì¡°ì •.
    """
    if not events: 
        return events
    out = []
    n = len(events)
    for i, e in enumerate(events):
        s = float(e["start"])
        ed = float(e["end"])
        if i + 1 < n:
            next_s = float(events[i+1]["start"])
            ed = min(ed, next_s - margin)  # ë‹¤ìŒ cue ì‹œì‘ë³´ë‹¤ ì¡°ê¸ˆ(=margin) ì¼ì° ëë‚´ê¸°
        # ë„ˆë¬´ ì§§ì•„ì ¸ë„ 20msëŠ” ë³´ì¥
        if ed < s + 0.02:
            ed = s + 0.02
        out.append({**e, "start": round(s, 3), "end": round(ed, 3)})
    # ë‹¨ì¡°ì„± ìµœì¢… ë³´ì •
    for i in range(n - 1):
        if out[i]["end"] > out[i+1]["start"] - margin:
            out[i]["end"] = max(out[i]["start"] + 0.02, out[i+1]["start"] - margin)
    return out

def enforce_min_duration_non_merging(events, min_dur=0.35, margin=0.05):
    """
    cueë¥¼ ë³‘í•©í•˜ì§€ ì•Šê³  'ê°€ëŠ¥í•œ ë²”ìœ„ì—ì„œë§Œ' ê¸¸ì´ë¥¼ ëŠ˜ë¦½ë‹ˆë‹¤.
    ë‹¤ìŒ cueì˜ startë¥¼ ì¹¨ë²”í•˜ì§€ ì•Šë„ë¡ marginì„ ë‚¨ê¸°ê³  í™•ì¥.
    """
    if not events:
        return events
    out = []
    for i, e in enumerate(events):
        s, ed = float(e["start"]), float(e["end"])
        dur = ed - s
        if dur < min_dur:
            target = s + min_dur
            if i + 1 < len(events):
                max_end = float(events[i+1]["start"]) - margin
                ed = min(target, max_end)
            else:
                ed = target
        out.append({**e, "start": round(s, 3), "end": round(ed, 3)})
    # ë§ˆì§€ë§‰ìœ¼ë¡œ ê²¹ì¹¨ ë°©ì§€
    return clamp_no_overlap(out, margin=margin)

def enforce_min_duration(segs, min_dur=0.35):
    out = []
    cur = None
    for s in segs:
        if cur is None:
            cur = dict(s); continue
        if (cur["end"] - cur["start"]) < min_dur:
            cur["end"]  = s["end"]
            cur["text"] = (cur["text"] + " " + s["text"]).strip()
        else:
            out.append(cur); cur = dict(s)
    if cur: out.append(cur)
    if len(out) >= 2 and (out[-1]["end"] - out[-1]["start"]) < min_dur:
        out[-2]["end"]  = out[-1]["end"]
        out[-2]["text"] = (out[-2]["text"] + " " + out[-1]["text"]).strip()
        out.pop()
    return out

def _tokenize_words_for_kr_en(text: str):
    """í•œ/ì˜ í˜¼í•© ë¬¸ì¥ì„ ë‹¨ì–´(ë˜ëŠ” ë©ì–´ë¦¬)+ë¬¸ì¥ë¶€í˜¸ ìˆ˜ì¤€ìœ¼ë¡œ í† í°í™”."""
    import re
    tokens = re.findall(r'[\uAC00-\uD7A3A-Za-z0-9]+|[^\s]', text or "")
    merged = []
    for t in tokens:
        if re.match(r'^[^\uAC00-\uD7A3A-Za-z0-9]+$', t) and merged:
            merged[-1] += t
        else:
            merged.append(t)
    return merged

def densify_subtitles_by_words(segments, target_min_events: int):
    import re
    total_tokens = 0
    per_seg_tokens = []
    for s in segments:
        toks = _tokenize_words_for_kr_en(s['text'])
        per_seg_tokens.append(toks)
        total_tokens += len(toks)

    if total_tokens == 0:
        return segments

    desired_events = max(target_min_events, len(segments))
    chunk_size = max(1, min(6, math.ceil(total_tokens / desired_events)))

    dense = []
    for s, toks in zip(segments, per_seg_tokens):
        if not toks:
            dense.append(s)
            continue
        seg_start, seg_end = s['start'], s['end']
        seg_dur = max(0.01, seg_end - seg_start)
        n_chunks = math.ceil(len(toks) / chunk_size)
        t0 = seg_start
        base_len = max(1, len("".join(toks)))
        extra = {k:v for k,v in s.items() if k not in ('start','end','text')}
        for i in range(n_chunks):
            part = toks[i*chunk_size:(i+1)*chunk_size]
            if not part: 
                continue
            is_kor = bool(re.search(r'[\uAC00-\uD7A3]', "".join(part)))
            text = ' '.join(part).strip()

            # ê³µë°±/ë¬¸ì¥ë¶€í˜¸ ì •ë¦¬ ì¶”ê°€
            text = re.sub(r'\s+([,?.!])', r'\1', text)         # ë¬¸ì¥ë¶€í˜¸ ì• ê³µë°± ì œê±°
            text = re.sub(r'([(\[â€œâ€˜])\s+', r'\1', text)        # ê´„í˜¸/ì¸ìš©ë¶€í˜¸ ë’¤ ê³µë°± ì œê±°
            text = re.sub(r'\s+([)\]â€â€™])', r'\1', text)        # ê´„í˜¸/ì¸ìš©ë¶€í˜¸ ì• ê³µë°± ì œê±°
            part_ratio = len("".join(part)) / base_len
            dur = seg_dur * part_ratio
            t1 = t0 + dur
            if i == n_chunks - 1:
                t1 = seg_end
            dense.append({'start': t0, 'end': t1, 'text': text, **extra})
            t0 = t1
    return dense

def coalesce_segments_for_videos(segments, clip_count: int):
    """
    ì˜ìƒì´ ì ì„ ë•Œ, ì—°ì† ì„¸ê·¸ë¨¼íŠ¸ë¥¼ clip_countê°œ êµ¬ê°„ìœ¼ë¡œ ë³‘í•©í•´
    ê° ì˜ìƒ í´ë¦½ì´ ë§¡ì„ êµ¬ê°„ì„ ë§Œë“¤ì–´ì¤Œ(ìë§‰ì€ ì´˜ì´˜í•œ dense ë²„ì „ìœ¼ë¡œ ë³„ë„ í‘œì‹œ).
    """
    if clip_count <= 0 or not segments:
        return segments
    total_duration = segments[-1]['end']
    target = total_duration / clip_count
    coalesced, cur_start, acc = [], segments[0]['start'], 0.0
    for s in segments:
        acc += (s['end'] - s['start'])
        if acc >= target and len(coalesced) < clip_count - 1:
            coalesced.append({'start': cur_start, 'end': s['end'], 'text': ''})
            cur_start, acc = s['end'], 0.0
    if len(coalesced) < clip_count:
        coalesced.append({'start': cur_start, 'end': segments[-1]['end'], 'text': ''})
    return coalesced

def get_web_documents_from_query(query: str):
    try:
        urls = get_links(query, num=40)
        crawl_results = clean_html_parallel(urls)
        docs = []
        for result in crawl_results:
            if result['success']:
                clean_text = filter_noise(result['text'])
                if len(clean_text.strip()) >= 100:
                    doc = Document(
                        page_content=clean_text.strip(),
                        metadata={"source": result['url']}
                    )
                    docs.append(doc)
        return docs, None
    except Exception as e:
        return [], str(e)


def patch_ass_center(ass_path: str):
    """ASS ìë§‰ì˜ ëª¨ë“  Dialogueì— {\an5}ë¥¼ ë¶™ì—¬ í™”ë©´ ì •ì¤‘ì•™ ì •ë ¬."""
    try:
        with open(ass_path, "r", encoding="utf-8") as f:
            lines = f.readlines()
        out = []
        for ln in lines:
            if ln.startswith("Dialogue:"):
                parts = ln.split(",", 9)
                if len(parts) >= 10 and r"{\an" not in parts[9]:
                    parts[9] = r"{\an5}" + parts[9]
                    ln = ",".join(parts)
            out.append(ln)
        with open(ass_path, "w", encoding="utf-8") as f:
            f.writelines(out)
    except Exception as e:
        print(f"ASS ì¤‘ì•™ ì •ë ¬ íŒ¨ì¹˜ ì‹¤íŒ¨: {e}")

def _normalize_scene_query(raw: str) -> str:
    import re
    if not raw:
        return ""
    s = raw.strip()

    # í”„ë¦¬ì•°ë¸” ì œê±°
    s = re.sub(r'(?i)^(here are .*?:)\s*', '', s)
    s = re.sub(r'(?i)^(keywords?|í‚¤ì›Œë“œ)\s*:\s*', '', s)

    # ì¤„ë°”ê¿ˆ/ë”°ì˜´í‘œ ì œê±°
    s = s.replace("\n", " ").replace("\r", " ")
    s = re.sub(r'["â€œâ€â€˜â€™\'`]+', '', s)

    # âœ… í—ˆìš© ë¬¸ì ë²”ìœ„ ì™„í™”: ì˜ë¬¸ + í•œê¸€ + ìˆ«ì + ê³µë°± + , - . / Â° %
    s = re.sub(r'[^A-Za-z\uAC00-\uD7A30-9 ,\-./Â°%]+', ' ', s)

    # âœ… ì²œë‹¨ìœ„Â·ì†Œìˆ˜ì  ì£¼ë³€ ê³µë°± ì •ë¦¬
    s = re.sub(r'\s*,\s*', ',', s)
    s = re.sub(r'\s*\.\s*', '.', s)

    # ê³µë°± ì •ë¦¬
    s = re.sub(r'\s{2,}', ' ', s).strip(' ,').strip()

    # ì‰¼í‘œë¡œ ìª¼ê°œ ìµœëŒ€ 3ì¡°ê°
    parts = [p.strip() for p in s.split(',') if p.strip()]
    if parts:
        s = ', '.join(parts[:3])

    # ë„ˆë¬´ ê¸¸ë©´ ì»·
    if len(s) > 90:
        s = s[:90].rstrip(' ,')

    return s

def _save_unique_image(src_path_or_url: str, idx: int) -> str:
    """
    ì´ë¯¸ì§€ê°€ ê°™ì€ íŒŒì¼ëª…ìœ¼ë¡œ ë®ì–´ì“°ê¸° ë˜ëŠ” ë¬¸ì œë¥¼ ë§‰ê¸° ìœ„í•´
    ë¬¸ì¥ ì¸ë±ìŠ¤ë³„ë¡œ ê³ ìœ  íŒŒì¼ëª…ìœ¼ë¡œ ì €ì¥/ë³µì‚¬í•©ë‹ˆë‹¤.
    - ë¡œì»¬ ê²½ë¡œë©´ copy
    - URLì´ë©´ ë‹¤ìš´ë¡œë“œ
    """
    import os, shutil, mimetypes
    import requests

    os.makedirs("assets/scene_images", exist_ok=True)

    def _guess_ext(p: str) -> str:
        # í™•ì¥ì ì¶”ì • (ì—†ìœ¼ë©´ .jpg)
        base, ext = os.path.splitext(p)
        if ext and len(ext) <= 5:
            return ext
        # URL/í—¤ë”ì—ì„œ MIMEìœ¼ë¡œ ì¶”ì •
        if p.startswith("http"):
            try:
                head = requests.head(p, timeout=10)
                ctype = head.headers.get("Content-Type", "")
                ext = mimetypes.guess_extension(ctype.split(";")[0].strip()) or ".jpg"
                return ext
            except Exception:
                return ".jpg"
        return ".jpg"

    ext = _guess_ext(src_path_or_url)
    dst = os.path.join("assets/scene_images", f"img_sent_{idx:02d}{ext}")

    try:
        if src_path_or_url.startswith("http"):
            r = requests.get(src_path_or_url, timeout=30)
            r.raise_for_status()
            with open(dst, "wb") as f:
                f.write(r.content)
        else:
            shutil.copyfile(src_path_or_url, dst)
    except Exception:
        # ì‹¤íŒ¨ ì‹œë¼ë„ ìµœì†Œí•œ ì›ë³¸ ê²½ë¡œë¥¼ ë°˜í™˜
        return src_path_or_url

    return dst

def get_scene_keywords_batch(sentence_units, persona_text: str):
    """
    ì—¬ëŸ¬ ë¬¸ì¥ì„ í•œ ë²ˆì— LLMì— ë³´ë‚´ì„œ, ë¬¸ì¥ ìˆ˜ë§Œí¼ í‚¤ì›Œë“œ ë¼ì¸ìœ¼ë¡œ ë°›ì•„ì˜µë‹ˆë‹¤.
    ì¶œë ¥ í˜•ì‹(ì¤‘ìš”): ië²ˆì§¸ ë¬¸ì¥ì€ 'i. keyword' í•œ ì¤„
    """
    scene_chain = get_default_chain(system_prompt="ë‹¹ì‹ ì€ ìˆí¼ ë¹„ì£¼ì–¼ ì¥ë©´ í‚¤ì›Œë“œ ìƒì„± ì „ë¬¸ê°€ì…ë‹ˆë‹¤.")

    numbered = "\n".join(f"{i+1}. {s}" for i, s in enumerate(sentence_units))
    prompt = f"""ë„ˆëŠ” ìˆí¼ ë¹„ë””ì˜¤/ì´ë¯¸ì§€ì˜ 'ì¥ë©´ ê²€ìƒ‰ í‚¤ì›Œë“œ'ë¥¼ ë§Œë“œëŠ” ë„ìš°ë¯¸ë‹¤.

[í˜ë¥´ì†Œë‚˜]
{persona_text}

[ë¬¸ì¥ë“¤]
{numbered}

[ìš”êµ¬]
- ê° ë¬¸ì¥ì— ëŒ€í•´ 1ì¤„ì˜ í‚¤ì›Œë“œë§Œ ìƒì„±
- ië²ˆì§¸ ì¤„ì€ 'i. one short phrase' í˜•ì‹
- ê° í‚¤ì›Œë“œëŠ” 3~6ë‹¨ì–´ì˜ "ì˜ì–´" êµ¬ë¬¸, ë°˜ë“œì‹œ ì˜ì–´ë¡œ 1ê°œë§Œ
- ë°˜ë“œì‹œ í‚¤ì›Œë“œë§Œ, ë¼ë²¨/ì„¤ëª…/ë”°ì˜´í‘œ/ì¤„ë°”ê¿ˆ ì¶”ê°€ ê¸ˆì§€
- ê°™ì€(í˜¹ì€ ê±°ì˜ ê°™ì€) í‚¤ì›Œë“œ/êµ¬ë¥¼ ì—¬ëŸ¬ ì¤„ì— ë°˜ë³µ ì‚¬ìš©í•˜ì§€ ë§ ê²ƒ. ìœ ì‚¬ ê°œë…ì´ë©´ ìŠ¤íƒ€ì¼Â·ì‹œê°„ëŒ€Â·ë¡œì¼€ì´ì…˜ì„ ë°”ê¿” ë³€ì£¼í•  ê²ƒ.

ì‘ë‹µ:
"""

    raw = scene_chain.invoke({"question": prompt, "chat_history": []}).strip()
    lines = [ln.strip() for ln in raw.splitlines() if ln.strip()]

    # ë¬¸ì¥ ìˆ˜ë§Œí¼ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ì¤€ë¹„
    out = [""] * len(sentence_units)
    for ln in lines:
        m = re.match(r"^\s*(\d+)\.\s*(.+)$", ln)
        if not m:
            continue
        idx = int(m.group(1)) - 1
        if 0 <= idx < len(out):
            out[idx] = _normalize_scene_query(m.group(2))

    # ë¹„ì–´ ìˆëŠ” ê±´ ë¬¸ì¥ ì›ë¬¸ì„ ì˜ì–´ë¡œ ë²ˆì—­í•´ì„œ í´ë°±
    for i, val in enumerate(out):
        if not val:
            try:
                t = GoogleTranslator(source='auto', target='en').translate(sentence_units[i])
            except Exception:
                t = sentence_units[i]
            out[i] = _normalize_scene_query(t)

    return out

# ---------- ì•± ê¸°ë³¸ ----------
st.set_page_config(page_title="Perfacto AI", page_icon="ğŸ¤–")
st.title("PerfactoAI")
st.markdown("Make your own vids automatically")


# ---------- ì„¸ì…˜ ----------
def _lock_title():
    st.session_state.title_locked = True


def _use_auto_title():
    st.session_state.title_locked = False
    auto = st.session_state.get("auto_video_title", "")
    if auto:
        st.session_state.video_title = auto


def _init_session():
    defaults = dict(
        messages=[],
        retriever=None,
        system_prompt="ë‹¹ì‹ ì€ ìœ ëŠ¥í•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.",
        last_user_query="",
        video_title="",
        auto_video_title="",
        title_locked=False,
        edited_script_content="",
        selected_tts_provider="ElevenLabs",
        selected_tts_template="educational",
        selected_polly_voice_key="korean_female1",
        selected_subtitle_template="educational",
        bgm_path=None,
        include_voice=True,
        generated_topics=[],
        selected_generated_topic="",
        audio_path=None,
        last_rag_sources=[],
        persona_rag_flags={},
        persona_rag_retrievers={},
        upload_clicked=False,
        youtube_link="",
        video_binary_data=None,
        final_video_path=""
    )
    for k, v in defaults.items():
        if k not in st.session_state:
            st.session_state[k] = v


_init_session()


# ---------- ì‚¬ì´ë“œë°” ----------
with st.sidebar:
    st.header("âš™ï¸ AI í˜ë¥´ì†Œë‚˜ ë° RAG ì„¤ì •")
    if "persona_blocks" not in st.session_state:
        st.session_state.persona_blocks = []

    delete_idx = None

    if st.button("â• í˜ë¥´ì†Œë‚˜ ì¶”ê°€"):
        st.session_state.persona_blocks.append({
            "name": "ìƒˆ í˜ë¥´ì†Œë‚˜",
            "text": "",
            "use_prev_idx": [],
            "result": ""
        })

    for i, block in enumerate(st.session_state.persona_blocks):
        st.markdown(f"---\n### í˜ë¥´ì†Œë‚˜ #{i+1} - `{block['name']}`")

        st.session_state.persona_blocks[i]["name"] = st.text_input(
            "í˜ë¥´ì†Œë‚˜ ì—­í•  ì´ë¦„", value=block["name"], key=f"name_{i}"
        )

        persona_options = [("persona", idx) for idx in range(len(st.session_state.persona_blocks)) if idx != i]
        prev_idxs = st.multiselect(
            "ì´ì „ í˜ë¥´ì†Œë‚˜ ì‘ë‹µ ì´ì–´ë°›ê¸°",
            options=persona_options,
            default=block.get("use_prev_idx", []),
            key=f"use_prev_idx_{i}",
            format_func=lambda x: f"{x[1]+1} - {st.session_state.persona_blocks[x[1]]['name']}"
        )
        st.session_state.persona_blocks[i]["use_prev_idx"] = prev_idxs

        st.session_state.persona_blocks[i]["text"] = st.text_area(
            "ì§€ì‹œ ë¬¸ì¥", value=block["text"], key=f"text_{i}"
        )

        rag_source = st.radio(
            "ğŸ“¡ ì‚¬ìš©í•  RAG ìœ í˜•:",
            options=["ì›¹ ê¸°ë°˜ RAG", "ìœ íŠœë¸Œ ìë§‰ ê¸°ë°˜ RAG"],
            index=None,
            key=f"rag_source_{i}"
        )

        youtube_channel_input = None
        if rag_source == "ìœ íŠœë¸Œ ìë§‰ ê¸°ë°˜ RAG":
            youtube_channel_input = st.text_input(
                "ìœ íŠœë¸Œ ì±„ë„ í•¸ë“¤ ë˜ëŠ” URL ì…ë ¥:",
                value="@ì—­ì‚¬ì´ì•¼ê¸°",
                key=f"youtube_channel_input_{i}"
            )

        if st.button(f"ğŸ§  í˜ë¥´ì†Œë‚˜ ì‹¤í–‰", key=f"run_{i}"):
            prev_blocks = []
            for ptype, pidx in st.session_state.persona_blocks[i].get("use_prev_idx", []):
                if ptype == "persona" and pidx != i:
                    prev_blocks.append(f"[í˜ë¥´ì†Œë‚˜ #{pidx+1}]\n{st.session_state.persona_blocks[pidx]['result']}")
            final_prompt = ("\n\n".join(prev_blocks) + "\n\nì§€ì‹œ:\n" + block["text"]) if prev_blocks else block["text"]

            retriever = None
            if rag_source == "ì›¹ ê¸°ë°˜ RAG":
                docs, error = get_web_documents_from_query(block["text"])
                if not error and docs:
                    retriever = get_retriever_from_source("docs", docs)
                    st.success(f"ğŸ“„ ì›¹ ë¬¸ì„œ {len(docs)}ê±´ ì ìš© ì™„ë£Œ")
                    with st.expander("ğŸ”— ì ìš©ëœ ì›¹ ë¬¸ì„œ ì¶œì²˜ ë³´ê¸°"):
                        for idx, doc in enumerate(docs, start=1):
                            url = doc.metadata.get("source", "ì¶œì²˜ ì—†ìŒ")
                            if url.startswith("http"):
                                st.markdown(f"- [ë¬¸ì„œ {idx}]({url})")
                            else:
                                st.markdown(f"- ë¬¸ì„œ {idx}: {url}")
                else:
                    st.warning(f"ì›¹ ë¬¸ì„œ ìˆ˜ì§‘ ì‹¤íŒ¨: {error or 'ë¬¸ì„œ ì—†ìŒ'}")
            elif rag_source == "ìœ íŠœë¸Œ ìë§‰ ê¸°ë°˜ RAG":
                if youtube_channel_input and youtube_channel_input.strip():
                    subtitle_docs = load_best_subtitles_documents(youtube_channel_input.strip())
                    if subtitle_docs:
                        retriever = get_retriever_from_source("docs", subtitle_docs)
                        st.success(f"ğŸ¬ ìœ íŠœë¸Œ ìë§‰ {len(subtitle_docs)}ê±´ ì ìš© ì™„ë£Œ")
                    else:
                        st.warning("ìœ íŠœë¸Œ ìë§‰ì´ ì—†ìŠµë‹ˆë‹¤.")
                else:
                    st.warning("ìœ íŠœë¸Œ ì±„ë„ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”.")

            if retriever:
                st.session_state.persona_rag_flags[i] = True
                st.session_state.persona_rag_retrievers[i] = retriever
                rag_chain = get_conversational_rag_chain(retriever, st.session_state.system_prompt)
                rag_response = rag_chain.invoke({"input": final_prompt})
                content = rag_response.get("answer", rag_response.get("result", rag_response.get("content", "")))
                source_docs = rag_response.get("source_documents", [])
                sources = []
                for doc in source_docs:
                    snippet = doc.page_content.strip()
                    if len(snippet) > 300:
                        snippet = snippet[:300] + "..."
                    sources.append({"content": snippet, "source": doc.metadata.get("source", "ì¶œì²˜ ì—†ìŒ")})
                st.session_state.messages.append(AIMessage(content=content, additional_kwargs={"sources": sources}))
                st.session_state.persona_blocks[i]["result"] = content
            else:
                st.session_state.persona_rag_flags[i] = False
                result_text = generate_response_from_persona(final_prompt)
                st.session_state.messages.append(AIMessage(content=result_text))
                st.session_state.persona_blocks[i]["result"] = result_text

        if st.button(f"ğŸ—‘ï¸ í˜ë¥´ì†Œë‚˜ ì‚­ì œ", key=f"delete_{i}"):
            delete_idx = i

    if delete_idx is not None:
        del st.session_state.persona_blocks[delete_idx]
        st.rerun()

    st.markdown("---")
    with st.expander("ì˜ìƒ ì œì‘ ì„¤ì •", expanded=True):
        # ì˜ìƒ ìŠ¤íƒ€ì¼
        st.session_state.video_style = st.selectbox(
            "ì˜ìƒ ìŠ¤íƒ€ì¼ ì„ íƒ",
            ["ê¸°ë³¸ ì´ë¯¸ì§€+íƒ€ì´í‹€", "ê°ì„± í…ìŠ¤íŠ¸ ì˜ìƒ", VIDEO_TEMPLATE],
            index=0
        )
        is_emotional = (st.session_state.video_style == "ê°ì„± í…ìŠ¤íŠ¸ ì˜ìƒ")
        is_video_template = (st.session_state.video_style == VIDEO_TEMPLATE)

        st.subheader("ğŸ“œ ì‚¬ìš©í•  ìŠ¤í¬ë¦½íŠ¸ ì„ íƒ")
        available_personas_with_results = [
            (i, block["name"]) for i, block in enumerate(st.session_state.persona_blocks) if block.get("result", "").strip()
        ]

        if available_personas_with_results:
            selected_script_persona_idx = st.selectbox(
                "ìŠ¤í¬ë¦½íŠ¸ë¡œ ì‚¬ìš©í•  í˜ë¥´ì†Œë‚˜ ì„ íƒ:",
                options=available_personas_with_results,
                format_func=lambda x: f"{x[0]+1} - {x[1]}",
                key="selected_script_persona_for_video",
                index=0
            )
            selected_idx = selected_script_persona_idx[0]
            selected_script = st.session_state.persona_blocks[selected_idx]["result"]
            st.session_state.selected_script_persona_index = selected_idx
            
            st.session_state.edited_script_content = st.text_area(
                "ğŸ¬ ìŠ¤í¬ë¦½íŠ¸ ë‚´ìš© ìˆ˜ì •",
                value=selected_script,
                key="script_editor_editable"
            )

            if not is_video_template:
                with st.spinner("ìŠ¤í¬ë¦½íŠ¸ì—ì„œ ì˜ìƒ ì œëª©ì„ ì¶”ì¶œ ì¤‘..."):
                    title_prompt = f"""ë‹¤ìŒ ìŠ¤í¬ë¦½íŠ¸ì— ê¸°ë°˜í•´ ë§¤ë ¥ì ì´ê³  ì„íŒ©íŠ¸ ìˆëŠ” ì§§ì€ í•œêµ­ì–´ ì˜ìƒ ì œëª©ì„ ìƒì„±í•˜ì„¸ìš”. ì œëª©ë§Œ ì‘ë‹µí•˜ì„¸ìš”.

ìŠ¤í¬ë¦½íŠ¸:
{selected_script}

ì œëª©:"""
                    title_llm_chain = get_default_chain(system_prompt="ë‹¹ì‹ ì€ ìˆí¼ ì˜ìƒ ì œëª©ì„ ì§“ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.")
                    title = title_llm_chain.invoke({"question": title_prompt, "chat_history": []}).strip()
                    st.session_state.auto_video_title = title
                    if not st.session_state.get("title_locked", False):
                        st.session_state.video_title = title
        else:
            st.warning("ì‚¬ìš© ê°€ëŠ¥í•œ í˜ë¥´ì†Œë‚˜ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € í˜ë¥´ì†Œë‚˜ ì‹¤í–‰ì„ í†µí•´ ê²°ê³¼ë¥¼ ìƒì„±í•´ ì£¼ì„¸ìš”.")

        # ì œëª© ì…ë ¥ì¹¸: VIDEO_TEMPLATEì—ì„œëŠ” ìˆ¨ê¹€
        if not is_video_template:
            st.session_state.video_title = st.text_input(
                "ì˜ìƒ ì œëª© (ì˜ìƒ ìœ„ì— í‘œì‹œë  ì œëª©)",
                value=st.session_state.video_title,
                key="video_title_input_final",
                on_change=_lock_title
            )
        else:
            st.session_state.video_title = ""  # ì œëª© ì‚¬ìš© ì•ˆ í•¨

        if is_emotional:
            st.info("ê°ì„± í…ìŠ¤íŠ¸ ì˜ìƒì€ **ì´ë¯¸ì§€/ìŒì„± ì—†ì´** í…ìŠ¤íŠ¸ + (ì„ íƒ) BGMìœ¼ë¡œë§Œ ì œì‘ë©ë‹ˆë‹¤.")
            st.session_state.include_voice = False
        else:
            st.session_state.include_voice = st.checkbox("ì˜ìƒì— AI ëª©ì†Œë¦¬ í¬í•¨", value=st.session_state.include_voice)
            if st.session_state.include_voice:
                st.session_state.selected_tts_provider = st.radio(
                    "ìŒì„± ì„œë¹„ìŠ¤ ê³µê¸‰ì ì„ íƒ:",
                    ("ElevenLabs", "Amazon Polly"),
                    index=0 if st.session_state.selected_tts_provider == "ElevenLabs" else 1,
                    key="tts_provider_select"
                )
                if st.session_state.selected_tts_provider == "ElevenLabs":
                    elevenlabs_template_names = list(TTS_ELEVENLABS_TEMPLATES.keys())
                    st.session_state.selected_tts_template = st.selectbox(
                        "ElevenLabs ìŒì„± í…œí”Œë¦¿ ì„ íƒ:",
                        options=elevenlabs_template_names,
                        index=elevenlabs_template_names.index(st.session_state.selected_tts_template)
                        if st.session_state.selected_tts_template in elevenlabs_template_names else 0,
                        key="elevenlabs_template_select"
                    )
                else:
                    polly_voice_keys = list(TTS_POLLY_VOICES.keys())
                    st.session_state.selected_polly_voice_key = st.selectbox(
                        "Amazon Polly ìŒì„± ì„ íƒ:",
                        options=polly_voice_keys,
                        index=polly_voice_keys.index(st.session_state.selected_polly_voice_key)
                        if st.session_state.selected_polly_voice_key in polly_voice_keys else 0,
                        key="polly_voice_select"
                    )

        st.session_state.selected_tts_lang = st.radio(
            "ğŸ™ï¸ ìŒì„± ì–¸ì–´ ì„ íƒ:",
            options=["ko", "en"],
            index=0,  # ê¸°ë³¸ í•œêµ­ì–´
            key="tts_lang_select"
        )
        
        if not is_emotional:
            st.session_state.selected_subtitle_template = st.selectbox(
                "ìë§‰ í…œí”Œë¦¿ ì„ íƒ",
                options=list(SUBTITLE_TEMPLATES.keys()),
                index=list(SUBTITLE_TEMPLATES.keys()).index(st.session_state.selected_subtitle_template)
            )

        uploaded_bgm_file = st.file_uploader("BGM íŒŒì¼ ì—…ë¡œë“œ (ì„ íƒ ì‚¬í•­, .mp3, .wav)", type=["mp3", "wav"])
        if uploaded_bgm_file:
            temp_bgm_path = os.path.join("assets", uploaded_bgm_file.name)
            os.makedirs("assets", exist_ok=True)
            with open(temp_bgm_path, "wb") as f:
                f.write(uploaded_bgm_file.read())
            st.session_state.bgm_path = temp_bgm_path
            st.success(f"ë°°ê²½ ìŒì•… '{uploaded_bgm_file.name}' ì—…ë¡œë“œ ì™„ë£Œ!")

        st.subheader("ì˜ìƒ ì œì‘")
        if st.button("ì˜ìƒ ë§Œë“¤ê¸°"):
            # ì´ˆê¸°í™”
            st.session_state.video_binary_data = None
            st.session_state.final_video_path = ""
            st.session_state.youtube_link = ""
            st.session_state.upload_clicked = False
            # main.py â€” "ì˜ìƒ ë§Œë“¤ê¸°" ë²„íŠ¼ ì•ˆ, ë¬¸ì¥ë³„ ì˜ìƒ ê²€ìƒ‰ ì§ì „ì— ì¶”ê°€
            if "seen_video_ids" not in st.session_state:
                st.session_state.seen_video_ids = set()
            if "query_page_cursor" not in st.session_state:
                st.session_state.query_page_cursor = {}  # {query: next_page_int}
            if "seen_photo_ids" not in st.session_state:
                st.session_state.seen_photo_ids = set()
            if "query_page_cursor_img" not in st.session_state:
                st.session_state.query_page_cursor_img = {}  # {query: next_page_int}
                
            final_script_for_video = st.session_state.edited_script_content
            final_title_for_video = st.session_state.video_title  # VIDEO_TEMPLATEì´ë©´ ë¹ˆ ë¬¸ìì—´ì´ì–´ë„ ë¨

            if not final_script_for_video.strip():
                st.error("ìŠ¤í¬ë¦½íŠ¸ ë‚´ìš©ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                st.stop()

            # ì œëª©ì€ VIDEO_TEMPLATEì¼ ë•Œ í•„ìˆ˜ ì•„ë‹˜
            if (not is_video_template) and (not final_title_for_video.strip()):
                st.error("ì˜ìƒ ì œëª©ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                st.stop()

            with st.spinner("âœ¨ ì˜ìƒ ì œì‘ ì¤‘ì…ë‹ˆë‹¤..."):
                try:
                    media_query_final = ""
                    audio_path = None
                    segments = []
                    ass_path = None

                    # --- ìŒì„± í¬í•¨/ë¯¸í¬í•¨ ë¶„ê¸° ---
                    if not is_emotional and st.session_state.include_voice:
                        # (ì¤‘ìš”) ì´ì¤‘ ìŒì„± ë°©ì§€: ë³„ë„ì˜ ë‹¨ë°œ TTS ìƒì„± ì—†ì´
                        # generate_subtitle_from_script í•œ ë²ˆìœ¼ë¡œ ë¼ì¸ë³„ TTSâ†’ë³‘í•©ê¹Œì§€ ìˆ˜í–‰.
                        audio_output_dir = "assets"
                        os.makedirs(audio_output_dir, exist_ok=True)
                        audio_path = os.path.join(audio_output_dir, "generated_audio.mp3")

                        st.write("ğŸ—£ï¸ ë¼ì¸ë³„ TTS ìƒì„±/ë³‘í•© ë° ì„¸ê·¸ë¨¼íŠ¸ ì‚°ì¶œ ì¤‘...")
                        provider = "elevenlabs" if st.session_state.selected_tts_provider == "ElevenLabs" else "polly"
                        tmpl = st.session_state.selected_tts_template if provider == "elevenlabs" else st.session_state.selected_polly_voice_key
                        
                        script_text = koreanize_if_english(final_script_for_video)
                        sentence_lines = breath_linebreaks(script_text, honor_newlines=False, log=False)
                        script_text_for_tts = "\n".join(sentence_lines)
                        
                        # âœ… í† í° ì—†ì´ ë¡œê·¸ ë§Œë“¤ ìˆ˜ ìˆë„ë¡ ì„¸ì…˜ì— ì €ì¥
                        st.session_state["_orig_lines_for_tts"] = sentence_lines[:]   # ì›ë¬¸ ë¼ì¸(ë¸Œë ˆìŠ¤ ê²°ê³¼)
                        st.session_state["_used_br_lines"]      = sentence_lines[:]   # ë¸Œë ˆìŠ¤ ë¼ì¸ ê·¸ëŒ€ë¡œ
                        
                        segments, audio_clips, ass_path = generate_subtitle_from_script(
                            script_text,
                            ass_path,
                            provider=tts_provider,
                            template=voice_template,
                            polly_voice_key=polly_voice_key,
                            subtitle_lang=subtitle_lang,
                            translate_only_if_english=translate_only_if_english,
                            strip_trailing_punct_last=True
                        )

                        # === SSML ë³€í™˜ 'í›„' (ì‹¤ì‚¬ìš©ë³¸) ===
                        try:
                            # segments ê° ìš”ì†Œì— ssmlì´ ë“¤ì–´ì˜¤ëŠ” êµ¬ì¡°ë©´ ì´ ë¦¬ìŠ¤íŠ¸ê°€ ì±„ì›Œì§‘ë‹ˆë‹¤.
                            used_ssml_lines = [
                                s["ssml"] for s in segments
                                if isinstance(s.get("ssml"), str) and s["ssml"].strip()
                            ]
                        except Exception as e:
                            used_ssml_lines = []
                            print("[SSML] used_ssml_lines build error:", e)
                            
                        st.session_state["_used_ssml_lines"] = used_ssml_lines[:] if used_ssml_lines else []
                        try:
                            if not st.session_state.bgm_path or not os.path.exists(st.session_state.bgm_path):
                                st.session_state.bgm_path = DEFAULT_BGM
                        except Exception:
                            st.session_state.bgm_path = DEFAULT_BGM
                        
                        try:
                            with AudioFileClip(audio_path) as aud:
                                aud_dur = float(aud.duration or 0.0)
                            if segments and aud_dur > 0:
                                # ì•½ê°„ì˜ ì—¬ìœ (20ms) ì¤˜ì„œ -shortest íŠ¸ë¦¼ ë°©ì§€
                                if aud_dur + 0.02 > segments[-1]["end"]:
                                    segments[-1]["end"] = aud_dur + 0.02
                        except Exception as e:
                            print("Audio length check failed:", e)
                        
                        # âœ… ìƒì„± ì§í›„ 'ì§„ì§œë¡œ' ë§Œë“¤ì–´ì¡ŒëŠ”ì§€ ê°•ì œ ê²€ì¦
                        if not segments:
                            st.error("TTS ìƒì„± ì‹¤íŒ¨: ì„¸ê·¸ë¨¼íŠ¸ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. (ë¼ì¸ë³„ ì‹¤íŒ¨ ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”)")
                            st.stop()

                        try:
                            sz = os.path.getsize(audio_path)
                        except Exception:
                            sz = 0
                        if sz < 5_000:  # 5KB ë¯¸ë§Œì´ë©´ ì‚¬ì‹¤ìƒ ì‹¤íŒ¨ë¡œ ê°„ì£¼
                            st.error(f"TTS ìƒì„± ì‹¤íŒ¨: ì˜¤ë””ì˜¤ íŒŒì¼ ìš©ëŸ‰ì´ ë¹„ì •ìƒì ì…ë‹ˆë‹¤ ({sz} bytes).")
                            st.stop()

                        # === dense_events: 2ì°¨ ë¶„ì ˆ ì—†ì´ 'ë¼ì¸ ë‹¨ìœ„' ê·¸ëŒ€ë¡œ ì‚¬ìš© ===
                        line_events = []
                        for i, seg in enumerate(segments):
                            s = float(seg["start"]); e = float(seg["end"])
                            # ğŸ”’ 2ì°¨ ë¶„ì ˆ ê¸ˆì§€: ì ˆëŒ€ \N ì‚½ì…/ë˜í•‘í•˜ì§€ ì•ŠìŒ
                            raw = (seg.get("text") or "").strip()
                            txt = sanitize_ass_text(raw) or "\u00A0"  # ì™„ì „ ë¹ˆ ê²½ìš° NBSP í•˜ë‚˜

                            # pitch ì¶”ì¶œ(ê·¸ëŒ€ë¡œ ìœ ì§€)
                            pitch_val = None
                            ssml = (seg.get("ssml") or "")
                            m = re.search(r'pitch="\s*([+-]?\d+)\s*st"', ssml)
                            if m:
                                try: pitch_val = int(m.group(1))
                                except: pass

                            line_events.append({"start": s, "end": e, "text": txt, "pitch": pitch_val})

                        # ì‹œê°„ ë³´ì •ë§Œ ìˆ˜í–‰(ë¶„ì ˆ/ë³‘í•© ì—†ìŒ)
                        line_events = clamp_no_overlap(line_events, margin=0.02)
                        line_events = enforce_min_duration_non_merging(line_events, min_dur=0.50, margin=0.02)
                        line_events = quantize_events(line_events, fps=30.0)
                        line_events = ensure_min_frames(line_events, fps=30.0, min_frames=2)

                        # ì´í›„ ì½”ë“œ í˜¸í™˜ì„ ìœ„í•´ ì´ë¦„ ìœ ì§€
                        dense_events = line_events  # ìë§‰/SSMLê³¼ ë™ì¼ íƒ€ì„ë¼ì¸
                        
                        with AudioFileClip(audio_path) as aud:
                            audio_dur = float(aud.duration)
                        if dense_events:
                            dense_events[-1]["end"] = max(dense_events[-1]["end"], round(audio_dur, 3))

                        # ë¡œê·¸í™•ì¸
                        st.write("ğŸ§ª ë§ˆì§€ë§‰ 3ê°œ ì¡°ê° ë¯¸ë¦¬ë³´ê¸°:",
                                [(round(e["start"],3), round(e["end"],3), e.get("text","")) for e in dense_events[-3:]])
                        try:
                            with AudioFileClip(audio_path) as aud:
                                st.write(f"ğŸ”Š ì˜¤ë””ì˜¤ ê¸¸ì´: {aud.duration:.3f}s, ìë§‰ ë: {dense_events[-1]['end']:.3f}s")
                        except Exception as ee:
                            st.write("ğŸ”Š ì˜¤ë””ì˜¤ ê¸¸ì´ í™•ì¸ ì‹¤íŒ¨:", ee)
                        
                        # â‘¤ ASS ìƒì„±
                        generate_ass_subtitle(
                            segments=dense_events,
                            ass_path=ass_path,
                            template_name=st.session_state.selected_subtitle_template,
                            strip_trailing_punct_last=True,
                            max_chars_per_line=14,  # â† ë˜í•‘ ë”
                            max_lines=2,           # â† ë˜í•‘ ë”
                            wrap_mode="smart"      # â† í•µì‹¬
                        )
                        segments_for_video = segments

                        # ğŸ”§ ì‹œê°í´ë¦½ ìµœì†Œ ê¸¸ì´ ë³´ì¥(ë³‘í•©). 0.55s ë¯¸ë§Œì´ë©´ ì• ì„¸ê·¸ë¨¼íŠ¸ì— í¡ìˆ˜.
                        def _merge_tiny_visuals(segs, min_dur=0.55):
                            out = []
                            for s in segs:
                                if not out:
                                    out.append(dict(s)); continue
                                dur = float(s["end"]) - float(s["start"])
                                if dur < min_dur:
                                    out[-1]["end"] = max(out[-1]["end"], float(s["end"]))
                                else:
                                    out.append(dict(s))
                            # ë§ˆì§€ë§‰ë„ ë„ˆë¬´ ì§§ìœ¼ë©´ ì§ì „ê³¼ ë³‘í•©
                            if out and (out[-1]["end"] - out[-1]["start"]) < min_dur:
                                if len(out) >= 2:
                                    out[-2]["end"] = out[-1]["end"]
                                    out.pop()
                                else:
                                    out[-1]["end"] = out[-1]["start"] + min_dur
                            return out

                        segments_for_video = _merge_tiny_visuals(segments_for_video, min_dur=0.55)


                        try:
                            if audio_clips is not None:
                                audio_clips.close()
                        except:
                            pass

                        if is_video_template:
                            patch_ass_center(ass_path)
                        st.success(f"ìŒì„±/ìë§‰ ìƒì„± ì™„ë£Œ: {audio_path}, {ass_path}")
                        st.session_state.audio_path = audio_path

                    else:
                        # ìŒì„± ì—†ì´ ì„¸ê·¸ë¨¼íŠ¸ ìƒì„±(í…ìŠ¤íŠ¸ ê¸¸ì´ ê¸°ë°˜)
                        st.write("ğŸ”¤ ìŒì„± ì—†ì´ í…ìŠ¤íŠ¸ ê¸°ë°˜ ì„¸ê·¸ë¨¼íŠ¸ ìƒì„±")
                        sentences = re.split(r'(?<=[.?!])\s*', final_script_for_video.strip())
                        sentences = [s.strip() for s in sentences if s.strip()]
                        if not sentences:
                            sentences = [final_script_for_video.strip()]

                        words_per_minute = 150
                        total_script_words = len(final_script_for_video.split())
                        total_estimated_duration_seconds = max(5, (total_script_words / words_per_minute) * 60)

                        current_time = 0.0
                        total_chars = sum(len(s) for s in sentences)
                        for sentence_text in sentences:
                            min_segment_duration = 1.5
                            if total_chars > 0:
                                proportion = len(sentence_text) / total_chars
                                segment_duration = total_estimated_duration_seconds * proportion
                            else:
                                segment_duration = total_estimated_duration_seconds / len(sentences)
                            segment_duration = max(min_segment_duration, segment_duration)
                            segments.append({"start": current_time, "end": current_time + segment_duration, "text": sentence_text})
                            current_time += segment_duration
                        if segments:
                            segments[-1]["end"] = current_time

                        if not is_emotional:
                            ass_path = os.path.join("assets", "generated_subtitle.ass")
                            st.write("ğŸ“ ìë§‰ íŒŒì¼ ìƒì„± ì¤‘...")
                            generate_ass_subtitle(
                                segments=segments,
                                ass_path=ass_path,
                                template_name=st.session_state.selected_subtitle_template
                            )
                            if is_video_template:
                                patch_ass_center(ass_path)
                            st.success(f"ìë§‰ íŒŒì¼ ìƒì„± ì™„ë£Œ: {ass_path}")
                            # ğŸ”§ ì˜ìƒ í•©ì„±ì—ì„œ ì°¸ì¡°í•  ìµœì¢… ì„¸ê·¸ë¨¼íŠ¸ ì…‹ì—…
                            segments_for_video = [{**e, "text": prepare_text_for_ass(e["text"], one_line_threshold=12, biline_target=14)} for e in segments]
                            segments_for_video = dedupe_adjacent_texts(segments_for_video)
                            segments_for_video = drop_or_fix_empty_text(segments_for_video)
                            segments_for_video = clamp_no_overlap(segments_for_video, margin=0.02)
                            segments_for_video = enforce_min_duration_non_merging(segments_for_video, min_dur=0.50, margin=0.02)
                            segments_for_video = quantize_events(segments_for_video, fps=30.0)
                            segments_for_video = ensure_min_frames(segments_for_video, fps=30.0, min_frames=2)

                    # --- ë¯¸ë””ì–´(ì´ë¯¸ì§€ or ì˜ìƒ) ìˆ˜ì§‘ ---
                    image_paths, video_paths = [], []
                    if st.session_state.video_style != "ê°ì„± í…ìŠ¤íŠ¸ ì˜ìƒ":
                        if is_video_template:
                            # âœ… ë¬¸ì¥ ë‹¨ìœ„(segments)ë¡œ ë¬¸ì¥ë³„ í‚¤ì›Œë“œ ìƒì„± â†’ ì˜ìƒ 1ê°œì”© ë§¤ì¹­
                            st.write("ğŸ¯ ë¬¸ì¥ë³„ë¡œ í˜ë¥´ì†Œë‚˜ ê¸°ë°˜ í‚¤ì›Œë“œë¥¼ ë§Œë“¤ì–´ ê°œë³„ ì˜ìƒ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.")

                            # 1) ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸
                            sentence_units = [s.get("text", "") for s in segments_for_video]
                            
                            # ì„ íƒëœ ìŠ¤í¬ë¦½íŠ¸ í˜ë¥´ì†Œë‚˜ì˜ ì§€ì‹œë¬¸ì„ persona_textë¡œ ì•ˆì „ ì·¨ë“
                            try:
                                pidx = st.session_state.get("selected_script_persona_index", None)
                                persona_text = st.session_state.persona_blocks[pidx]["text"] if pidx is not None else ""
                            except Exception:
                                persona_text = ""

                            # 3) âœ… ë¬¸ì¥ë³„ í‚¤ì›Œë“œë¥¼ í•œ ë²ˆì— ë°›ê¸° (ë°°ì¹˜)
                            per_sentence_queries = get_scene_keywords_batch(sentence_units, persona_text)
                            for i, q in enumerate(per_sentence_queries, start=1):
                                st.write(f"ğŸ§© ë¬¸ì¥ {i} í‚¤ì›Œë“œ(ì •ê·œí™”): {q}")

                            # 4) ë¬¸ì¥ë³„ë¡œ ì˜ìƒ 1ê°œì”© ê²€ìƒ‰
                            video_paths = []

                            def _try_search_once(q: str, clip_idx: int):
                                # í‚¤ì›Œë“œë³„ ë‹¤ìŒ í˜ì´ì§€ ì»¤ì„œ (ê¸°ë³¸ 1)
                                pg = st.session_state.query_page_cursor.get(q, 1)
                                paths, ids = generate_videos_for_topic(
                                    query=q,
                                    num_videos=1,
                                    start_index=clip_idx,           # íŒŒì¼ëª… ì¼ê´€ì„± ìœ ì§€
                                    orientation="portrait",
                                    page=pg,                        # âœ… ì´ í‚¤ì›Œë“œëŠ” ì—¬ê¸°ì„œë¶€í„°
                                    exclude_ids=st.session_state.seen_video_ids,  # âœ… ì´ë¯¸ ì“´ ê±´ ê±´ë„ˆë›°ê¸°
                                    return_ids=True
                                )
                                if paths:
                                    # ì„±ê³µ â†’ ë‹¤ìŒì— ê°™ì€ í‚¤ì›Œë“œ ì“°ë©´ ë‹¤ìŒ í˜ì´ì§€ë¶€í„°
                                    st.session_state.query_page_cursor[q] = pg + 1
                                    st.session_state.seen_video_ids.update(ids)
                                return paths

                            for clip_idx, q in enumerate(per_sentence_queries, start=1):
                                st.write(f"ğŸï¸ ë¬¸ì¥ {clip_idx} ê²€ìƒ‰: {q}")

                                got = _try_search_once(q, clip_idx)

                                # ì½¤ë§ˆë¡œ ë‚˜ë‰œ êµ¬ë¬¸ì´ë©´ ì¡°ê°ë³„ë¡œë„ ì¬ì‹œë„
                                if not got and ("," in q):
                                    for piece in [p.strip() for p in q.split(",") if p.strip()]:
                                        got = _try_search_once(piece, clip_idx)
                                        if got: break

                                # ê·¸ë˜ë„ ì—†ìœ¼ë©´ í‚¤ì›Œë“œ ì •ê·œí™” í›„ í•œ ë²ˆ ë”
                                if not got:
                                    fb = _normalize_scene_query(q)
                                    got = _try_search_once(fb, clip_idx)

                                if got:
                                    video_paths.extend(got)

                            # 5) ê¸¸ì´ ë³´ì •
                            # ğŸ” ì¤‘ë³µ ì œê±°(ê²½ë¡œ/ë‚´ìš© ê¸°ë°˜)
                            before = len(video_paths)
                            seen_fp = set()
                            unique_paths = []
                            for p in video_paths:
                                fp = _fingerprint_video(p)
                                if fp in seen_fp:
                                    continue
                                seen_fp.add(fp)
                                unique_paths.append(p)
                            video_paths = unique_paths
                            after = len(video_paths)
                            st.write(f"ğŸ” ì˜ìƒ ì¤‘ë³µ ì œê±°: {before} â†’ {after}")

                            # âœ… ë¶€ì¡±ë¶„ì€ ìƒˆë¡œ ê²€ìƒ‰í•´ì„œ ì±„ìš°ê¸°(ë¼ìš´ë“œë¡œë¹ˆ)
                            need = len(segments_for_video) - len(video_paths)
                            if need > 0:
                                st.info(f"ğŸ” ì¤‘ë³µ ì œê±°ë¡œ {need}ê°œ ë¶€ì¡± â†’ ê³ ìœ  ì˜ìƒ ì¬ê²€ìƒ‰ ì‹œì‘")
                                added = 0
                                max_passes = 8  # ê° ì¿¼ë¦¬ë¥¼ ì—¬ëŸ¬ ë°”í€´ ë„ëŠ” ìƒí•œ (ë„ˆë¬´ ê¸¸ë©´ ëŠ˜ë¦¬ì„¸ìš”)

                                # ì´ë¯¸ í™•ë³´í•œ ê³ ìœ ì§€ë¬¸ì€ ê³„ì† ìœ ì§€
                                seen_fp = {_fingerprint_video(p) for p in video_paths}

                                # per_sentence_queries ëŠ” ë°”ë¡œ ìœ„ì—ì„œ ë§Œë“  ë¬¸ì¥ë³„ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                                for pass_no in range(max_passes):
                                    for clip_idx, q in enumerate(per_sentence_queries, start=1):
                                        if added >= need:
                                            break

                                        # ê²€ìƒ‰ ì „ëµ: ì›ë¬¸ â†’ ì½¤ë§ˆë¶„í•  ì¡°ê° â†’ ì •ê·œí™” í‚¤ì›Œë“œ
                                        candidates = [q]
                                        if "," in q:
                                            candidates += [piece.strip() for piece in q.split(",") if piece.strip()]
                                        norm = _normalize_scene_query(q)
                                        if norm and norm not in candidates:
                                            candidates.append(norm)

                                        found_unique = False
                                        for cand in candidates:
                                            got = _try_search_once(cand, len(video_paths) + added + 1)
                                            if not got:
                                                continue
                                            pth = got[0]
                                            fp  = _fingerprint_video(pth)
                                            if fp in seen_fp:
                                                # ì¤‘ë³µì´ë©´ ê°™ì€ candë¡œ ë‹¤ìŒ í˜ì´ì§€ ê³„ì†(ë‹¤ìŒ ë¼ìš´ë“œì—ì„œ pgê°€ ì¦ê°€ë¨)
                                                continue
                                            # ìƒˆ ê³ ìœ  í´ë¦½ íšë“
                                            video_paths.append(pth)
                                            seen_fp.add(fp)
                                            added += 1
                                            st.write(f"â• ë³´ê°• {added}/{need}: {cand} â†’ {os.path.basename(pth)}")
                                            found_unique = True
                                            break  # candidates ë£¨í”„ íƒˆì¶œ

                                        if not found_unique:
                                            # candë“¤ë¡œ ëª»ì°¾ì•˜ìœ¼ë©´ ë‹¤ìŒ pass ë•Œ ê°™ì€ ì¿¼ë¦¬ì˜ ë‹¤ìŒ í˜ì´ì§€ë¥¼ ìë™ íƒìƒ‰
                                            pass

                                    if added >= need:
                                        break

                                # ê·¸ë˜ë„ ëª¨ìë¼ë©´ ë§ˆì§€ë§‰ìœ¼ë¡œ íŒ¨ë”©
                                if added < need:
                                    st.warning(f"ë³´ê°• ê²€ìƒ‰ í›„ì—ë„ {need - added}ê°œ ë¶€ì¡± â†’ ë§ˆì§€ë§‰ í´ë¦½ìœ¼ë¡œ íŒ¨ë”©")
                                    if video_paths:
                                        video_paths += [video_paths[-1]] * (need - added)
                        else:
                            # --- ì´ë¯¸ì§€ ìˆ˜ì§‘(ë¬¸ì¥ë‹¹ 1ì¥, ë¶€ì¡± ì‹œ ì¶”ê°€ íƒìƒ‰) ---
                            st.write("ğŸ–¼ï¸ ë¬¸ì¥ë³„ë¡œ í˜ë¥´ì†Œë‚˜ ê¸°ë°˜ í‚¤ì›Œë“œë¥¼ ë§Œë“¤ì–´ ì´ë¯¸ì§€ 1ì¥ì”© ìƒì„±/ê²€ìƒ‰í•©ë‹ˆë‹¤.") 

                            persona_text = ""
                            try:
                                pidx = st.session_state.get("selected_script_persona_index", None)
                                if pidx is not None:
                                    persona_text = st.session_state.persona_blocks[pidx]["text"]
                            except Exception:
                                pass

                            image_paths = build_image_paths_for_dense_segments(segments_for_video, persona_text)


                    # --- í•©ì„± ---
                    DEFAULT_BGM = "assets/[BGM] í™í•© ë¹„íŠ¸ ì‹ ë‚˜ëŠ” ìŒì•…  ë¬´ë£Œë¸Œê¸ˆ  HYP-Show Me - HYP MUSIC - BGM Design.mp3"
                    bgm_path = st.session_state.bgm_path
                    if not (bgm_path and os.path.exists(bgm_path)):
                        if os.path.exists(DEFAULT_BGM):
                            bgm_path = DEFAULT_BGM
                    # ì„¸ì´í”„ê°€ë“œ: ì„¸ì…˜ì—ë„ ìµœì¢…ê°’ ë°˜ì˜
                    st.session_state.bgm_path = bgm_path

                    st.write("ğŸ§ ìµœì¢… BGM ê²½ë¡œ:", bgm_path, "exists:", os.path.exists(bgm_path))
                    
                    video_output_dir = "assets"
                    os.makedirs(video_output_dir, exist_ok=True)
                    temp_video_path = os.path.join(video_output_dir, "temp_video.mp4")
                    final_video_path = os.path.join(video_output_dir, "final_video_with_subs.mp4")

                    st.write("ğŸ¬ ë¹„ë””ì˜¤ í•©ì„± ì¤‘...")
                    if is_emotional:
                        created_video_path = create_dark_text_video(
                            script_text=final_script_for_video,
                            title_text="",  # ê°ì„± í…ìŠ¤íŠ¸: í™”ë©´ ì œëª© ë¹„ì‚¬ìš©
                            audio_path=None,
                            bgm_path=bgm_path,
                            save_path=temp_video_path
                        )
                        final_video_with_subs_path = created_video_path
                    else:
                        # âœ… ìœ„ 'ë¯¸ë””ì–´ ìˆ˜ì§‘'ì—ì„œ ì´ë¯¸ video_pathsë¥¼ í™•ë³´í•¨
                        N = len(segments_for_video)
                        include_voice = bool(st.session_state.get("include_voice", True))
                        bgm_path = st.session_state.get("bgm_path") or "assets/[BGM] í™í•© ë¹„íŠ¸ ì‹ ë‚˜ëŠ” ìŒì•…  ë¬´ë£Œë¸Œê¸ˆ  HYP-Show Me - HYP MUSIC - BGM Design.mp3"
                        temp_video_path = os.path.join("assets", "video_from_videos.mp4")

                        # (ì„ íƒ) ê°œìˆ˜/ì •í•©ì„± ë¡œê·¸
                        st.write(f"âœ… TTS ë¼ì¸ ìˆ˜: {len(segments)} / ì „í™˜ êµ¬ê°„ ìˆ˜: {len(segments_for_video)} / í™•ë³´í•œ ì˜ìƒ ìˆ˜: {len(video_paths)}")


                        if is_video_template:
                            created_video_path = create_video_from_videos(
                                video_paths=video_paths[:N],
                                segments=segments_for_video,
                                audio_path=(audio_path if include_voice else None),
                                topic_title="",                 # ìƒë‹¨ íƒ€ì´í‹€ ë¯¸ì‚¬ìš©
                                include_topic_title=False,
                                bgm_path=bgm_path,
                                save_path=temp_video_path
                            )
                        else:
                            created_video_path = create_video_with_segments(
                                image_paths=image_paths,
                                segments=segments_for_video,
                                audio_path=st.session_state.audio_path if st.session_state.include_voice else None,
                                topic_title=st.session_state.video_title,
                                include_topic_title=True,
                                bgm_path=bgm_path,
                                save_path=temp_video_path
                            )

                        # ìë§‰ ì˜¤ë²„ë ˆì´
                        st.write("ğŸ“ ìë§‰ ì…íˆëŠ” ì¤‘...")
                        final_video_with_subs_path = add_subtitles_to_video(
                            input_video_path=created_video_path,
                            ass_path=ass_path,
                            output_path=final_video_path
                        )

                    st.success(f"âœ… ìµœì¢… ì˜ìƒ ìƒì„± ì™„ë£Œ: {final_video_with_subs_path}")
                    st.session_state["final_video_path"] = final_video_with_subs_path

                except Exception as e:
                    st.error(f"âŒ ì˜ìƒ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
                    st.exception(e)

    st.divider()
    # ---------- ë‹¤ìš´ë¡œë“œ & ì—…ë¡œë“œ ----------
    with st.expander("ğŸ“¤ ë‹¤ìš´ë¡œë“œ ë° ì—…ë¡œë“œ", expanded=True):
        final_path = st.session_state.get("final_video_path", "")
        if final_path and os.path.exists(final_path):
            st.video(final_path)
            data_for_download = st.session_state.get("video_binary_data", None)
            if data_for_download is None:
                try:
                    with open(final_path, "rb") as f:
                        data_for_download = f.read()
                    st.session_state.video_binary_data = data_for_download
                except Exception as e:
                    st.error(f"ì˜ìƒ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}")
                    data_for_download = b""
            # --- SSML ë¡œê·¸ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ (ì—‘ì…€ ë˜ëŠ” CSV) ---
            orig_lines = st.session_state.get("_orig_lines_for_tts") or []
            used_lines = st.session_state.get("_used_ssml_lines") or []
            used_br    = st.session_state.get("_used_br_lines") or orig_lines  # ë¸Œë ˆìŠ¤ ì—†ìœ¼ë©´ ì›ë¬¸ ë¼ì¸ìœ¼ë¡œ

            if orig_lines:
                script_hash = _hl.md5("\n".join(orig_lines).encode("utf-8")).hexdigest()[:8]
                # âœ… ì´ì œ LLM í˜¸ì¶œ ì—†ì´ ì„¸ì…˜ ê°’ë§Œ ì‚¬ìš©
                data_bytes, ext, mime = build_ssml_log_file(orig_lines, used_lines, used_br)
                st.download_button(
                    label=f"ğŸ§¾ SSML ë¡œê·¸ ë‹¤ìš´ë¡œë“œ ({ext.upper()})",
                    data=data_bytes,
                    file_name=f"ssml_log_{script_hash}.{ext}",
                    mime=mime,
                    key=f"download_ssml_log_{script_hash}"
                )
                if ext == "csv":
                    st.caption("â€» openpyxl/xlsxwriter ë¯¸ì„¤ì¹˜ë¡œ CSVë¡œ ì œê³µë©ë‹ˆë‹¤. Excelì—ì„œ ë°”ë¡œ ì—´ ìˆ˜ ìˆì–´ìš”.")
            st.download_button(
                label="ğŸ¬ ì˜ìƒ ë‹¤ìš´ë¡œë“œ",
                data=data_for_download,
                file_name="generated_multimodal_video.mp4",
                mime="video/mp4"
            )
            if not st.session_state.upload_clicked:
                if st.button("YouTubeì— ìë™ ì—…ë¡œë“œ"):
                    try:
                        youtube_link = upload_to_youtube(
                            final_path,
                            title=st.session_state.get("video_title") or "AI ìë™ ìƒì„± ì˜ìƒ"  # ê¸°ë³¸ê°’
                        )
                        st.session_state.upload_clicked = True
                        st.session_state.youtube_link = youtube_link
                        st.success("âœ… YouTube ì—…ë¡œë“œ ì™„ë£Œ!")
                        st.markdown(f"[ğŸ“º ì˜ìƒ ë³´ëŸ¬ê°€ê¸°]({youtube_link})")
                    except Exception as e:
                        st.error(f"âŒ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
            else:
                st.success("âœ… YouTube ì—…ë¡œë“œ ì™„ë£Œë¨")
                st.markdown(f"[ğŸ“º ì˜ìƒ ë³´ëŸ¬ê°€ê¸°]({st.session_state.youtube_link})")
        else:
            st.info("ğŸ“Œ ë¨¼ì € 'ì˜ìƒ ë§Œë“¤ê¸°'ë¥¼ ì‹¤í–‰í•´ ì£¼ì„¸ìš”.")

    if st.button("ëŒ€í™” ì´ˆê¸°í™”"):
        st.session_state.clear()
        st.rerun()


# ---------- ë©”ì¸ ì±„íŒ… ----------
for message in st.session_state.messages:
    with st.chat_message(message.type):
        st.markdown(message.content)
        if message.type == "ai" and hasattr(message, "additional_kwargs") and "sources" in message.additional_kwargs and message.additional_kwargs["sources"]:
            st.subheader("ğŸ“š ì°¸ê³  ë¬¸ë‹¨ (RAG ê¸°ë°˜)")
            for idx, source_item in enumerate(message.additional_kwargs["sources"], start=1):
                content_display = source_item["content"]
                source_url_display = source_item.get("source", "N/A")
                if len(content_display) > 200:
                    content_display = content_display[:200] + "..."
                if source_url_display != 'N/A':
                    st.markdown(f"**ì¶œì²˜ {idx}:** [{source_url_display}]({source_url_display})\n> {content_display}")
                else:
                    st.markdown(f"**ì¶œì²˜ {idx}:**\n> {content_display}")

if user_input := st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš” (ì˜ˆ: ìµœê·¼ AI ê¸°ìˆ  íŠ¸ë Œë“œ ì•Œë ¤ì¤˜)"):
    st.session_state.messages.append(HumanMessage(content=user_input))
    with st.chat_message("human"):
        st.markdown(user_input)
    st.session_state.last_user_query = user_input

    with st.chat_message("ai"):
        container = st.empty()
        ai_answer = ""
        sources_list = []

        if st.session_state.retriever:
            rag_chain = get_conversational_rag_chain(st.session_state.retriever, st.session_state.system_prompt)
            rag_response = rag_chain.invoke({"input": user_input})
            ai_answer = rag_response.get("answer", rag_response.get("result", rag_response.get("content", "")))
            source_docs = rag_response.get("source_documents", [])
            sources_list = []
            for doc in source_docs:
                sources_list.append({"content": doc.page_content[:200], "source": doc.metadata.get("source", "ì¶œì²˜ ì—†ìŒ")})
            container.markdown(ai_answer)
            st.session_state.messages.append(AIMessage(content=ai_answer, additional_kwargs={"sources": sources_list}))
        else:
            chain = get_default_chain(st.session_state.system_prompt)
            for token in chain.stream({"question": user_input, "chat_history": st.session_state.messages}):
                ai_answer += token
                container.markdown(ai_answer)

        if sources_list:
            st.write("### ğŸ“š ì°¸ê³  ë¬¸ë‹¨ (RAG ê¸°ë°˜)")
            for idx, source_item in enumerate(sources_list, start=1):
                content_display = source_item["content"]
                source_url_display = source_item.get("source", "N/A")
                if len(content_display) > 200:
                    content_display = content_display[:200] + "..."
                if source_url_display != 'N/A':
                    st.markdown(f"**ì¶œì²˜ {idx}:** [{source_url_display}]({source_url_display})\n> {content_display}")
                else:
                    st.markdown(f"**ì¶œì²˜ {idx}:**\n> {content_display}")

    # ì±„íŒ… ë‹µë³€ì„ ê³§ë°”ë¡œ ìŠ¤í¬ë¦½íŠ¸/ì£¼ì œë¡œ ë°˜ì˜
    st.session_state.edited_script_content = ai_answer
    if st.session_state.video_style != VIDEO_TEMPLATE:
        with st.spinner("ë‹µë³€ì—ì„œ ì˜ìƒ ì œëª©ì„ ìë™ ì¶”ì¶œ ì¤‘..."):
            title_extraction_prompt = f"""ë‹¹ì‹ ì€ TikTok, YouTube Shorts, Instagram Reelsìš© **ë§¤ë ¥ì ì´ê³  ë°”ì´ëŸ´ì„± ìˆëŠ” ìˆí¼ ë¹„ë””ì˜¤ ì œëª©**ì„ ì‘ì„±í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ë‹¤ìŒ ìŠ¤í¬ë¦½íŠ¸ì—ì„œ **ìµœëŒ€ 5ë‹¨ì–´ ì´ë‚´**ì˜ ê°•ë ¬í•œ í•œêµ­ì–´ ì œëª©ë§Œ ìƒì„±í•˜ì„¸ìš”.

ìŠ¤í¬ë¦½íŠ¸:
{ai_answer}

ì˜ìƒ ì œëª©:"""
            title_llm_chain = get_default_chain(
                system_prompt="ë‹¹ì‹ ì€ ìˆí¼ ë¹„ë””ì˜¤ìš© ë§¤ìš° ì§§ê³  ê°•ë ¬í•œ í•œêµ­ì–´ ì œëª©ì„ ìƒì„±í•˜ëŠ” ì „ë¬¸ AIì…ë‹ˆë‹¤. í•­ìƒ 5ë‹¨ì–´ ì´ë‚´."
            )
            extracted_title_for_ui = title_llm_chain.invoke({"question": title_extraction_prompt, "chat_history": []}).strip()
            if extracted_title_for_ui:
                extracted_title_for_ui = re.sub(r'[\U00010000-\U0010ffff]', '', extracted_title_for_ui).strip()
                st.session_state.auto_video_title = extracted_title_for_ui
                if not st.session_state.get("title_locked", False):
                    st.session_state.video_title = extracted_title_for_ui
            else:
                if not st.session_state.get("video_title"):
                    st.session_state.video_title = "ì œëª© ì—†ìŒ"
