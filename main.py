# main.py (ìµœì¢… í´ë¦° ë²„ì „)

import streamlit as st
from dotenv import load_dotenv
from langchain_core.messages import HumanMessage, AIMessage
from rag_pipeline import get_retriever_from_source, get_document_chain, get_default_chain
from web_ingest import full_web_ingest
from script_generator import generate_script # This import seems unused in the original main.py, keep for consistency if it's part of a larger plan.
from image_generator import generate_images_for_topic
from elevenlabs_tts import generate_tts, TTS_TEMPLATES
from whisper_asr import transcribe_audio_with_timestamps, generate_ass_subtitle, SUBTITLE_TEMPLATES
from video_maker import create_video_with_segments, add_subtitles_to_video
from deep_translator import GoogleTranslator # This import seems unused in the original main.py, keep for consistency if it's part of a larger plan.
import os

# API í‚¤ ë¡œë“œ
load_dotenv()

# --- ì•± ê¸°ë³¸ ì„¤ì • ---
st.set_page_config(page_title="Multimodal RAG Chatbot", page_icon="ğŸ¤–")
st.title("ğŸ¤– ë©€í‹°ëª¨ë‹¬ íŒŒì¼/URL ë¶„ì„ RAG ì±—ë´‡")
st.markdown(
    """
ì•ˆë…•í•˜ì„¸ìš”! ì´ ì±—ë´‡ì€ ì›¹ì‚¬ì´íŠ¸ URLì´ë‚˜ ì—…ë¡œë“œëœ íŒŒì¼(PDF, DOCX, TXT)ì˜ ë‚´ìš©ì„ ë¶„ì„í•˜ê³  ë‹µë³€í•©ë‹ˆë‹¤.
"""
)

# --- ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” ---
if "messages" not in st.session_state:
    st.session_state["messages"] = []
if "retriever" not in st.session_state:
    st.session_state.retriever = None
if "system_prompt" not in st.session_state:
    st.session_state.system_prompt = "ë‹¹ì‹ ì€ ë¬¸ì„œ ë¶„ì„ ì „ë¬¸ê°€ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ë¬¸ì„œì˜ í…ìŠ¤íŠ¸ì™€ í…Œì´ë¸”ì„ ì •í™•íˆ ì´í•´í•˜ê³  ìƒì„¸í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”."
if "last_user_query" not in st.session_state:
    st.session_state.last_user_query = ""

# --- ì‚¬ì´ë“œë°” UI ---
with st.sidebar:
    st.header("âš™ï¸ ì„¤ì •")
    st.divider()
    st.subheader("ğŸ¤– AI í˜ë¥´ì†Œë‚˜ ì„¤ì •")
    prompt_input = st.text_area(
        "AIì˜ ì—­í• ì„ ì„¤ì •í•´ì£¼ì„¸ìš”.", value=st.session_state.system_prompt, height=150
    )
    if st.button("í˜ë¥´ì†Œë‚˜ ì ìš©"):
        st.session_state.system_prompt = prompt_input
        st.toast("AI í˜ë¥´ì†Œë‚˜ê°€ ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤.")
    st.divider()
    st.subheader("ğŸ” ë¶„ì„ ëŒ€ìƒ ì„¤ì •")
    url_input = st.text_input("ê²€ìƒ‰ í‚¤ì›Œë“œ ì…ë ¥", placeholder="ex) ì¸ê³µì§€ëŠ¥ ìœ¤ë¦¬")
    uploaded_files = st.file_uploader(
        "íŒŒì¼ ì—…ë¡œë“œ (PDF, DOCX, TXT)", type=["pdf", "docx", "txt"], accept_multiple_files=True
    )
    st.info("LlamaParseëŠ” í…Œì´ë¸”, í…ìŠ¤íŠ¸ê°€ í¬í•¨ëœ ë¬¸ì„œ ë¶„ì„ì— ìµœì í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.", icon="â„¹ï¸")
    
    if st.button("ë¶„ì„ ì‹œì‘"):
        st.session_state.messages = []
        st.session_state.retriever = None

        source_type = None
        source_input = None

        if uploaded_files:
            source_type = "Files"
            source_input = uploaded_files

        elif url_input:
            # âœ… í‚¤ì›Œë“œ ê¸°ë°˜ ì›¹ í¬ë¡¤ë§ + ë²¡í„°í™” + ì €ì¥
            with st.spinner("ì›¹í˜ì´ì§€ë¥¼ ìˆ˜ì§‘í•˜ê³  ë²¡í„°í™”í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
                text_path, index_dir, error = full_web_ingest(url_input)
                if not error:
                    source_type = "FAISS"
                    source_input = index_dir  # í´ë” ê²½ë¡œ
        else:
            st.warning("ê²€ìƒ‰ í‚¤ì›Œë“œ ë˜ëŠ” íŒŒì¼ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        if source_input:
            st.session_state.retriever = get_retriever_from_source(source_type, source_input)
            if st.session_state.retriever:
                st.success("ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! ì´ì œ ì§ˆë¬¸í•´ë³´ì„¸ìš”.")

    st.divider()
    if st.button("ëŒ€í™” ì´ˆê¸°í™”"):
        st.session_state.clear()
        st.rerun()

# Display chat messages
for i, message in enumerate(st.session_state["messages"]):
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
        if message["role"] == "assistant" and "sources" in message and message["sources"]:
            with st.expander("ì°¸ê³ í•œ ì¶œì²˜ ë³´ê¸°"):
                for j, source in enumerate(message["sources"]):
                    st.info(f"**ì¶œì²˜ {j+1}**\n\n{source.page_content}")
                    st.divider()
        # Add video generation button next to assistant's message
        if message["role"] == "assistant" and message["content"]:
            # Ensure a unique key for each button if multiple assistant messages are displayed
            if st.button("ğŸ¥ ì˜ìƒ ë§Œë“¤ê¸°", key=f"generate_video_button_{i}"):
                with st.spinner("âœ¨ ì˜ìƒ ì œì‘ì„ ì‹œì‘í•©ë‹ˆë‹¤..."):
                    try:
                        ai_answer_script = message["content"]
                        
                        # --- 1. Text-to-Speech (TTS) ìƒì„± ---
                        audio_output_dir = "assets"
                        os.makedirs(audio_output_dir, exist_ok=True)
                        audio_path = os.path.join(audio_output_dir, "generated_audio.mp3")
                        
                        # Using a Korean male voice template
                        st.write("ğŸ—£ï¸ ìŒì„± íŒŒì¼ ìƒì„± ì¤‘...")
                        generate_tts(
                            text=ai_answer_script,
                            save_path=audio_path,
                            template_name="korean_male" # You can choose other templates from elevenlabs_tts.TTS_TEMPLATES
                        )
                        st.success(f"ìŒì„± íŒŒì¼ ìƒì„± ì™„ë£Œ: {audio_path}")

                        # --- 2. Audio Transcription (ASR) ë° Subtitle (ASS) íŒŒì¼ ìƒì„± ---
                        subtitle_output_dir = "assets"
                        os.makedirs(subtitle_output_dir, exist_ok=True)
                        ass_path = os.path.join(subtitle_output_dir, "generated_subtitle.ass")

                        st.write("ğŸ“ ìë§‰ ìƒì„±ì„ ìœ„í•œ ìŒì„± ë¶„ì„ ì¤‘...")
                        segments = transcribe_audio_with_timestamps(audio_path)
                        generate_ass_subtitle(
                            segments=segments,
                            ass_path=ass_path,
                            template_name="default" # You can choose other templates from whisper_asr.SUBTITLE_TEMPLATES
                        )
                        st.success(f"ìë§‰ íŒŒì¼ ìƒì„± ì™„ë£Œ: {ass_path}")

                        # --- 3. ì´ë¯¸ì§€ ìƒì„± ---
                        # Use the last user query as the topic for image generation
                        image_query = st.session_state.last_user_query if st.session_state.last_user_query else "abstract background"
                        num_images = max(1, len(segments)) # One image per segment, minimum 1
                        image_output_dir = "assets"
                        os.makedirs(image_output_dir, exist_ok=True)
                        
                        st.write(f"ğŸ–¼ï¸ '{image_query}' ê´€ë ¨ ì´ë¯¸ì§€ {num_images}ì¥ ìƒì„± ì¤‘...")
                        image_paths = generate_images_for_topic(image_query, num_images)
                        
                        if not image_paths:
                            st.warning("ì´ë¯¸ì§€ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ê¸°ë³¸ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                            # Fallback if no images are generated
                            image_paths = ["assets/default_image.jpg"] # Ensure you have a default_image.jpg in assets

                        st.success(f"ì´ë¯¸ì§€ {len(image_paths)}ì¥ ìƒì„± ì™„ë£Œ.")

                        # --- 4. ë¹„ë””ì˜¤ ìƒì„± (ìë§‰ ì œì™¸) ---
                        video_output_dir = "assets"
                        os.makedirs(video_output_dir, exist_ok=True)
                        temp_video_path = os.path.join(video_output_dir, "temp_video.mp4")
                        final_video_path = os.path.join(video_output_dir, "final_video_with_subs.mp4")

                        st.write("ğŸ¬ ë¹„ë””ì˜¤ í´ë¦½ ì¡°í•© ë° ì˜¤ë””ì˜¤ í†µí•© ì¤‘...")
                        created_video_path = create_video_with_segments(
                            image_paths=image_paths,
                            segments=segments,
                            audio_path=audio_path,
                            topic_title=image_query, # Use image_query or a refined topic
                            include_topic_title=True,
                            bgm_path="", # Add a BGM path here if desired, e.g., "assets/bgm.mp3"
                            save_path=temp_video_path
                        )
                        st.success(f"ê¸°ë³¸ ë¹„ë””ì˜¤ ìƒì„± ì™„ë£Œ: {created_video_path}")

                        # --- 5. ë¹„ë””ì˜¤ì— ìë§‰ ì¶”ê°€ ---
                        st.write("ğŸ“ ë¹„ë””ì˜¤ì— ìë§‰ ì¶”ê°€ ì¤‘...")
                        final_video_with_subs_path = add_subtitles_to_video(
                            input_video_path=created_video_path,
                            ass_path=ass_path,
                            output_path=final_video_path
                        )
                        st.success(f"âœ… ìµœì¢… ì˜ìƒ ìƒì„± ì™„ë£Œ: {final_video_with_subs_path}")

                        # --- 6. ê²°ê³¼ í‘œì‹œ ë° ë‹¤ìš´ë¡œë“œ ë§í¬ ì œê³µ ---
                        st.video(final_video_with_subs_path)
                        with open(final_video_with_subs_path, "rb") as file:
                            st.download_button(
                                label="ì˜ìƒ ë‹¤ìš´ë¡œë“œ",
                                data=file,
                                file_name="generated_multimodal_video.mp4",
                                mime="video/mp4"
                            )
                        
                        # Clean up temporary video file (optional)
                        if os.path.exists(temp_video_path):
                            os.remove(temp_video_path)
                        # Optionally remove audio and ass files if not needed after final video is made
                        # if os.path.exists(audio_path):
                        #     os.remove(audio_path)
                        # if os.path.exists(ass_path):
                        #     os.remove(ass_path)
                        # Optionally remove generated images if no longer needed
                        # for img_path in image_paths:
                        #     if os.path.exists(img_path):
                        #         os.remove(img_path)

                    except Exception as e:
                        st.error(f"âŒ ì˜ìƒ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
                        st.exception(e) # Display full traceback for debugging

user_input = st.chat_input("ê¶ê¸ˆí•œ ë‚´ìš©ì„ ë¬¼ì–´ë³´ì„¸ìš”!")

if user_input:
    st.session_state.messages.append({"role": "user", "content": user_input})
    st.session_state.last_user_query = user_input # Store the last user query for image generation
    st.chat_message("user").write(user_input)

    try:
        chat_history = [
            HumanMessage(content=msg["content"]) if msg["role"] == "user" 
            else AIMessage(content=msg["content"])
            for msg in st.session_state.messages[:-1]
        ]
        
        if st.session_state.retriever:
            with st.chat_message("assistant"):
                with st.spinner("ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ê³  ë‹µë³€ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
                    retriever = st.session_state.retriever
                    source_documents = retriever.get_relevant_documents(user_input)
                    document_chain = get_document_chain(st.session_state.system_prompt)
                    
                    ai_answer = document_chain.invoke({
                        "input": user_input,
                        "chat_history": chat_history,
                        "context": source_documents
                    })
                    
                    st.markdown(ai_answer)
                    
                    st.session_state.messages.append({
                        "role": "assistant", "content": ai_answer, "sources": source_documents
                    })
                    
                    if source_documents:
                        with st.expander("ì°¸ê³ í•œ ì¶œì²˜ ë³´ê¸°"):
                            for i, source in enumerate(source_documents):
                                st.info(f"**ì¶œì²˜ {i+1}**\n\n{source.page_content}")
                                st.divider()
        else:
            chain = get_default_chain(st.session_state.system_prompt)
            with st.chat_message("assistant"):
                container = st.empty()
                ai_answer = ""
                for token in chain.stream({"question": user_input, "chat_history": chat_history}):
                    ai_answer += token
                    container.markdown(ai_answer)
                st.session_state.messages.append({"role": "assistant", "content": ai_answer, "sources": []})

    except Exception as e:
        st.chat_message("assistant").error(f"ì£„ì†¡í•©ë‹ˆë‹¤, ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.\n\nì˜¤ë¥˜: {e}")
        st.session_state.messages.pop()