from langchain_core.documents import Document as LangChainDocument
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableMap, RunnableLambda, RunnablePassthrough
#from langchain_groq import ChatGroq  # âœ… Groq import
from langchain_openai import ChatOpenAI
import os



def get_conversational_rag_chain(retriever, system_prompt):
    """
    ìµœì¢…ì ìœ¼ë¡œ ìƒì„±ëœ ë¬¸ì¥ ë‹¨ìœ„ì˜ ì¶œì²˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” RAG ì²´ì¸ì„ êµ¬ì„±í•©ë‹ˆë‹¤.
    """
    llm = ChatOpenAI(
        model="gpt-5-nano",        # ğŸ”‘ nano ëª¨ë¸
        temperature=1,
        api_key=os.getenv("OPENAI_API_KEY")
    )
    
    rag_prompt_template = f"""{system_prompt}

Answer the user's request based *only* on the provided "Context".
If the context does not contain the answer, say you don't know.
Do not use any prior knowledge.

**Context:**
{{context}}

**User's Request:**
{{input}}

**Answer (in Korean):**
"""
    rag_prompt = ChatPromptTemplate.from_template(rag_prompt_template)
    
    def format_docs_with_metadata(docs: list[LangChainDocument]) -> str:
        """ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ LLM í”„ë¡¬í”„íŠ¸ í˜•ì‹ì— ë§ê²Œ ë³€í™˜í•©ë‹ˆë‹¤."""
        if not docs:
            return "No context provided."
        
        sources = {}
        for doc in docs:
            source_url = doc.metadata.get("source", "Unknown Source")
            title = doc.metadata.get("title", "No Title")
            key = (source_url, title)
            if key not in sources:
                sources[key] = []
            sources[key].append(doc.page_content)

        formatted_string = ""
        for (source_url, title), sentences in sources.items():
            formatted_string += f"\n--- Source: {title} ({source_url}) ---\n"
            formatted_string += "\n".join(f"- {s}" for s in sentences)

        return formatted_string.strip()

    rag_chain = RunnableMap({
        "answer": (
            {"context": retriever | RunnableLambda(format_docs_with_metadata), "input": RunnablePassthrough()}
            | rag_prompt
            | llm
            | StrOutputParser()
        ),
        "source_documents": retriever  # ì›ë³¸ ë¬¸ì„œë¥¼ ê·¸ëŒ€ë¡œ ë°˜í™˜
    })

    return rag_chain

def get_default_chain(system_prompt):
    prompt = ChatPromptTemplate.from_messages(
        [("system", system_prompt), ("user", "{question}")]
    )
    # âœ… OpenAI nano ê³„ì—´ ëª¨ë¸
    llm = ChatOpenAI(
        model="gpt-5-nano",
        temperature=1,
        api_key=os.getenv("OPENAI_API_KEY")
    )
    return prompt | llm | StrOutputParser()
