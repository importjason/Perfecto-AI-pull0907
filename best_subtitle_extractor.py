import os
import re
import whisper
import subprocess
import unicodedata
from deep_translator import GoogleTranslator
from googleapiclient.discovery import build

from langchain_core.documents import Document as LangChainDocument
from langchain_openai import OpenAIEmbeddings
from langchain_experimental.text_splitter import SemanticChunker
from langchain_community.vectorstores import FAISS

# ===============================
# ğŸ”‘ [API KEY ì„¤ì • êµ¬ì—­]
# ===============================
# 1. ìœ íŠœë¸Œ APIí‚¤ (Youtube Data API v3 í™œì„±í™” í•„ìš”) # ìœ íŠœë¸Œ ë°ì´í„° ë°›ì•„ì˜¤ê¸° apií‚¤
GOOGLE_API_KEY = "AIzaSyDB1Hl5CqGnw6VyoEt2jlFsbY90zTf2WuM" 

# 2. ì˜¤í”ˆAI APIí‚¤ (https://platform.openai.com/api-keys) #ì„ë² ë”© ë° ë­ì²´ì¸ êµ¬í˜„
OPENAI_API_KEY = "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"

# ===============================
# ğŸ—‚ï¸ [ë””ë ‰í† ë¦¬/í™˜ê²½ ì„¤ì •]
# ===============================
MAX_RESULTS = 50
AUDIO_DIR = os.path.join("output", "youtube_subtitle", "audio")
TXT_DIR = os.path.join("output", "youtube_subtitle", "texts")
YT_DLP_PATH = r"C:\Users\jaemd\Downloads\yt-dlp.exe"

os.makedirs(AUDIO_DIR, exist_ok=True)
os.makedirs(TXT_DIR, exist_ok=True)
model = whisper.load_model("base")

# ===============================
# ğŸ“º [ìœ íŠœë¸Œ ì±„ë„ ID í•´ì„]
# ===============================
def extract_channel_id(input_str):
    if '@' in input_str:
        handle = re.search(r'@[\w\-.]+', input_str)
        if handle:
            return 'handle', handle.group()[1:]
    match = re.search(r'channel/([A-Za-z0-9_-]+)', input_str)
    if match:
        return 'id', match.group(1)
    if input_str.startswith('UC'):
        return 'id', input_str  
    return 'username', input_str

def resolve_channel_id(input_str):
    yt = build("youtube", "v3", developerKey=GOOGLE_API_KEY)
    mode, value = extract_channel_id(input_str)
    if mode == 'handle':
        req = yt.channels().list(part="id", forHandle=value)
        res = req.execute()
        if res.get("items"): return res["items"][0]["id"]
    elif mode == 'id':
        req = yt.channels().list(part="id", id=value)
        res = req.execute()
        if res.get("items"): return res["items"][0]["id"]
    else:
        req = yt.channels().list(part="id", forUsername=value)
        res = req.execute()
        if res.get("items"): return res["items"][0]["id"]
        # fallback: search
        search_req = yt.search().list(q=value, type="channel", part="snippet", maxResults=1)
        search_res = search_req.execute()
        items = search_res.get("items", [])
        if items:
            return items[0]["snippet"]["channelId"]
    raise Exception("ì±„ë„ IDë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ.")

# ===============================
# ğŸ“ [íŒŒì¼ëª… ë³€í™˜, ì˜¤ë””ì˜¤ ë‹¤ìš´ë¡œë“œ ë“±]
# ===============================
def safe_filename(title):
    """í•œê¸€ ì œëª©ì´ë©´ ì˜ì–´ë¡œ ë²ˆì—­í•´ì„œ ì•ˆì „í•œ íŒŒì¼ëª… ìƒì„±"""
    if any('\uac00' <= c <= '\ud7a3' for c in title):
        try:
            translated = GoogleTranslator(source='ko', target='en').translate(title)
        except Exception:
            translated = title
    else:
        translated = title
    safe_title = unicodedata.normalize("NFKD", translated)
    safe_title = safe_title.encode("ascii", "ignore").decode("ascii")
    safe_title = re.sub(r'[\\/*?:"<>|#;]', "", safe_title)
    safe_title = safe_title.strip().replace(" ", "_")
    return safe_title[:100] or "untitled"

def get_videos_by_viewcount(channel_id, max_results):
    yt = build("youtube", "v3", developerKey=GOOGLE_API_KEY)
    uploads_pid = yt.channels().list(part="contentDetails", id=channel_id)\
        .execute()["items"][0]["contentDetails"]["relatedPlaylists"]["uploads"]
    videos, next_page = [], None
    while len(videos) < 200:
        pl_req = yt.playlistItems().list(
            part="snippet", playlistId=uploads_pid, maxResults=50, pageToken=next_page
        )
        pl_res = pl_req.execute()
        for item in pl_res["items"]:
            title = item["snippet"]["title"]
            video_id = item["snippet"]["resourceId"]["videoId"]
            video_url = f"https://www.youtube.com/watch?v={video_id}"
            if "shorts" in title.lower() or "/shorts/" in video_url: continue
            videos.append((title, video_url))
            if len(videos) >= 200: break
        next_page = pl_res.get("nextPageToken")
        if not next_page: break
    # ì¸ê¸°ìˆœ ì •ë ¬
    video_ids = [link.split("v=")[-1] for _, link in videos]
    video_info = []
    for i in range(0, len(video_ids), 50):
        sub_ids = video_ids[i:i+50]
        stats_res = yt.videos().list(
            part="statistics,snippet", id=",".join(sub_ids)
        ).execute()
        for item in stats_res["items"]:
            view_count = int(item["statistics"].get("viewCount", 0))
            title = item["snippet"]["title"]
            video_url = f"https://www.youtube.com/watch?v={item['id']}"
            video_info.append((title, video_url, view_count))
    video_info.sort(key=lambda x: x[2], reverse=True)
    return [(title, link) for title, link, _ in video_info[:max_results]]

def download_audio(link, title):
    safe_title = safe_filename(title)
    output_path = os.path.join(AUDIO_DIR, f"{safe_title}.%(ext)s")
    cmd = [
        YT_DLP_PATH, "-f", "bestaudio", "-o", output_path,
        "--extract-audio", "--audio-format", "mp3", link
    ]
    subprocess.run(cmd, check=True, capture_output=True, text=True)
    final_path = os.path.join(AUDIO_DIR, f"{safe_title}.mp3")
    if not os.path.exists(final_path):
        raise FileNotFoundError(f"mp3 ìƒì„± ì‹¤íŒ¨: {final_path}")
    return final_path, safe_title

def transcribe_to_txt(audio_path, filename_base):
    result = model.transcribe(audio_path, task="transcribe", verbose=False)
    segments = result.get("segments", [])
    texts = [seg["text"].strip() for seg in segments if seg["text"].strip()]
    return texts

# ===============================
# ğŸ’¾ [ì„ë² ë”©/ë²¡í„°í™”(ë­ì²´ì¸)]
# ===============================
def vectorize_txt(txt_path):
    with open(txt_path, encoding='utf-8') as f:
        lines = [line.strip() for line in f if line.strip()]
    docs = [LangChainDocument(page_content=line) for line in lines]
    embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)
    splitter = SemanticChunker(embeddings, breakpoint_threshold_type="percentile")
    splits = splitter.split_documents(docs)
    vectorstore = FAISS.from_documents(splits, embeddings)
    print(f"âœ… {txt_path} â†’ {len(splits)}ê°œ ë¬¸ë‹¨ìœ¼ë¡œ ì„ë² ë”©, ë²¡í„°DB ì €ì¥ ì™„ë£Œ")
    return vectorstore

def query_vectorstore(vectorstore, query, k=5):
    results = vectorstore.similarity_search(query, k=k)
    print(f"\n[ê²€ìƒ‰ê²°ê³¼ Top-{k}]")
    for i, doc in enumerate(results):
        print(f"\n--- {i+1} ---")
        print(doc.page_content)

# ===============================
# ğŸš€ [ë©”ì¸]
# ===============================
def main():
    user_input = input("ìœ íŠœë¸Œ @ì•„ì´ë””/ì±„ë„ID/URL ì…ë ¥: ").strip()
    try:
        channel_id = resolve_channel_id(user_input)
    except Exception as e:
        print(f"ì±„ë„ ì¸ì‹ ì‹¤íŒ¨: {e}"); return
    print(f"ì±„ë„ID: {channel_id}")

    videos = get_videos_by_viewcount(channel_id, MAX_RESULTS)
    for idx, (title, link) in enumerate(videos):
        print(f"\n[{idx+1}/{len(videos)}] ğŸ¬ {title}")
        try:
            audio_path, filename_base = download_audio(link, title)
            print("ğŸ§  Whisper í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘...")
            texts = transcribe_to_txt(audio_path, filename_base)
            
            # í…ìŠ¤íŠ¸ ì¶œë ¥
            print(f"\n[{idx+1}/{len(videos)}] ğŸ“„ ìë§‰ ë‚´ìš©:")
            print("=" * 80)
            for text in texts:
                print(text)
            print("=" * 80)
            print(f"âœ… í…ìŠ¤íŠ¸ ì¶œë ¥ ì™„ë£Œ")

        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜: {e}"); continue

if __name__ == "__main__":
    main()
