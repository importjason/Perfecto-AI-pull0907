import os
import re
import whisper
import subprocess
import unicodedata
import streamlit as st
from deep_translator import GoogleTranslator
from googleapiclient.discovery import build
from yt_dlp import YoutubeDL
from langchain_core.documents import Document as LangChainDocument
from langchain_openai import OpenAIEmbeddings
from langchain_experimental.text_splitter import SemanticChunker
from langchain_community.vectorstores import FAISS

# ===============================
# ğŸ”‘ [API KEY ì„¤ì • êµ¬ì—­]
# ===============================
# 1. ìœ íŠœë¸Œ APIí‚¤ (Youtube Data API v3 í™œì„±í™” í•„ìš”) # ìœ íŠœë¸Œ ë°ì´í„° ë°›ì•„ì˜¤ê¸° apií‚¤
GOOGLE_API_KEY = st.secrets["GOOGLE_API_KEY"]

# 2. ì˜¤í”ˆAI APIí‚¤ (https://platform.openai.com/api-keys) #ì„ë² ë”© ë° ë­ì²´ì¸ êµ¬í˜„
OPENAI_API_KEY = st.secrets["OPENAI_API_KEY"]

# ===============================
# ğŸ—‚ï¸ [ë””ë ‰í† ë¦¬/í™˜ê²½ ì„¤ì •]
# ===============================
MAX_RESULTS = 50
AUDIO_DIR = os.path.join("output", "youtube_subtitle", "audio")
TXT_DIR = os.path.join("output", "youtube_subtitle", "texts")

os.makedirs(AUDIO_DIR, exist_ok=True)
os.makedirs(TXT_DIR, exist_ok=True)
model = whisper.load_model("base")

# ===============================
# ğŸ“º [ìœ íŠœë¸Œ ì±„ë„ ID í•´ì„]
# ===============================
def extract_channel_id(input_str):
    if '@' in input_str:
        handle = re.search(r'@[\w\-.]+', input_str)
        if handle:
            return 'handle', handle.group()[1:]
    match = re.search(r'channel/([A-Za-z0-9_-]+)', input_str)
    if match:
        return 'id', match.group(1)
    if input_str.startswith('UC'):
        return 'id', input_str  
    return 'username', input_str

def resolve_channel_id(input_str):
    yt = build("youtube", "v3", developerKey=GOOGLE_API_KEY)
    mode, value = extract_channel_id(input_str)
    if mode == 'handle':
        req = yt.channels().list(part="id", forHandle=value)
        res = req.execute()
        if res.get("items"): return res["items"][0]["id"]
    elif mode == 'id':
        req = yt.channels().list(part="id", id=value)
        res = req.execute()
        if res.get("items"): return res["items"][0]["id"]
    else:
        req = yt.channels().list(part="id", forUsername=value)
        res = req.execute()
        if res.get("items"): return res["items"][0]["id"]
        # fallback: search
        search_req = yt.search().list(q=value, type="channel", part="snippet", maxResults=1)
        search_res = search_req.execute()
        items = search_res.get("items", [])
        if items:
            return items[0]["snippet"]["channelId"]
    raise Exception("ì±„ë„ IDë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ.")

# ===============================
# ğŸ“ [íŒŒì¼ëª… ë³€í™˜, ì˜¤ë””ì˜¤ ë‹¤ìš´ë¡œë“œ ë“±]
# ===============================
def safe_filename(title):
    """í•œê¸€ ì œëª©ì´ë©´ ì˜ì–´ë¡œ ë²ˆì—­í•´ì„œ ì•ˆì „í•œ íŒŒì¼ëª… ìƒì„±"""
    if any('\uac00' <= c <= '\ud7a3' for c in title):
        try:
            translated = GoogleTranslator(source='ko', target='en').translate(title)
        except Exception:
            translated = title
    else:
        translated = title
    safe_title = unicodedata.normalize("NFKD", translated)
    safe_title = safe_title.encode("ascii", "ignore").decode("ascii")
    safe_title = re.sub(r'[\\/*?:"<>|#;]', "", safe_title)
    safe_title = safe_title.strip().replace(" ", "_")
    return safe_title[:100] or "untitled"

def get_videos_by_viewcount(channel_id, max_results):
    yt = build("youtube", "v3", developerKey=GOOGLE_API_KEY)
    uploads_pid = yt.channels().list(part="contentDetails", id=channel_id)\
        .execute()["items"][0]["contentDetails"]["relatedPlaylists"]["uploads"]
    videos, next_page = [], None
    while len(videos) < 200:
        pl_req = yt.playlistItems().list(
            part="snippet", playlistId=uploads_pid, maxResults=50, pageToken=next_page
        )
        pl_res = pl_req.execute()
        for item in pl_res["items"]:
            title = item["snippet"]["title"]
            video_id = item["snippet"]["resourceId"]["videoId"]
            video_url = f"https://www.youtube.com/watch?v={video_id}"
            if "shorts" in title.lower() or "/shorts/" in video_url: continue
            videos.append((title, video_url))
            if len(videos) >= 200: break
        next_page = pl_res.get("nextPageToken")
        if not next_page: break
    # ì¸ê¸°ìˆœ ì •ë ¬
    video_ids = [link.split("v=")[-1] for _, link in videos]
    video_info = []
    for i in range(0, len(video_ids), 50):
        sub_ids = video_ids[i:i+50]
        stats_res = yt.videos().list(
            part="statistics,snippet", id=",".join(sub_ids)
        ).execute()
        for item in stats_res["items"]:
            view_count = int(item["statistics"].get("viewCount", 0))
            title = item["snippet"]["title"]
            video_url = f"https://www.youtube.com/watch?v={item['id']}"
            video_info.append((title, video_url, view_count))
    video_info.sort(key=lambda x: x[2], reverse=True)
    return [(title, link) for title, link, _ in video_info[:max_results]]

def download_audio(link, title):
    safe_title = safe_filename(title)
    output_path = os.path.join(AUDIO_DIR, f"{safe_title}.mp3")

    ydl_opts = {
        'format': 'bestaudio/best',
        'outtmpl': output_path,
        'postprocessors': [{
            'key': 'FFmpegExtractAudio',
            'preferredcodec': 'mp3',
            'preferredquality': '192',
        }],
        'quiet': False,
        'noplaylist': True,
        'verbose': True
    }

    st.info(f"ğŸŒ€ [DEBUG] yt-dlp ë‹¤ìš´ë¡œë“œ ì‹œì‘:\nğŸ”— {link}\nğŸ“ ì €ì¥ ìœ„ì¹˜: `{output_path}`")

    try:
        with YoutubeDL(ydl_opts) as ydl:
            ydl.download([link])
    except Exception as e:
        st.error(f"âŒ [DEBUG] yt-dlp ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨:\n```\n{e}\n```")
        raise RuntimeError(f"âŒ yt-dlp ì˜¤ë¥˜ ë°œìƒ: {e}")

    # ë””ë ‰í† ë¦¬ ë‚´ë¶€ íŒŒì¼ ëª©ë¡ í™•ì¸
    try:
        file_list = os.listdir(AUDIO_DIR)
        st.info(f"ğŸ“‚ [DEBUG] AUDIO_DIR ë‚´ë¶€ íŒŒì¼:\n{file_list}")
    except Exception as e:
        st.warning(f"âš ï¸ AUDIO_DIR ëª©ë¡ í™•ì¸ ì‹¤íŒ¨: {e}")

    if not os.path.exists(output_path):
        st.warning(f"â— [DEBUG] mp3 íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: `{output_path}`")
        raise FileNotFoundError(f"âŒ mp3 ìƒì„± ì‹¤íŒ¨: {output_path}")

    st.success(f"âœ… [DEBUG] mp3 ìƒì„± ì„±ê³µ: `{output_path}`")
    return output_path, safe_title

def transcribe_to_txt(audio_path, filename_base):
    result = model.transcribe(audio_path, task="transcribe", verbose=True)
    segments = result.get("segments", [])
    texts = [seg["text"].strip() for seg in segments if seg["text"].strip()]
    return texts

# ===============================
# ğŸ’¾ [ì„ë² ë”©/ë²¡í„°í™”(ë­ì²´ì¸)]
# ===============================
def vectorize_txt(txt_path):
    with open(txt_path, encoding='utf-8') as f:
        lines = [line.strip() for line in f if line.strip()]
    docs = [LangChainDocument(page_content=line) for line in lines]
    embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)
    splitter = SemanticChunker(embeddings, breakpoint_threshold_type="percentile")
    splits = splitter.split_documents(docs)
    vectorstore = FAISS.from_documents(splits, embeddings)
    print(f"âœ… {txt_path} â†’ {len(splits)}ê°œ ë¬¸ë‹¨ìœ¼ë¡œ ì„ë² ë”©, ë²¡í„°DB ì €ì¥ ì™„ë£Œ")
    return vectorstore

def query_vectorstore(vectorstore, query, k=5):
    results = vectorstore.similarity_search(query, k=k)
    print(f"\n[ê²€ìƒ‰ê²°ê³¼ Top-{k}]")
    for i, doc in enumerate(results):
        print(f"\n--- {i+1} ---")
        print(doc.page_content)

def load_best_subtitles_documents(channel_handle_or_url, max_results=10):
    """
    ìœ íŠœë¸Œ ìë§‰ ê¸°ë°˜ ë¬¸ì„œ ë¡œë”© í•¨ìˆ˜.
    - ê³ ì •ëœ ì±„ë„ ID ëŒ€ì‹  í•¸ë“¤ ë˜ëŠ” URL ì…ë ¥ì„ ë°›ì•„ ì²˜ë¦¬
    - Whisperë¡œ ìë§‰ ì¶”ì¶œ ì‹œ ë¹ˆ í…ìŠ¤íŠ¸ ì—¬ë¶€ë„ í™•ì¸
    """

    documents = []

    try:
        # ì±„ë„ ID ì¶”ì¶œ
        st.info(f"ğŸ” ì±„ë„ ì •ë³´ íŒŒì‹± ì‹œì‘: {channel_handle_or_url}")
        channel_id = resolve_channel_id(channel_handle_or_url)
        st.success(f"âœ… ì±„ë„ ID ì¶”ì¶œ ì„±ê³µ: {channel_id}")
    except Exception as e:
        st.error(f"âŒ ì±„ë„ ID ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return []

    try:
        videos = get_videos_by_viewcount(channel_id, max_results)
        st.info(f"ğŸ“º ì´ {len(videos)}ê°œì˜ ì˜ìƒ ê°€ì ¸ì˜´")
    except Exception as e:
        st.error(f"âŒ ì¸ê¸° ì˜ìƒ ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨: {e}")
        return []

    for title, link in videos:
        try:
            st.info(f"ğŸ¬ ì²˜ë¦¬ ì¤‘: {title} | {link}")
            audio_path, filename_base = download_audio(link, title)
            texts = transcribe_to_txt(audio_path, filename_base)

            if not texts:
                print(f"âš ï¸ ìë§‰ ì—†ìŒ ë˜ëŠ” Whisper ì‹¤íŒ¨: {title}")
                continue

            for line in texts:
                if line.strip():
                    documents.append(
                        LangChainDocument(page_content=line.strip(), metadata={"source": link})
                    )
        except Exception as e:
            st.error(f"âŒ [{title}] ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            continue

    st.success(f"ğŸ“¦ ì´ {len(documents)}ê°œì˜ ìë§‰ ë¬¸ì„œ ìƒì„± ì™„ë£Œ")
    return documents