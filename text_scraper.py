import requests
from bs4 import BeautifulSoup
from googlesearch import search
import os   
import re
import time
import concurrent.futures
from collections import defaultdict
from urllib.parse import urlparse, urljoin
import threading
from urllib.robotparser import RobotFileParser
from config import *

# robots.txt í™•ì¸ ì„¤ì •
ROBOTS_CHECK_ENABLED = True  # robots.txt í™•ì¸ í™œì„±í™”
ROBOTS_TIMEOUT = 10  # robots.txt ìš”ì²­ íƒ€ì„ì•„ì›ƒ

# í¬ë¡¤ë§ ì œí•œ ì„¤ì •
MAX_CRAWL_LIMIT = 70  # ìµœëŒ€ í¬ë¡¤ë§ ê°œìˆ˜ ì œí•œ

def check_robots_txt(url):
    """robots.txt í™•ì¸í•˜ì—¬ ìŠ¤í¬ë˜í•‘ í—ˆìš© ì—¬ë¶€ íŒë‹¨"""
    if not ROBOTS_CHECK_ENABLED:
        return True, "robots.txt í™•ì¸ ë¹„í™œì„±í™”"
    
    try:
        parsed_url = urlparse(url)
        robots_url = f"{parsed_url.scheme}://{parsed_url.netloc}/robots.txt"
        
        # robots.txt ìš”ì²­
        response = requests.get(robots_url, timeout=ROBOTS_TIMEOUT)
        
        if response.status_code == 404:
            return True, "robots.txt ì—†ìŒ (ê¸°ë³¸ í—ˆìš©)"
        
        if response.status_code != 200:
            return True, f"robots.txt ì ‘ê·¼ ì‹¤íŒ¨ ({response.status_code})"
        
        # robots.txt ë‚´ìš© íŒŒì‹±
        robots_content = response.text
        path_analysis = analyze_robots_paths(robots_content, parsed_url.path)
        
        # RobotFileParserë¡œ íŒŒì‹±
        rp = RobotFileParser()
        rp.set_url(robots_url)
        rp.read()
        
        # User-agent: * ì— ëŒ€í•œ í—ˆìš© ì—¬ë¶€ í™•ì¸
        can_fetch = rp.can_fetch("*", url)
        
        if can_fetch:
            if path_analysis:
                return True, f"robots.txt í—ˆìš© ({path_analysis})"
            else:
                return True, "robots.txt í—ˆìš©"
        else:
            if path_analysis:
                return False, f"robots.txt ê¸ˆì§€ ({path_analysis})"
            else:
                return False, "robots.txt ê¸ˆì§€"
            
    except requests.exceptions.Timeout:
        return True, "robots.txt íƒ€ì„ì•„ì›ƒ (ê¸°ë³¸ í—ˆìš©)"
    except Exception as e:
        return True, f"robots.txt í™•ì¸ ì‹¤íŒ¨: {str(e)}"

def analyze_robots_paths(robots_content, path):
    """robots.txtì—ì„œ ê²½ë¡œë³„ í—ˆìš©/ê¸ˆì§€ ê·œì¹™ ë¶„ì„"""
    try:
        if not robots_content:
            return None
        
        # ê²½ë¡œë³„ ê·œì¹™ì„ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬
        path_rules = {
            'allowed': [],
            'disallowed': []
        }
        
        lines = robots_content.split('\n')
        current_user_agent = None
        
        for line in lines:
            line = line.strip()
            if not line or line.startswith('#'):
                continue
                
            if line.lower().startswith('user-agent:'):
                current_user_agent = line.split(':', 1)[1].strip()
            elif line.lower().startswith('allow:') and current_user_agent in ['*', None]:
                allowed_path = line.split(':', 1)[1].strip()
                path_rules['allowed'].append(allowed_path)
            elif line.lower().startswith('disallow:') and current_user_agent in ['*', None]:
                disallowed_path = line.split(':', 1)[1].strip()
                path_rules['disallowed'].append(disallowed_path)
        
        # í˜„ì¬ ê²½ë¡œì— ëŒ€í•œ ê·œì¹™ ë¶„ì„
        path_analysis = []
        
        # í—ˆìš©ëœ ê²½ë¡œ í™•ì¸
        for allowed_path in path_rules['allowed']:
            if path.startswith(allowed_path) or allowed_path == '/':
                path_analysis.append(f"í—ˆìš©: {allowed_path}")
        
        # ê¸ˆì§€ëœ ê²½ë¡œ í™•ì¸
        for disallowed_path in path_rules['disallowed']:
            if path.startswith(disallowed_path):
                path_analysis.append(f"ê¸ˆì§€: {disallowed_path}")
        
        # ê²½ë¡œë³„ ìš°ì„ ìˆœìœ„ ê·œì¹™ ì ìš©
        if path_analysis:
            # ë” êµ¬ì²´ì ì¸ ê²½ë¡œê°€ ìš°ì„  (ê¸´ ê²½ë¡œê°€ ìš°ì„ )
            allowed_rules = [rule for rule in path_analysis if rule.startswith('í—ˆìš©:')]
            disallowed_rules = [rule for rule in path_analysis if rule.startswith('ê¸ˆì§€:')]
            
            if allowed_rules and disallowed_rules:
                # ê°€ì¥ êµ¬ì²´ì ì¸ ê·œì¹™ ë¹„êµ
                most_specific_allowed = max(allowed_rules, key=lambda x: len(x.split(':')[1]))
                most_specific_disallowed = max(disallowed_rules, key=lambda x: len(x.split(':')[1]))
                
                allowed_path_len = len(most_specific_allowed.split(':')[1])
                disallowed_path_len = len(most_specific_disallowed.split(':')[1])
                
                if allowed_path_len > disallowed_path_len:
                    return f"ê²½ë¡œë³„ í—ˆìš© ìš°ì„ : {most_specific_allowed}"
                elif disallowed_path_len > allowed_path_len:
                    return f"ê²½ë¡œë³„ ê¸ˆì§€ ìš°ì„ : {most_specific_disallowed}"
                else:
                    return f"ë™ì¼ ìš°ì„ ìˆœìœ„: {most_specific_allowed}, {most_specific_disallowed}"
            elif allowed_rules:
                return f"ê²½ë¡œë³„ í—ˆìš©: {', '.join(allowed_rules)}"
            elif disallowed_rules:
                return f"ê²½ë¡œë³„ ê¸ˆì§€: {', '.join(disallowed_rules)}"
        
        return None
        
    except Exception as e:
        return f"ê²½ë¡œ ë¶„ì„ ì‹¤íŒ¨: {str(e)}"

def get_links(query, num=30):
    start_time = time.time()
    print(f"\n[+] '{query}' ê´€ë ¨ ë§í¬ ê²€ìƒ‰ ì¤‘... (ëª©í‘œ: {num}ê°œ)")
    
    try:
        results = []
        all_urls = []
        
        # ë” ë§ì€ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ìˆ˜ì§‘
        for url in search(query, num_results=num):
            all_urls.append(url)
            if any(domain in url for domain in SEARCH_DOMAINS):
                results.append(url)
                if len(results) >= MAX_SITES:
                    break
        
        # ì›í•˜ëŠ” ë„ë©”ì¸ì´ ë¶€ì¡±í•˜ë©´ ë‹¤ë¥¸ ì‚¬ì´íŠ¸ë„ ì¶”ê°€
        if len(results) < MAX_SITES:
            print(f"[!] ì›í•˜ëŠ” ë„ë©”ì¸ ì‚¬ì´íŠ¸ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ({len(results)}ê°œ)")
            print(f"[!] ë‹¤ë¥¸ ì‚¬ì´íŠ¸ë„ ì¶”ê°€ë¡œ ìˆ˜ì§‘í•©ë‹ˆë‹¤...")
            
            for url in all_urls:
                if url not in results and len(results) < MAX_SITES:
                    # ì œì™¸í•  ë„ë©”ì¸ë“¤
                    exclude_domains = ["youtube.com", "facebook.com", "twitter.com", "instagram.com", "linkedin.com"]
                    if not any(exclude in url for exclude in exclude_domains):
                        results.append(url)
        
        elapsed = time.time() - start_time
        print(f"[+] {len(results)}ê°œ ë§í¬ ìˆ˜ì§‘ ì™„ë£Œ (ì†Œìš”ì‹œê°„: {elapsed:.2f}ì´ˆ)")
        print(f"[+] ê²€ìƒ‰ëœ ì´ URL: {len(all_urls)}ê°œ, í•„í„°ë§ í›„: {len(results)}ê°œ")
        return results
    except Exception as e:
        print(f"[-] ë§í¬ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        return []

def clean_html_worker(args):
    url, url_index = args
    start_time = time.time()
    
    try:
        if SHOW_DETAILED_PROGRESS:
            print(f"[{url_index+1:2d}] í˜ì´ì§€ íŒŒì‹± ì¤‘: {url}")
        
        # ìš”ì²­ ì„¤ì •
        request_kwargs = {}
        if ENABLE_TIMEOUT:
            request_kwargs['timeout'] = TIMEOUT_SECONDS
        if ENABLE_USER_AGENT:
            request_kwargs['headers'] = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
        
        response = requests.get(url, **request_kwargs)
        response.encoding = response.apparent_encoding  # ì¸ì½”ë”© ìë™ ê°ì§€ ì¶”ê°€
        soup = BeautifulSoup(response.text, "html.parser")
        
        # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
        for tag in soup(["script", "style", "footer", "nav", "form", "header", "aside", "iframe"]):
            tag.decompose()
        
        # í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì •ë¦¬
        text = soup.get_text(separator=" ", strip=True)
        # í•œê¸€, ì˜ë¬¸, ìˆ«ì, ê³µë°±, ì¼ë¶€ íŠ¹ìˆ˜ë¬¸ìë§Œ ë‚¨ê¸°ê¸°
        text = re.sub(r'[^ê°€-í£a-zA-Z0-9 .,!?\n\r\t]', '', text)
        
        elapsed = time.time() - start_time
        if SHOW_DETAILED_PROGRESS:
            print(f"[{url_index+1:2d}] âœ“ ì„±ê³µ: {url} (ì†Œìš”ì‹œê°„: {elapsed:.2f}ì´ˆ)")
        
        return {
            'url': url,
            'text': text,
            'success': True,
            'elapsed': elapsed,
            'error': None
        }
        
    except requests.exceptions.Timeout:
        elapsed = time.time() - start_time
        if SHOW_DETAILED_PROGRESS:
            print(f"[{url_index+1:2d}] âœ— íƒ€ì„ì•„ì›ƒ: {url} (ì†Œìš”ì‹œê°„: {elapsed:.2f}ì´ˆ)")
        return {
            'url': url,
            'text': "",
            'success': False,
            'elapsed': elapsed,
            'error': 'íƒ€ì„ì•„ì›ƒ'
        }
    except Exception as e:
        elapsed = time.time() - start_time
        if SHOW_DETAILED_PROGRESS:
            print(f"[{url_index+1:2d}] âœ— ì‹¤íŒ¨: {url} - {e} (ì†Œìš”ì‹œê°„: {elapsed:.2f}ì´ˆ)")
        return {
            'url': url,
            'text': "",
            'success': False,
            'elapsed': elapsed,
            'error': str(e)
        }

def check_robots_for_urls(urls):
    """URL ëª©ë¡ì— ëŒ€í•´ robots.txt í™•ì¸"""
    print(f"\n[+] robots.txt í™•ì¸ ì¤‘... ({len(urls)}ê°œ ì‚¬ì´íŠ¸)")
    
    robots_results = []
    for i, url in enumerate(urls):
        domain = urlparse(url).netloc
        print(f"[{i+1:2d}] robots.txt í™•ì¸: {domain}")
        
        is_allowed, reason = check_robots_txt(url)
        robots_results.append({
            'url': url,
            'domain': domain,
            'allowed': is_allowed,
            'reason': reason
        })
        
        status = "âœ… í—ˆìš©" if is_allowed else "âŒ ê¸ˆì§€"
        print(f"[{i+1:2d}] {status}: {domain} - {reason}")
    
    return robots_results

def filter_urls_by_robots(robots_results):
    """robots.txt ê²°ê³¼ì— ë”°ë¼ URL í•„í„°ë§ ë° ì‚¬ìš©ì ì„ íƒ"""
    print(f"\n" + "="*60)
    print("ğŸ¤– robots.txt í•„í„°ë§ ê²°ê³¼")
    print("="*60)
    
    allowed_urls = [r for r in robots_results if r['allowed']]
    blocked_urls = [r for r in robots_results if not r['allowed']]
    
    print(f"ğŸ“Š ì´ {len(robots_results)}ê°œ ì‚¬ì´íŠ¸ ë¶„ì„ ì™„ë£Œ")
    print(f"âœ… ìŠ¤í¬ë˜í•‘ í—ˆìš©: {len(allowed_urls)}ê°œ")
    print(f"âŒ ìŠ¤í¬ë˜í•‘ ê¸ˆì§€: {len(blocked_urls)}ê°œ")
    
    # í¬ë¡¤ë§ ì œí•œ í™•ì¸
    if len(allowed_urls) > MAX_CRAWL_LIMIT:
        print(f"\nâš ï¸  í—ˆìš©ëœ ì‚¬ì´íŠ¸ê°€ {MAX_CRAWL_LIMIT}ê°œë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤.")
        print(f"   ìµœëŒ€ {MAX_CRAWL_LIMIT}ê°œê¹Œì§€ë§Œ ì²˜ë¦¬ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
        allowed_urls = allowed_urls[:MAX_CRAWL_LIMIT]
        print(f"   ìƒìœ„ {MAX_CRAWL_LIMIT}ê°œ ì‚¬ì´íŠ¸ë§Œ ì„ íƒë©ë‹ˆë‹¤.")
    
    if len(blocked_urls) > 0:
        print(f"\nâŒ ìŠ¤í¬ë˜í•‘ ê¸ˆì§€ëœ ì‚¬ì´íŠ¸ë“¤:")
        for i, result in enumerate(blocked_urls, 1):
            print(f"  {i:2d}. {result['domain']} - {result['reason']}")
    
    if len(allowed_urls) == 0:
        print("\nâš ï¸  ìŠ¤í¬ë˜í•‘ì´ í—ˆìš©ëœ ì‚¬ì´íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
        choice = input("ê¸ˆì§€ëœ ì‚¬ì´íŠ¸ë„ í¬í•¨í•˜ì—¬ ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").strip().lower()
        if choice == 'y':
            # ê¸ˆì§€ëœ ì‚¬ì´íŠ¸ë„ í¬í•¨í•˜ë˜ ì œí•œ ì ìš©
            all_urls = [r['url'] for r in robots_results]
            if len(all_urls) > MAX_CRAWL_LIMIT:
                print(f"âš ï¸  ëª¨ë“  ì‚¬ì´íŠ¸ê°€ {MAX_CRAWL_LIMIT}ê°œë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤.")
                all_urls = all_urls[:MAX_CRAWL_LIMIT]
            return all_urls
        else:
            return []
    
    print(f"\nâœ… ìŠ¤í¬ë˜í•‘ í—ˆìš©ëœ ì‚¬ì´íŠ¸ë“¤ (ìµœëŒ€ {MAX_CRAWL_LIMIT}ê°œ):")
    for i, result in enumerate(allowed_urls, 1):
        print(f"  {i:2d}. {result['domain']} - {result['reason']}")
    
    print(f"\n" + "-"*60)
    print("í•„í„°ë§ ì˜µì…˜:")
    print("1. í—ˆìš©ëœ ì‚¬ì´íŠ¸ë§Œ ì²˜ë¦¬")
    print("2. ëª¨ë“  ì‚¬ì´íŠ¸ ì²˜ë¦¬ (ê¸ˆì§€ëœ ì‚¬ì´íŠ¸ í¬í•¨)")
    print("3. ìˆ˜ë™ ì„ íƒ")
    print("0. ì·¨ì†Œ")
    
    while True:
        try:
            choice = input("\nì„ íƒí•˜ì„¸ìš” (0-3): ").strip()
            
            if choice == "0":
                return []
            elif choice == "1":
                print(f"âœ… í—ˆìš©ëœ ì‚¬ì´íŠ¸ {len(allowed_urls)}ê°œ ì„ íƒë¨")
                return [r['url'] for r in allowed_urls]
            elif choice == "2":
                # ëª¨ë“  ì‚¬ì´íŠ¸ ì„ íƒ ì‹œì—ë„ ì œí•œ ì ìš©
                all_urls = [r['url'] for r in robots_results]
                if len(all_urls) > MAX_CRAWL_LIMIT:
                    print(f"âš ï¸  ëª¨ë“  ì‚¬ì´íŠ¸ê°€ {MAX_CRAWL_LIMIT}ê°œë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤.")
                    all_urls = all_urls[:MAX_CRAWL_LIMIT]
                    print(f"   ìƒìœ„ {MAX_CRAWL_LIMIT}ê°œ ì‚¬ì´íŠ¸ë§Œ ì„ íƒë©ë‹ˆë‹¤.")
                print(f"âœ… ëª¨ë“  ì‚¬ì´íŠ¸ {len(all_urls)}ê°œ ì„ íƒë¨")
                return all_urls
            elif choice == "3":
                return manual_url_selection(allowed_urls)
            else:
                print("âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤. 0-3 ì¤‘ ì„ íƒí•´ì£¼ì„¸ìš”.")
        except KeyboardInterrupt:
            print("\nâŒ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            return []

def manual_url_selection(allowed_urls):
    """ìˆ˜ë™ URL ì„ íƒ"""
    print(f"\nğŸ“ ìˆ˜ë™ ì„ íƒ ëª¨ë“œ (í—ˆìš©ëœ ì‚¬ì´íŠ¸ {len(allowed_urls)}ê°œ, ìµœëŒ€ {MAX_CRAWL_LIMIT}ê°œ ì„ íƒ ê°€ëŠ¥)")
    print("ì²˜ë¦¬í•  ì‚¬ì´íŠ¸ ë²ˆí˜¸ë¥¼ ì‰¼í‘œë¡œ êµ¬ë¶„í•˜ì—¬ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: 1,3,5)")
    print("ë˜ëŠ” 'all' ì…ë ¥ ì‹œ ëª¨ë“  í—ˆìš©ëœ ì‚¬ì´íŠ¸ ì„ íƒ")
    
    while True:
        try:
            choice = input("ì„ íƒ: ").strip()
            
            if choice.lower() == 'all':
                # ìµœëŒ€ ì œí•œ ì ìš©
                if len(allowed_urls) > MAX_CRAWL_LIMIT:
                    print(f"âš ï¸  ëª¨ë“  ì‚¬ì´íŠ¸ê°€ {MAX_CRAWL_LIMIT}ê°œë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤.")
                    selected_urls = [r['url'] for r in allowed_urls[:MAX_CRAWL_LIMIT]]
                    print(f"   ìƒìœ„ {MAX_CRAWL_LIMIT}ê°œ ì‚¬ì´íŠ¸ë§Œ ì„ íƒë©ë‹ˆë‹¤.")
                else:
                    selected_urls = [r['url'] for r in allowed_urls]
                print(f"âœ… ì„ íƒëœ ì‚¬ì´íŠ¸ {len(selected_urls)}ê°œ")
                return selected_urls
            
            # ë²ˆí˜¸ íŒŒì‹±
            selected_indices = [int(x.strip()) - 1 for x in choice.split(',')]
            selected_urls = []
            
            for idx in selected_indices:
                if 0 <= idx < len(allowed_urls):
                    selected_urls.append(allowed_urls[idx]['url'])
                else:
                    print(f"âš ï¸  ì˜ëª»ëœ ë²ˆí˜¸: {idx + 1}")
            
            # ìµœëŒ€ ì œí•œ í™•ì¸
            if len(selected_urls) > MAX_CRAWL_LIMIT:
                print(f"âš ï¸  ì„ íƒí•œ ì‚¬ì´íŠ¸ê°€ {MAX_CRAWL_LIMIT}ê°œë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤.")
                selected_urls = selected_urls[:MAX_CRAWL_LIMIT]
                print(f"   ìƒìœ„ {MAX_CRAWL_LIMIT}ê°œ ì‚¬ì´íŠ¸ë§Œ ì„ íƒë©ë‹ˆë‹¤.")
            
            if selected_urls:
                print(f"âœ… ì„ íƒëœ ì‚¬ì´íŠ¸ {len(selected_urls)}ê°œ:")
                for i, url in enumerate(selected_urls, 1):
                    domain = urlparse(url).netloc
                    print(f"  {i}. {domain}")
                return selected_urls
            else:
                print("âŒ ì„ íƒëœ ì‚¬ì´íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì„ íƒí•´ì£¼ì„¸ìš”.")
                
        except (ValueError, KeyboardInterrupt):
            print("âŒ ì˜ëª»ëœ ì…ë ¥ì…ë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")

def clean_html_parallel(urls):
    start_time = time.time()
    print(f"\n[+] ë³‘ë ¬ ì²˜ë¦¬ë¡œ {len(urls)}ê°œ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì‹œì‘...")
    
    if ENABLE_PARALLEL:
        results = []
        with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            # URLê³¼ ì¸ë±ìŠ¤ë¥¼ í•¨ê»˜ ì „ë‹¬
            future_to_url = {executor.submit(clean_html_worker, (url, i)): url for i, url in enumerate(urls)}
            
            for future in concurrent.futures.as_completed(future_to_url):
                result = future.result()
                results.append(result)
    else:
        # ìˆœì°¨ ì²˜ë¦¬
        results = []
        for i, url in enumerate(urls):
            result = clean_html_worker((url, i))
            results.append(result)
    
    total_time = time.time() - start_time
    print(f"[+] í¬ë¡¤ë§ ì™„ë£Œ (ì´ ì†Œìš”ì‹œê°„: {total_time:.2f}ì´ˆ)")
    
    return results

def filter_noise(text):
    ad_patterns = [
        r"ë°°ë„ˆ\s?(ê´‘ê³ |í´ë¦­)", r"ê´‘ê³ ë¬¸ì˜", r"ë§ˆì¼€íŒ…\s?ë¬¸ì˜",
        r"ì œíœ´\s?(ë¬¸ì˜|ë§í¬)", r"êµ¬ë§¤\s?ë§í¬", r"í”„ë¡œëª¨ì…˜", r"ìŠ¤í°ì„œ", r"ê´‘ê³ \s?ìˆ˜ìµ",
        r"í›„ì›\s?(ê³„ì¢Œ|ë§í¬|ë¬¸ì˜|í•´ì£¼ì‹œë©´|ë°›ìŠµë‹ˆë‹¤|ë°”ëë‹ˆë‹¤)", r"ì•„ë˜.*í›„ì›", r"í›„ì›í•´\s?ì£¼ì„¸ìš”",
        r"í˜‘ì°¬\s?(ë¬¸ì˜|ë§í¬|í•´ì£¼ì‹œë©´)", r"ì¿ íŒ¡\s?íŒŒíŠ¸ë„ˆìŠ¤", r"êµ¬ë§¤ë§í¬",
        r"ì´ ê¸€ì€ .*? ê´‘ê³ ë¥¼ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤",
        r"ì´ í¬ìŠ¤íŠ¸ëŠ” .*? í›„ì›ì„ ë°›ê³  ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤",
        r"ê´‘ê³ ì„± ë¬¸êµ¬", r"ìœ ë£Œ ê´‘ê³ ", r"ì œíœ´ ë§ˆì¼€íŒ…", r"ì²´í—˜ë‹¨",
        r"Sponsored by", r"ì´ë²¤íŠ¸ ì°¸ì—¬", r"ì´ë²¤íŠ¸ ì•ˆë‚´", r"ì±„ë„ ê°€ì…"
    ]
    ad_regex = re.compile("|".join(ad_patterns), re.IGNORECASE)
    lines = text.split('\n')
    filtered = []
    for line in lines:
        line_stripped = line.strip()
        # 1. ë„ˆë¬´ ì§§ì€ ì¤„ ì œê±°
        if len(line_stripped) <= 30:
            continue
        # 2. ê´‘ê³ /ìŠ¤íŒ¸ íŒ¨í„´ ì œê±°
        if ad_regex.search(line_stripped):
            continue
        # 3. ì´ë©”ì¼, ì˜¤í”ˆì±„íŒ…, ì—°ë½ì²˜ ë“± ì œê±°
        if re.search(r'([a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+)|(ì˜¤í”ˆì±„íŒ…)|(ì¹´í†¡)|(ì—°ë½ì²˜)|(ë¬¸ì˜:)', line_stripped):
            continue
        # 4. ì•ŒíŒŒë²³/ìˆ«ì ë¹„ìœ¨ì´ 80% ì´ìƒì´ë©´ ì œê±° (ë‚œìˆ˜/í•´ì‹œ/ì½”ë“œ ë“±)
        if len(line_stripped) > 10:
            ratio = sum(c.isalnum() for c in line_stripped) / len(line_stripped)
            if ratio > 0.8:
                continue
        filtered.append(line_stripped)
    return "\n".join(filtered)

def print_texts(text_list, url_list):
    """ìˆ˜ì§‘ëœ í…ìŠ¤íŠ¸ë¥¼ printë¬¸ìœ¼ë¡œ ì¶œë ¥"""
    print("\n" + "="*80)
    print("ğŸ“„ ìˆ˜ì§‘ëœ í…ìŠ¤íŠ¸ ë°ì´í„°")
    print("="*80)
    
    for i, (text, url) in enumerate(zip(text_list, url_list)):
        print(f"\n[ë¬¸ì„œ {i+1}] (URL: {url})")
        print("-" * 60)
        print(text)
        print("-" * 60)
    
    print(f"\n[+] ì´ {len(text_list)}ê°œ ë¬¸ì„œ ì¶œë ¥ ì™„ë£Œ")

def simple_text_search(texts, urls, query, k=5):
    """ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ í…ìŠ¤íŠ¸ ê²€ìƒ‰"""
    print(f"\n[+] \"{query}\"ë¡œ í…ìŠ¤íŠ¸ ê²€ìƒ‰ Top-{k}")
    
    # ê²€ìƒ‰ì–´ë¥¼ í‚¤ì›Œë“œë¡œ ë¶„ë¦¬
    keywords = query.lower().split()
    
    # ê° í…ìŠ¤íŠ¸ì— ëŒ€í•œ ì ìˆ˜ ê³„ì‚°
    scores = []
    for i, text in enumerate(texts):
        text_lower = text.lower()
        score = 0
        
        # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
        for keyword in keywords:
            if keyword in text_lower:
                score += text_lower.count(keyword)
        
        # í…ìŠ¤íŠ¸ ê¸¸ì´ë¡œ ì •ê·œí™” (ì§§ì€ í…ìŠ¤íŠ¸ì— ê°€ì¤‘ì¹˜)
        score = score / (len(text) / 1000 + 1)
        
        scores.append((score, i, text, urls[i]))
    
    # ì ìˆ˜ìˆœìœ¼ë¡œ ì •ë ¬
    scores.sort(reverse=True)
    
    # ìƒìœ„ kê°œ ê²°ê³¼ ì¶œë ¥
    for i, (score, doc_idx, text, url) in enumerate(scores[:k]):
        if score > 0:
            print(f"\n--- ê²°ê³¼ {i+1} (ì ìˆ˜: {score:.2f}) ---")
            print(f"URL: {url}")
            print(f"ë‚´ìš©: {text[:400]}...")
        else:
            break
    
    if not any(score > 0 for score, _, _, _ in scores[:k]):
        print("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

def analyze_failures(crawl_results):
    """ì‹¤íŒ¨ ì›ì¸ ë¶„ì„"""
    if not SHOW_FAILURE_ANALYSIS:
        return
        
    print("\n====== [ì‹¤íŒ¨ ì›ì¸ ë¶„ì„] ======")
    
    success_count = sum(1 for r in crawl_results if r['success'])
    failure_count = len(crawl_results) - success_count
    
    print(f"ì„±ê³µ: {success_count}ê°œ, ì‹¤íŒ¨: {failure_count}ê°œ")
    
    if failure_count > 0:
        error_types = defaultdict(int)
        for result in crawl_results:
            if not result['success']:
                error_types[result['error']] += 1
        
        print("\nì‹¤íŒ¨ ì›ì¸ë³„ í†µê³„:")
        for error, count in error_types.items():
            print(f"- {error}: {count}ê°œ")
    
    print("===============================")

if __name__ == "__main__":
    total_start_time = time.time()
    
    # ì„¤ì • í™•ì¸
    print(f"=== ì„¤ì • ì •ë³´ ===")
    print(f"ìµœëŒ€ ì‚¬ì´íŠ¸ ê°œìˆ˜: {MAX_SITES}")
    print(f"ìµœëŒ€ í¬ë¡¤ë§ ê°œìˆ˜: {MAX_CRAWL_LIMIT}ê°œ")
    print(f"íƒ€ì„ì•„ì›ƒ: {TIMEOUT_SECONDS}ì´ˆ")
    print(f"ë™ì‹œ ì²˜ë¦¬ ìŠ¤ë ˆë“œ: {MAX_WORKERS}ê°œ")
    print(f"ìµœì†Œ í…ìŠ¤íŠ¸ ê¸¸ì´: {MIN_TEXT_LENGTH}ì")
    print(f"robots.txt í™•ì¸: {'í™œì„±í™”' if ROBOTS_CHECK_ENABLED else 'ë¹„í™œì„±í™”'}")
    print("================")
    
    query = input("ê²€ìƒ‰í•  ì£¼ì œë¥¼ ì…ë ¥í•˜ì„¸ìš”: ").strip()
    
    # 1ë‹¨ê³„: ë§í¬ ìˆ˜ì§‘
    print(f"\n=== 1ë‹¨ê³„: ë§í¬ ìˆ˜ì§‘ ===")
    urls = get_links(query, num=MAX_SITES * SEARCH_MULTIPLIER)  # ì—¬ìœ ìˆê²Œ ê²€ìƒ‰
    
    if not urls:
        print("[-] ë§í¬ë¥¼ ìˆ˜ì§‘í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        exit()
    
    print(f"[+] ìˆ˜ì§‘ëœ URL ê°œìˆ˜: {len(urls)}ê°œ")
    
    # 1.5ë‹¨ê³„: robots.txt í™•ì¸ ë° í•„í„°ë§
    if ROBOTS_CHECK_ENABLED:
        print(f"\n=== 1.5ë‹¨ê³„: robots.txt í™•ì¸ ===")
        robots_results = check_robots_for_urls(urls)
        filtered_urls = filter_urls_by_robots(robots_results)
        
        if not filtered_urls:
            print("[-] ì²˜ë¦¬í•  ì‚¬ì´íŠ¸ê°€ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            exit()
        
        urls = filtered_urls
        print(f"[+] í•„í„°ë§ í›„ URL ê°œìˆ˜: {len(urls)}ê°œ")
    
    # 2ë‹¨ê³„: ë³‘ë ¬ í¬ë¡¤ë§
    print(f"\n=== 2ë‹¨ê³„: ì›¹ í¬ë¡¤ë§ ===")
    crawl_results = clean_html_parallel(urls)
    
    # 3ë‹¨ê³„: í…ìŠ¤íŠ¸ í•„í„°ë§ ë° ì €ì¥
    print(f"\n=== 3ë‹¨ê³„: í…ìŠ¤íŠ¸ í•„í„°ë§ ===")
    texts = []
    used_urls = []
    failed_urls = []
    
    domain_count_raw = defaultdict(int)
    domain_count_final = defaultdict(int)
    
    print(f"[+] í¬ë¡¤ë§ ê²°ê³¼ ë¶„ì„ ì¤‘...")
    for i, result in enumerate(crawl_results):
        url = result['url']
        domain = urlparse(url).netloc
        domain_count_raw[domain] += 1
        
        if result['success']:
            filtered = filter_noise(result['text'])
            if len(filtered) > MIN_TEXT_LENGTH:
                texts.append(filtered)
                used_urls.append(url)
                domain_count_final[domain] += 1
                print(f"[{i+1:2d}] âœ“ ì„±ê³µ: {domain} (í…ìŠ¤íŠ¸ ê¸¸ì´: {len(filtered)}ì)")
            else:
                failed_urls.append((url, f"í…ìŠ¤íŠ¸ ê¸¸ì´ ë¶€ì¡± ({len(filtered)}ì)"))
                print(f"[{i+1:2d}] âœ— ì‹¤íŒ¨: {domain} (í…ìŠ¤íŠ¸ ê¸¸ì´: {len(filtered)}ì)")
        else:
            failed_urls.append((url, result['error']))
            print(f"[{i+1:2d}] âœ— ì‹¤íŒ¨: {domain} ({result['error']})")
    
    print(f"[+] í•„í„°ë§ ì™„ë£Œ: {len(texts)}ê°œ ë¬¸ì„œ í†µê³¼")
    
    # ì‹¤íŒ¨ ì›ì¸ ë¶„ì„
    analyze_failures(crawl_results)
    
    if not texts:
        print("[-] ì¶©ë¶„í•œ ë¬¸ì„œë¥¼ ìˆ˜ì§‘í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        exit()
    
    # 4ë‹¨ê³„: í…ìŠ¤íŠ¸ ì¶œë ¥
    print_texts(texts, used_urls)
    
    total_time = time.time() - total_start_time
    
    print("\n====== [ìˆ˜ì§‘ ìš”ì•½] ======")
    print(f"ì´ {len(urls)}ê°œ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì‹œë„, {len(texts)}ê°œ ë¬¸ì„œ í•„í„° í†µê³¼ ë° ì¶œë ¥")
    print(f"ì „ì²´ ì†Œìš”ì‹œê°„: {total_time:.2f}ì´ˆ")
    print("== ì‚¬ì´íŠ¸ë³„ (ì‹œë„ â†’ ìµœì¢… ì¶œë ¥):")
    for domain in sorted(domain_count_raw, key=lambda d: -domain_count_final[d]):
        tried = domain_count_raw[domain]
        saved = domain_count_final[domain]
        print(f"- {domain}: {tried}ê°œ ì‹œë„ â†’ {saved}ê°œ ì¶œë ¥")
    
    if failed_urls:
        print("\n== ì‹¤íŒ¨í•œ ì‚¬ì´íŠ¸ë“¤:")
        for url, reason in failed_urls[:MAX_FAILURE_DISPLAY]:  # ì„¤ì •ëœ ê°œìˆ˜ë§Œí¼ í‘œì‹œ
            print(f"- {url}: {reason}")
        if len(failed_urls) > MAX_FAILURE_DISPLAY:
            print(f"... ì™¸ {len(failed_urls) - MAX_FAILURE_DISPLAY}ê°œ") 
    
    print("=========================")
    
    # 5ë‹¨ê³„: ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ê²€ìƒ‰
    while True:
        q = input('\nê²€ìƒ‰í•  í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”(ì—”í„°ë§Œ ëˆ„ë¥´ë©´ ì¢…ë£Œ): ').strip()
        if not q:
            break
        simple_text_search(texts, used_urls, q, k=5)    