from moviepy import (
    ImageClip, VideoFileClip, AudioFileClip, concatenate_videoclips,
    CompositeVideoClip, TextClip, ColorClip, CompositeAudioClip
)
import os
import random
import subprocess
import numpy as np
from moviepy.audio.AudioClip import AudioArrayClip, concatenate_audioclips
import gc
import imageio_ffmpeg
# ìƒë‹¨ ì„í¬íŠ¸ ê·¼ì²˜
try:
    from moviepy.audio.fx import audio_loop          # moviepy 2.x
except Exception:
    try:
        from moviepy.audio.fx.all import audio_loop  # moviepy 1.x
    except Exception:
        audio_loop = None

def _st(msg):
    try:
        import streamlit as st
        st.write(msg)
    except Exception:
        print(msg)

def _with_audio_compat(video, audio):
    try:
        return video.with_audio(audio)   # moviepy 2.x
    except AttributeError:
        return video.set_audio(audio)    # moviepy 1.x
    
def _loop_audio_manual(clip, duration):
    rep = int(np.ceil(duration / max(clip.duration, 0.1)))
    looped = concatenate_audioclips([clip] * max(1, rep))
    return looped.subclip(0, duration)

def create_motion_clip(img_path, duration, width, height):
    base_clip_original_size = ImageClip(img_path)

    # ì´ë¯¸ì§€ ì´ˆê¸° ë¦¬ì‚¬ì´ì§• ì „ëµ ë³€ê²½: 'ì»¤ë²„' ë°©ì‹ìœ¼ë¡œ í•­ìƒ í™”ë©´ì„ ê°€ë“ ì±„ì›€
    scale_w = width / base_clip_original_size.w
    scale_h = height / base_clip_original_size.h
    
    # ë‘ ìŠ¤ì¼€ì¼ ì¤‘ ë” í° ê°’ì„ ì„ íƒí•˜ì—¬, ì´ë¯¸ì§€ê°€ ë¹„ë””ì˜¤ í”„ë ˆì„ì„ ì™„ì „íˆ ë®ë„ë¡ í•¨
    scale_factor_cover = max(scale_w, scale_h)
    
    # Roundë¥¼ ì‚¬ìš©í•˜ì—¬ ì†Œìˆ˜ì  ì²˜ë¦¬ ì˜¤ë¥˜ ë°©ì§€, ìµœì†Œ í¬ê¸° 1 ë³´ì¥
    resized_w_cover = max(1, round(base_clip_original_size.w * scale_factor_cover))
    resized_h_cover = max(1, round(base_clip_original_size.h * scale_factor_cover))

    # ìƒˆë¡œìš´ 'ì»¤ë²„' ë°©ì‹ìœ¼ë¡œ ë¦¬ì‚¬ì´ì¦ˆëœ base_clip ìƒì„±
    base_clip = base_clip_original_size.resized((resized_w_cover, resized_h_cover)).with_duration(duration)

    clip_width = base_clip.w
    clip_height = base_clip.h

    motion_type = random.choice(["zoom_in_out", "left_to_right", "right_to_left", "static"])

    # ì¤‘ì•™ ì •ë ¬ì„ ìœ„í•œ ê¸°ë³¸ ìœ„ì¹˜
    center_x = round((width - clip_width) / 2)
    center_y = round((height - clip_height) / 2)

    if motion_type == "zoom_in_out":
        start_scale = 1.0
        end_scale = 1.05
        scale_diff = end_scale - start_scale

        # í™”ë©´ì— ê½‰ ì±„ìš°ë„ë¡ ì»¤ë²„ ë¦¬ì‚¬ì´ì¦ˆ ë¨¼ì € ì§„í–‰
        scale_w = width / base_clip_original_size.w
        scale_h = height / base_clip_original_size.h
        base_scale = max(scale_w, scale_h)

        base_clip = base_clip_original_size.resized(base_scale).with_duration(duration)

        def zoom_factor(t):
            return start_scale + scale_diff * (t / duration)

        def position(t):
            scale = zoom_factor(t)
            w_scaled = base_clip.w * scale
            h_scaled = base_clip.h * scale
            x = round((width - w_scaled) / 2)
            y = round((height - h_scaled) / 2)
            # yê°€ ìŒìˆ˜ë©´ 0ìœ¼ë¡œ ë³´ì •í•´ì„œ ì•„ë˜ ê²€ì€ ì—¬ë°± ë°©ì§€
            if y < 0:
                y = 0
            return (x, y)

        zoomed_clip = base_clip.resized(zoom_factor).with_position(position)

        return zoomed_clip


    elif motion_type == "left_to_right":
        if clip_width > width:
            max_move = clip_width - width

            # ğŸ’¡ ì°¨ì´ê°€ ì•„ì£¼ ì‘ìœ¼ë©´ ê±°ì˜ ê³ ì • (ì˜ˆ: 30í”½ì…€ ì´í•˜)
            if max_move < 30:
                return base_clip.with_position((center_x, center_y))

            start_ratio = 0.3
            end_ratio = 0.6
            move_ratio = end_ratio - start_ratio

            start_offset = -max_move * start_ratio
            move_distance = max_move * move_ratio

            def ease_in_out(t):
                progress = t / duration
                return 3 * (progress ** 2) - 2 * (progress ** 3)

            def position(t):
                eased = ease_in_out(t)
                x = round(start_offset + move_distance * eased)
                y = center_y
                return (x, y)

            return base_clip.with_position(position)
        else:
            return base_clip.with_position((center_x, center_y))


    elif motion_type == "right_to_left":
        if clip_width > width:
            max_move = clip_width - width

            # ğŸ’¡ ì°¨ì´ê°€ ì•„ì£¼ ì‘ìœ¼ë©´ ê±°ì˜ ê³ ì •
            if max_move < 30:
                return base_clip.with_position((center_x, center_y))

            start_ratio = 0.3
            end_ratio = 0.6
            move_ratio = end_ratio - start_ratio

            start_offset = -max_move * end_ratio
            move_distance = max_move * move_ratio

            def ease_in_out(t):
                progress = t / duration
                return 3 * (progress ** 2) - 2 * (progress ** 3)

            def position(t):
                eased = ease_in_out(t)
                x = round(start_offset - move_distance * eased)
                y = center_y
                return (x, y)

            return base_clip.with_position(position)
        else:
            return base_clip.with_position((center_x, center_y))


    else: # "static" (ê³ ì •)
        # ì´ë¯¸ì§€ê°€ í”„ë ˆì„ ì¤‘ì•™ì— ê³ ì •ë˜ì–´ ì˜ë¦¬ì§€ ì•Šê³  ë³´ì—¬ì§‘ë‹ˆë‹¤.
        # round()ë¥¼ ì‚¬ìš©í•˜ì—¬ ì†Œìˆ˜ì  ì²˜ë¦¬ ì˜¤ë¥˜ ë°©ì§€
        return base_clip.with_position((center_x, center_y))
    
def auto_split_title(text: str, max_first_line_chars=18):
    words = text.split()
    total_chars = sum(len(w) for w in words)
    target = total_chars // 2

    char_count = 0
    split_idx = None
    for i, word in enumerate(words):
        char_count += len(word)
        if (char_count >= target or char_count >= max_first_line_chars) and i < len(words) - 1:
            split_idx = i + 1
            break

    if split_idx is None:
        return text, ""  # í•œ ì¤„
    return " ".join(words[:split_idx]), " ".join(words[split_idx:])

# âœ… ì˜ìƒ ìƒì„± ë©”ì¸ í•¨ìˆ˜ (size=None ì „ë‹¬ ê¸ˆì§€ ì²˜ë¦¬ í¬í•¨)
def create_video_with_segments(
    image_paths,
    segments,
    audio_path,
    topic_title,
    include_topic_title=True,
    bgm_path="",
    save_path="assets/video.mp4",
    ass_path=None,   # (í˜¸í™˜ìš©) ì—¬ê¸°ì„  ì‚¬ìš©í•˜ì§€ ì•ŠìŒ. ìë§‰ì€ mainì—ì„œ add_subtitles_to_videoë¡œ ë²ˆì¸.
):
    W, H = 720, 1080
    clips = []
    total_dur = segments[-1]['end'] if segments else 10.0

    # ---------- ë‚´ë¶€ í—¬í¼ë“¤ ----------
    def _normalize_image_paths(paths, n_needed):
        paths = list(paths or [])
        if len(paths) < n_needed:
            last_valid = next((p for p in reversed(paths) if p and os.path.exists(p)), None)
            paths += [last_valid] * (n_needed - len(paths))
        elif len(paths) > n_needed:
            paths = paths[:n_needed]
        return [p if (p and os.path.exists(p)) else None for p in paths]

    def _build_text_clip(text: str, font_path: str, font_size: int, max_width: int):
        try:
            clip = TextClip(
                text=text + "\n",
                font=font_path, font_size=font_size,
                color="white",
                stroke_color="skyblue", stroke_width=1,
                method="caption", size=(max_width, None),
                align="center",
            )
            return clip, True
        except TypeError:
            clip = TextClip(
                text=text + "\n",
                font=font_path, font_size=font_size,
                color="white",
                method="label",
            )
            return clip, False
        except Exception:
            return None, False

    def _measure_text_h(text: str, font_path: str, font_size: int, max_width: int, used_caption: bool):
        try:
            if used_caption:
                dummy = TextClip(text=text, font=font_path, font_size=font_size, method="caption", size=(max_width, None))
            else:
                dummy = TextClip(text=text, font=font_path, font_size=font_size, method="label")
            h = dummy.h
            try: dummy.close()
            except: pass
            return h
        except Exception:
            return 0

    def auto_split_title(text: str, max_first_line_chars=18):
        words = text.split()
        total = sum(len(w) for w in words)
        target = max_first_line_chars if total > max_first_line_chars*2 else (total // 2 or total)
        acc = 0
        for i, w in enumerate(words[:-1]):
            acc += len(w)
            if acc >= target:
                return " ".join(words[:i+1]), " ".join(words[i+1:])
        return text, ""

    def _fallback_motion_clip(img_path, duration, width, height):
        try:
            base = ImageClip(img_path)
            scale = max(width / base.w, height / base.h)
            base = base.resized(scale).with_duration(duration)
            cx = round((width - base.w) / 2)
            cy = round((height - base.h) / 2)
            return base.with_position((cx, cy))
        except Exception:
            return ColorClip(size=(width, height), color=(0, 0, 0)).with_duration(duration)

    # ---------- ìŒì„±(ë‚´ë ˆì´ì…˜) ----------
    narration = None
    if audio_path and os.path.exists(audio_path):
        try:
            narration = AudioFileClip(audio_path)
        except Exception as e:
            print(f"âš ï¸ ë‚´ë ˆì´ì…˜ ë¡œë“œ ì‹¤íŒ¨: {e}")
            narration = None

    # ---------- ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸ ì •ë¦¬ ----------
    image_paths = _normalize_image_paths(image_paths, len(segments))

    # ---------- íƒ€ì´í‹€(ì˜µì…˜) ----------
    title_clip_proto, used_caption, title_bar_h = None, False, 0
    title_text = (topic_title or "").strip()
    if include_topic_title and title_text:
        font_path = os.path.join("assets", "fonts", "BMJUA_ttf.ttf")
        l1, l2 = auto_split_title(title_text)
        full_title = l1 + ("\n" + l2 if l2 else "")
        title_clip_proto, used_caption = _build_text_clip(full_title, font_path, 32, W - 40)
        if title_clip_proto is not None:
            title_bar_h = _measure_text_h(full_title, font_path, 32, W - 40, used_caption) + 32
        else:
            title_bar_h = 0

    # ---------- ì„¸ê·¸ë¨¼íŠ¸ë³„ í•©ì„± ----------
    for i, seg in enumerate(segments):
        start = seg['start']
        dur   = max(0.1, seg['end'] - start)

        img_path = image_paths[i]
        if img_path is None:
            base = ColorClip(size=(W, H), color=(0, 0, 0)).with_duration(dur)
        else:
            try:
                base = create_motion_clip(img_path, dur, W, H)  # í”„ë¡œì íŠ¸ì— ìˆìœ¼ë©´ ì‚¬ìš©
            except NameError:
                base = _fallback_motion_clip(img_path, dur, W, H)
            except Exception:
                base = ColorClip(size=(W, H), color=(0, 0, 0)).with_duration(dur)

        overlays = [base]

        if title_clip_proto is not None:
            title_clip = title_clip_proto.with_duration(dur)
            black_bar  = ColorClip(size=(W, int(title_bar_h)), color=(0, 0, 0)).with_duration(dur).with_position(("center","top"))
            tx = int(round((W - title_clip.w) / 2))
            ty = int(max(0, min(round((title_bar_h - title_clip.h) / 2) + 10, title_bar_h - title_clip.h)))
            overlays += [black_bar, title_clip.with_position((tx, ty))]

        seg_clip = CompositeVideoClip(overlays, size=(W, H)).with_duration(dur)
        clips.append(seg_clip)

    # ---------- BGM ì„ íƒ ----------
    chosen_bgm = bgm_path if (bgm_path and os.path.exists(bgm_path)) else None
    target_duration = narration.duration if narration else total_dur

    # ğŸ”§ pydubë¡œ ë¯¸ë¦¬ ë¯¹ìŠ¤(ë³´ì´ìŠ¤ ì—†ì–´ë„ BGMë§Œ ê¸¸ì´ì— ë§ì¶° ê¹”ë¦¼)
    mixed_path = os.path.join(os.path.dirname(save_path) or ".", "_mix_audio.mp3")
    final_audio = None
    try:
        _mix_voice_and_bgm(
            voice_path=(audio_path if (audio_path and os.path.exists(audio_path)) else None),
            bgm_path=chosen_bgm,
            out_path=mixed_path,
            bgm_gain_db=-30, #BGM ì†Œë¦¬ í¬ê¸°     
            add_tail_ms=250
        )
        final_audio = AudioFileClip(mixed_path)
    except Exception as e:
        print(f"âš ï¸ pre-mix ì‹¤íŒ¨ â†’ ì¦‰ì„ ë¯¹ìŠ¤ë¡œ í´ë°±: {e}")
        # â”€ í´ë°±: MoviePyë§Œìœ¼ë¡œ ì•ˆì „í•˜ê²Œ ë¯¹ìŠ¤
        try:
            import math
            parts = []
            if narration is not None:
                parts.append(narration)
            if chosen_bgm and os.path.exists(chosen_bgm):
                bgm_raw = AudioFileClip(chosen_bgm)
                need = target_duration if narration is None else narration.duration
                rep = int(math.ceil(need / max(bgm_raw.duration, 0.1)))
                bgm_tiled = concatenate_audioclips([bgm_raw] * max(1, rep)).subclip(0, need).volumex(0.15)
                parts.append(bgm_tiled)
            if parts:
                final_audio = CompositeAudioClip(parts)
        except Exception as ee:
            print(f"âš ï¸ í´ë°± ë¯¹ìŠ¤ë„ ì‹¤íŒ¨: {ee}")
            final_audio = narration  # ê·¸ë˜ë„ ë³´ì´ìŠ¤ëŠ” ìœ ì§€

    # ---------- íŒŒì¼ ì“°ê¸° ----------
    os.makedirs(os.path.dirname(save_path) or ".", exist_ok=True)
    tmp_out = os.path.join(os.path.dirname(save_path) or ".", "_temp_no_subs.mp4")

    video = concatenate_videoclips(clips, method="chain").with_fps(24)
    if final_audio is not None:
        video = _with_audio_compat(video, final_audio)

    video.write_videofile(
        tmp_out,
        codec="libx264",
        audio_codec="aac",
        audio_bitrate="192k"
    )

    try: video.close()
    except: pass
    try:
        for c in clips: c.close()
    except: pass
    if narration: 
        try: narration.close()
        except: pass
    gc.collect()

    if tmp_out != save_path:
        try:
            os.replace(tmp_out, save_path)
        except Exception:
            pass

    print(f"âœ… (ìë§‰ ë¯¸ì ìš©) ì˜ìƒ ì €ì¥ ì™„ë£Œ: {save_path}")
    return save_path


ffmpeg_path = imageio_ffmpeg.get_ffmpeg_exe()

# âœ… ìë§‰ ì¶”ê°€ í•¨ìˆ˜
def add_subtitles_to_video(input_video_path, ass_path, output_path):
    import subprocess, shlex, os
    fonts_dir = os.path.abspath(os.path.join("assets", "fonts"))
    # ê²½ë¡œì— ê³µë°±/ì—­ìŠ¬ë˜ì‹œê°€ ìˆì–´ë„ ì•ˆì „í•˜ê²Œ
    ass_q = ass_path.replace("\\", "/")
    fonts_q = fonts_dir.replace("\\", "/")

    cmd = [
        "ffmpeg", "-y",
        "-i", input_video_path,
        "-vf", f"ass='{ass_q}':fontsdir='{fonts_q}'",
        "-c:v", "libx264",
        "-c:a", "aac", "-b:a", "192k",
        # â˜… ë¹„ë””ì˜¤/ì˜¤ë””ì˜¤ ëª¨ë‘ ìœ ì§€(ì˜¤ë””ì˜¤ ì—†ìœ¼ë©´ ë¬´ì‹œ)
        "-map", "0:v:0", "-map", "0:a?", 
        output_path
    ]
    subprocess.run(cmd, check=True)
    return output_path

from pydub import AudioSegment
import math, os

def _mix_voice_and_bgm(voice_path: str | None, bgm_path: str | None, out_path: str,
                       bgm_gain_db: float = 6, add_tail_ms: int = 250) -> str | None:
    """
    - voiceë§Œ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ë³µì‚¬(ê¼¬ë¦¬ ë¬´ìŒ ì¶”ê°€)
    - bgmë§Œ ìˆìœ¼ë©´ ê¸¸ì´ì— ë§ì¶° ìë¥´ê³  ë‚´ë³´ëƒ„
    - ë‘˜ ë‹¤ ìˆìœ¼ë©´ voice ê¸¸ì´ì— bgmì„ ë£¨í”„/íŠ¸ë¦¼í•´ì„œ -18dBë¡œ ê¹”ê³  overlay
    """
    if not voice_path and not bgm_path:
        return None

    voice = AudioSegment.silent(duration=0)
    bgm   = AudioSegment.silent(duration=0)

    if voice_path and os.path.exists(voice_path):
        voice = AudioSegment.from_file(voice_path)
    if bgm_path and os.path.exists(bgm_path):
        bgm = AudioSegment.from_file(bgm_path)

    if len(voice) == 0 and len(bgm) == 0:
        return None

    if len(voice) == 0:
        # ë³´ì´ìŠ¤ê°€ ì—†ìœ¼ë©´ BGMë§Œ íŠ¸ë¦¼
        out = bgm[:]
        out.export(out_path, format="mp3")
        return out_path

    # ë³´ì´ìŠ¤ê°€ ìˆìœ¼ë©´ ê¸¸ì´ì— ë§ì¶° BGMì„ ë£¨í”„/íŠ¸ë¦¼í•˜ê³  ê°ì‡ 
    target_len = len(voice) + add_tail_ms
    if len(bgm) == 0:
        bed = AudioSegment.silent(duration=target_len)
    else:
        rep = math.ceil(target_len / len(bgm))
        bed = (bgm * max(1, rep))[:target_len]
        bed = bed + bgm_gain_db  # ìŒëŸ‰ ê°ì‡ (ì˜ˆ: -18dB)

    mixed = bed.overlay(voice)  # ë³´ì´ìŠ¤ë¥¼ ìœ„ì— ì–¹ëŠ”ë‹¤
    mixed.export(out_path, format="mp3")
    return out_path

def create_dark_text_video(script_text, title_text, audio_path=None, bgm_path="", save_path="assets/dark_text_video.mp4"):
    video_width, video_height = 720, 1080
    font_path = os.path.abspath(os.path.join("assets", "fonts", "BMJUA_ttf.ttf"))
    if not os.path.exists(font_path):
        raise FileNotFoundError(f"í°íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤: {font_path}")

    if audio_path and os.path.exists(audio_path):
        audio = AudioFileClip(audio_path)
        duration = audio.duration
    else:
        duration = 2
        audio = AudioArrayClip(np.array([[0.0, 0.0]]), fps=44100).with_duration(duration)

    bg_clip = ColorClip(size=(video_width, video_height), color=(0, 0, 0)).with_duration(duration)

    # ===== ë ˆì´ì•„ì›ƒ ìƒìˆ˜ =====
    TOP_MARGIN = 150
    BOTTOM_MARGIN = 80
    SAFE_BOTTOM_PAD = 24
    SAFE_SIDE_PAD = 24
    LEFT_BLEED_PAD = 12
    CONTENT_WIDTH = video_width - SAFE_SIDE_PAD * 2

    # ===== ì œëª© 2ì¤„ + ë§ì¤„ì„ =====
    def ellipsize_two_lines(text, max_chars_per_line=20):
        if not text: return ""
        import textwrap
        wrapped = textwrap.wrap(text.strip(), width=max_chars_per_line, break_long_words=True, break_on_hyphens=False)
        if len(wrapped) <= 2: return "\n".join(wrapped)
        out = wrapped[:2]
        out[1] = (out[1].rstrip()[:-1] + "â€¦") if len(out[1].rstrip()) > 0 else "â€¦"
        return "\n".join(out)

    title_text = ellipsize_two_lines(title_text or "", max_chars_per_line=18)

    # ===== í­ ì¸¡ì • ìœ í‹¸(ë˜í•‘/í­ ê³„ì‚°ë§Œ labelë¡œ ì‚¬ìš©) =====
    def line_width(s: str, fs: int) -> int:
        if not s: return 0
        c = TextClip(text=s, font=font_path, font_size=fs, method="label")
        w = c.w
        c.close()
        return w

    # ë‹¨ì–´ ë‹¨ìœ„ ë˜í•‘
    def wrap_to_width(text: str, max_w: int, fs: int):
        words = text.split()
        lines, cur = [], ""
        for w in words:
            test = (cur + " " + w).strip()
            if not cur or line_width(test, fs) <= max_w:
                cur = test
            else:
                lines.append(cur); cur = w
        if cur: lines.append(cur)
        return lines if lines else [""]

    # ì…ë ¥ ì¤„ë°”ê¿ˆ ë³´ì¡´ + ë¸”ë¡ë³„ ë˜í•‘
    def wrap_preserving_newlines(text: str, max_w: int, fs: int):
        out = []
        for block in (text or "").splitlines():
            if block.strip() == "":
                out.append("")              # ë¹ˆ ì¤„ ìœ ì§€
            else:
                out.extend(wrap_to_width(block, max_w, fs))
        return out

    # ì œëª© ê°€ìš´ë° ë§ì¶¤(ì‹œê°ì )
    def center_label_multiline(raw_text: str, max_w: int, fs: int, pad_char="\u00A0"):
        blocks = raw_text.split("\n")
        lines = [b if b.strip() else "" for b in blocks]
        def _w(s):
            if not s: return 0
            c = TextClip(text=s, font=font_path, font_size=fs, method="label")
            w = c.w; c.close(); return w
        maxw = max((_w(l) for l in lines), default=0)
        spacew = max(_w(pad_char), 1)
        centered = []
        for l in lines:
            lw = _w(l)
            pad = int(round((maxw - lw) / (2 * spacew))) if maxw > lw else 0
            centered.append(pad_char * pad + l)
        return "\n".join(centered) + "\n"

    # ===== ì œëª© =====
    title_fontsize = 38
    max_title_width = CONTENT_WIDTH - 2 * LEFT_BLEED_PAD
    centered_title_text = center_label_multiline(title_text, max_title_width, title_fontsize)
    title_clip_tmp = TextClip(
        text=centered_title_text, font=font_path, font_size=title_fontsize, color="white", method="label"
    )
    title_h = title_clip_tmp.h
    title_y = TOP_MARGIN
    title_x = int(SAFE_SIDE_PAD + LEFT_BLEED_PAD + ((CONTENT_WIDTH - 2 * LEFT_BLEED_PAD) - title_clip_tmp.w) / 2)
    title_clip = title_clip_tmp.with_position((title_x, int(title_y))).with_duration(duration)

    # ===== ë³¸ë¬¸(ì™¼ìª½ ì •ë ¬ì²˜ëŸ¼ ë³´ì´ê²Œ + í•˜ë‹¨ ì˜ë¦¼ ë°©ì§€) =====
    GAP_TITLE_BODY = 32
    allowed_body_height = video_height - BOTTOM_MARGIN - (title_y + title_h + GAP_TITLE_BODY) - SAFE_BOTTOM_PAD

    if allowed_body_height <= 0:
        video = CompositeVideoClip([bg_clip, title_clip], size=(video_width, video_height)).with_duration(duration)
    else:
        body_fontsize  = 28
        body_width_px  = CONTENT_WIDTH

        LINE_GAP       = int(round(body_fontsize * 0.3))  # ì¤„ ì‚¬ì´ ì¶”ê°€ ê°„ê²©
        TOP_PAD_PX     = int(round(body_fontsize * 0.12))  # ì²« ì¤„ ìœ„ ì—¬ìœ 
        BOTTOM_PAD_PX  = int(round(body_fontsize * 0.25))  # ë§ˆì§€ë§‰ ì¤„ ì•„ë˜ ì—¬ìœ 
        DESCENDER_EXTRA = 2                                # ê° ì¤„ í•˜ë‹¨ ì—¬ìœ ìš© ë³´ì •(px)

        MIN_FONT_SIZE   = 14
        MIN_WIDTH_RATIO = 0.60
        min_width_px    = int(CONTENT_WIDTH * MIN_WIDTH_RATIO)

        # ì¢Œìš° 1.5 ê¸€ì ë‚´ë¶€ íŒ¨ë”©
        base_char_w = max(8, line_width("ê°€", body_fontsize), line_width("M", body_fontsize))
        INNER_PAD = int(round(base_char_w * 1.5))

        NBSP, HAIR = "\u00A0", "\u200A"

        def spacer(h):
            return ColorClip(size=(1, max(1, int(h))), color=(0, 0, 0)).with_opacity(0)

        def build_body(fs: int, width_px: int):
            eff_wrap_w = max(20, width_px - 2 * INNER_PAD - 2 * LEFT_BLEED_PAD)
            lines = wrap_preserving_newlines((script_text or "").rstrip(), eff_wrap_w, fs)

            clips = []
            y = TOP_PAD_PX
            maxw = 1

            for i, line in enumerate(lines):
                if line.strip() == "":
                    sg = spacer(fs + LINE_GAP)
                    clips.append(sg.with_position((0, y))); y += sg.h
                    continue

                # 1) ì´ ì¤„ì˜ ì‹¤ì œ í…ìŠ¤íŠ¸ í­ì„ labelë¡œ ì¸¡ì •
                plain_w = line_width(line, fs)
                # 2) ê°€ìš´ë° ì •ë ¬ì„ ë§‰ê¸° ìœ„í•´ caption ë°•ìŠ¤ í­ì„ "ì‹¤ì œ í…ìŠ¤íŠ¸í­ + ì—¬ìœ "ë¡œ ì„¤ì •
                cap_w = max(plain_w + 6, 10)

                # 3) í•˜ë‹¨ ì˜ë¦¼ ë°©ì§€ë¥¼ ìœ„í•´ ì¤„ ëì— "\n\u200A" ì¶”ê°€
                safe_line = NBSP + line + "\n" + HAIR

                c = TextClip(
                    text=safe_line,
                    font=font_path,
                    font_size=fs,
                    color="white",
                    method="label",
                    size=(cap_w, None),   # ì¤„ ê¸¸ì´ë§Œí¼ë§Œ ë°•ìŠ¤ ìƒì„± â†’ ì‹œê°ì  ì™¼ìª½ì •ë ¬
                    interline=0
                )
                clips.append(c.with_position((0, y)))
                y += c.h + DESCENDER_EXTRA
                maxw = max(maxw, c.w)

                if i < len(lines) - 1:
                    gap = spacer(LINE_GAP)
                    clips.append(gap.with_position((0, y))); y += gap.h

            total_h = y + BOTTOM_PAD_PX
            if not clips:
                return CompositeVideoClip([spacer(int(fs * 1.2)).with_position((0, 0))],
                                          size=(1, int(fs * 1.2))).with_duration(duration)

            return CompositeVideoClip(clips, size=(maxw, total_h)).with_duration(duration)

        # allowed_body_heightì— ë§ì¶° ì¡°ì •
        fit_clip = None
        for _ in range(80):
            body_label = build_body(body_fontsize, body_width_px)
            if body_label.h <= allowed_body_height:
                fit_clip = body_label
                break
            if body_fontsize > MIN_FONT_SIZE:
                body_fontsize = max(MIN_FONT_SIZE, body_fontsize - 2)
                base_char_w = max(8, line_width("ê°€", body_fontsize), line_width("M", body_fontsize))
                INNER_PAD = int(round(base_char_w * 1.5))
                continue
            if body_width_px > min_width_px:
                body_width_px = max(min_width_px, body_width_px - 10)
                continue
            scale = allowed_body_height / float(body_label.h)
            fit_clip = body_label.resized(scale)
            break

        if fit_clip is None:
            fit_clip = build_body(body_fontsize, body_width_px)

        # ì¢Œìš° 1.5ì íŒ¨ë”© ë˜í¼
        body_wrapper_w = fit_clip.w + 2 * INNER_PAD
        body_wrapper_h = fit_clip.h
        body_wrapper = CompositeVideoClip(
            [fit_clip.with_position((INNER_PAD, 0))],
            size=(body_wrapper_w, body_wrapper_h)
        ).with_duration(duration)

        # ì¤‘ì•™ ì •ë ¬(ë˜í¼ ê¸°ì¤€)
        body_x = int(SAFE_SIDE_PAD + ((CONTENT_WIDTH - body_wrapper.w) / 2))
        body_y = int(title_y + title_h + GAP_TITLE_BODY)
        body_clip = body_wrapper.with_position((body_x, body_y)).with_duration(duration)

        # í•˜ë‹¨ íˆ¬ëª… íŒ¨ë“œ
        pad_clip = ColorClip(size=(video_width, SAFE_BOTTOM_PAD), color=(0, 0, 0)).with_opacity(0) \
                   .with_duration(duration).with_position(("center", video_height - SAFE_BOTTOM_PAD))

        video = CompositeVideoClip([bg_clip, title_clip, body_clip, pad_clip],
                                   size=(video_width, video_height)).with_duration(duration)

    # ===== ì˜¤ë””ì˜¤ & ì €ì¥ =====
    final_audio = audio
    if bgm_path and os.path.exists(bgm_path):
        bgm = AudioFileClip(bgm_path).volumex(0.05).with_duration(duration)
        final_audio = CompositeAudioClip([audio, bgm])

    final_video = video.with_audio(final_audio).with_fps(24)
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    final_video.write_videofile(save_path, codec="libx264", audio_codec="aac")
    return save_path

def create_video_from_videos(
    video_paths,
    segments,
    audio_path,
    topic_title,
    include_topic_title=True,
    bgm_path="",
    save_path="assets/video_from_videos.mp4",
):
    """
    ì—¬ëŸ¬ ì†ŒìŠ¤ ë™ì˜ìƒì„ ì„¸ê·¸ë¨¼íŠ¸ ê¸¸ì´ì— ë§ì¶° ìë¥´ê³ (ë¶€ì¡±í•˜ë©´ ë°˜ë³µ),
    ìƒë‹¨ íƒ€ì´í‹€ ì˜¤ë²„ë ˆì´(ì„ íƒ)ë¥¼ ì–¹ì€ ë’¤, ë‚´ë ˆì´ì…˜/ë°°ê²½ìŒì•…ì„ ë¯¹ìŠ¤í•´
    ìµœì¢… ì˜ìƒì„ ì €ì¥í•©ë‹ˆë‹¤.

    - MoviePyì˜ TextClip(method="label")ì— size=Noneì„ ë„˜ê¸°ì§€ ì•Šë„ë¡ ì•ˆì „ ì²˜ë¦¬.
    - caption ê°€ëŠ¥í•˜ë©´ captionì„ ì‚¬ìš©(size=(W,None)), ì‹¤íŒ¨ ì‹œ labelë¡œ í´ë°±.
    - ì„¸ê·¸ë¨¼íŠ¸ë³„ íŒŒì¼ë¡œ ë¨¼ì € ë Œë”ë§ í›„ ffmpeg concat â†’ ì˜¤ë””ì˜¤ mux.
    """
    import os
    import math
    import re
    import gc
    import shutil
    import tempfile
    import subprocess
    import numpy as np
    
    # â”€â”€ ê¸°ë³¸ ì„¤ì •
    video_width, video_height = 720, 1080
    os.makedirs(os.path.dirname(save_path) or ".", exist_ok=True)

    # â”€â”€ ì „ì²´ ê¸¸ì´(ì„¸ê·¸ë¨¼íŠ¸ ë)
    total_video_duration = segments[-1]["end"] if segments else 10.0

    # â”€â”€ ë‚´ë ˆì´ì…˜(or ë¬´ìŒ)
    if audio_path and os.path.exists(audio_path):
        narration = AudioFileClip(audio_path)
    else:
        # ë¬´ìŒ(ìŠ¤í…Œë ˆì˜¤) ìƒì„±
        nframes = max(1, int(total_video_duration * 44100))
        silent = np.zeros((nframes, 2), dtype=np.float32)
        narration = AudioArrayClip(silent, fps=44100)
        narration = narration.with_duration(total_video_duration)
        print("ğŸ”Š ìŒì„± íŒŒì¼ì´ ì—†ì–´ ë¬´ìŒ ì˜¤ë””ì˜¤ íŠ¸ë™ì„ ìƒì„±í–ˆìŠµë‹ˆë‹¤.")

        # â”€â”€ BGM ë¯¹ìŠ¤(ì˜µì…˜)
    final_audio = narration
    if bgm_path and os.path.exists(bgm_path):
        try:
            _st(f"ğŸ§ BGM path: {bgm_path} (exists={os.path.exists(bgm_path)})")
            bgm_raw = AudioFileClip(bgm_path)
            sr = 44100

            if not getattr(bgm_raw, "duration", 0) or bgm_raw.duration <= 0.1:
                raise RuntimeError("BGM duration too short")

            narr_dur = float(narration.duration)

            # BGMì„ ë°°ì—´ë¡œ ë³€í™˜ â†’ ê¸¸ì´ì— ë§ì¶° íƒ€ì¼ë§
            bgm_arr = bgm_raw.to_soundarray(fps=sr)
            if bgm_arr.ndim == 1:  # ëª¨ë…¸ë©´ ìŠ¤í…Œë ˆì˜¤ë¡œ ë³µì œ
                bgm_arr = np.column_stack([bgm_arr, bgm_arr])

            need = int(np.ceil(narr_dur / max(bgm_raw.duration, 0.001)))
            tiled = np.tile(bgm_arr, (need, 1))
            n_samples = int(np.round(narr_dur * sr))
            tiled = tiled[:n_samples]

            # ë³¼ë¥¨: ì ë‹¹íˆ ë“¤ë¦¬ê²Œ (ì›í•˜ë©´ 0.5~0.8 ì‚¬ì´ë¡œ ì¡°ì ˆ)
            gain = 0.2
            tiled = tiled * gain

            # ë°°ì—´ â†’ AudioArrayClip â†’ ë‚´ë ˆì´ì…˜ê³¼ í•©ì„±
            bgm_clip = AudioArrayClip(tiled, fps=sr).with_duration(narr_dur)
            final_audio = CompositeAudioClip([narration, bgm_clip])

            _st(f"âœ… Mixed BGM (narr={narr_dur:.3f}s, bgm={bgm_raw.duration:.3f}s, sr={sr})")
        except Exception as e:
            _st(f"âš ï¸ BGM mix failed (continue w/o BGM): {e}")
    else:
        _st("â„¹ï¸ No BGM path or not found â€” narration only")

    # â”€â”€ ì†ŒìŠ¤ ë™ì˜ìƒ ìˆ˜ ë³´ì •(ë¶€ì¡± ì‹œ ìˆœí™˜)
    if len(video_paths) < len(segments) and video_paths:
        cycle = (len(segments) + len(video_paths) - 1) // len(video_paths)
        video_paths = (video_paths * cycle)[:len(segments)]

    # â”€â”€ íƒ€ì´í‹€ ë„ìš°ë¯¸(ë‚´ë¶€)
    def _auto_split_title(title: str):
        t = (title or "").strip()
        if not t:
            return "", ""
        # ê¸¸ë©´ ëŒ€ëµ ì ˆë°˜ì—ì„œ ê³µë°± ê·¼ì²˜ë¡œ ë¶„ë¦¬
        if len(t) <= 14:
            return t, ""
        mid = len(t) // 2
        # mid ì£¼ë³€ ê³µë°± íƒìƒ‰
        left = t.rfind(" ", 0, mid)
        right = t.find(" ", mid)
        if left == -1 and right == -1:
            return t, ""
        cut = left if (left != -1 and (mid - left) <= (right - mid if right != -1 else 1e9)) else (right if right != -1 else left)
        return t[:cut].strip(), t[cut:].strip()

    # â”€â”€ ì•ˆì „í•œ TextClip ë¹Œë”
    def _build_text_clip(text: str, font_path: str, font_size: int, max_width: int):
        # caption ë¨¼ì € ì‹œë„
        try:
            clip = TextClip(
                text=text + "\n",
                font_size=font_size,
                color="white",
                font=font_path,
                stroke_color="skyblue",
                stroke_width=1,
                method="caption",               # â† captionì´ë©´ size í—ˆìš©
                size=(max_width, None),
                align="center",
            )
            return clip, True
        except TypeError:
            # label í´ë°± (size ì ˆëŒ€ ì „ë‹¬í•˜ì§€ ì•ŠìŒ!)
            clip = TextClip(
                text=text + "\n",
                font_size=font_size,
                color="white",
                font=font_path,
                method="label",
            )
            return clip, False

    # â”€â”€ ë¦¬ì‚¬ì´ì¦ˆ(cover)
    def _resize_cover(clip, W, H):
        scale = max(W / clip.w, H / clip.h)
        resized = clip.resized(scale)
        x = int(round((W - resized.w) / 2))
        y = int(round((H - resized.h) / 2))
        return resized.with_position((x, y))

    # â”€â”€ ì„¸ê·¸ë¨¼íŠ¸ë³„ íŒŒì¼ ìƒì„±
    seg_files = []
    ffmpeg_path = imageio_ffmpeg.get_ffmpeg_exe()

    for i, seg in enumerate(segments):
        duration = max(0.1, seg["end"] - seg["start"])
        src_path = video_paths[i % len(video_paths)] if video_paths else None

        if (not src_path) or (not os.path.exists(src_path)):
            # ë¹„ìƒ: ìƒ‰ ë°°ê²½
            base = ColorClip(size=(video_width, video_height), color=(0, 0, 0)).with_duration(duration)
        else:
            raw = VideoFileClip(src_path).without_audio()
            try:
                if raw.duration < duration:
                    # ë¶€ì¡±í•˜ë©´ ë°˜ë³µ
                    repeat = int(math.ceil(duration / max(raw.duration, 0.1)))
                    rep = concatenate_videoclips([raw] * repeat, method="chain").subclip(0, duration)
                    base = _resize_cover(rep, video_width, video_height)
                else:
                    base = _resize_cover(raw.subclipped(0, duration), video_width, video_height)
            finally:
                try:
                    raw.close()
                except Exception:
                    pass

        overlays = [base]

        # â”€â”€ ìƒë‹¨ íƒ€ì´í‹€(ì„ íƒ)
        if include_topic_title and (topic_title or "").strip():
            font_path = os.path.join("assets", "fonts", "BMJUA_ttf.ttf")
            line1, line2 = _auto_split_title(topic_title)
            title_text = line1 + ("\n" + line2 if line2 else "")
            max_title_w = video_width - 40

            title_clip, used_caption = _build_text_clip(title_text, font_path, 48, max_title_w)
            # bar ë†’ì´ëŠ” ë§Œë“¤ì–´ì§„ clip ë†’ì´ì— paddingë§Œ ë”í•¨ (dummy ë¶ˆí•„ìš”)
            title_bar_h = int(title_clip.h + 32)

            black_bar = ColorClip(size=(video_width, title_bar_h), color=(0, 0, 0)).with_duration(duration).with_position(("center", "top"))

            tx = int(round((video_width - title_clip.w) / 2))
            ty = int(max(0, min(round((title_bar_h - title_clip.h) / 2) + 10, title_bar_h - title_clip.h)))
            title_clip = title_clip.with_duration(duration).with_position((tx, ty))

            overlays.extend([black_bar, title_clip])

        seg_clip = CompositeVideoClip(overlays, size=(video_width, video_height)).with_duration(duration)

        seg_out = os.path.join(os.path.dirname(save_path) or ".", f"_seg_{i:03d}.mp4")
        seg_clip.write_videofile(
            seg_out,
            codec="libx264",
            audio=False,
            fps=24,
            preset="ultrafast",
            threads=max(1, (os.cpu_count() or 2) // 2),
            ffmpeg_params=["-pix_fmt", "yuv420p", "-movflags", "+faststart"],
            logger=None,
        )
        try:
            seg_clip.close()
        except Exception:
            pass
        gc.collect()

        seg_files.append(os.path.abspath(seg_out))

    # â”€â”€ ì˜ëª»ëœ ì„¸ê·¸ë¨¼íŠ¸ ê²€ì‚¬
    bad = [p for p in seg_files if (not os.path.exists(p)) or os.path.getsize(p) < 1024]
    if bad:
        raise RuntimeError(f"ì˜ëª»ëœ ì„¸ê·¸ë¨¼íŠ¸ íŒŒì¼ ë°œê²¬: {bad}")

    # â”€â”€ ê²½ë¡œ sanitize + concat ë¦¬ìŠ¤íŠ¸
    def _sanitize_for_concat(paths):
        safe = []
        tmp_dir = None
        for p in paths:
            ap = os.path.abspath(p).replace("\\", "/").replace("\r", "").replace("\n", "")
            if re.search(r"[^A-Za-z0-9._/\-]", ap):
                if tmp_dir is None:
                    tmp_dir = tempfile.mkdtemp(prefix="_concat_safe_")
                base = re.sub(r"[^A-Za-z0-9._-]", "_", os.path.basename(ap))
                safe_ap = os.path.join(tmp_dir, base)
                if not os.path.exists(safe_ap):
                    shutil.copy2(ap, safe_ap)
                ap = os.path.abspath(safe_ap).replace("\\", "/")
            safe.append(ap)
        return safe

    safe_paths = _sanitize_for_concat(seg_files)
    concat_txt = os.path.join(os.path.dirname(save_path) or ".", "_concat.txt")
    with open(concat_txt, "wb") as f:
        f.write(b"ffconcat version 1.0\n")
        for ap in safe_paths:
            f.write(f"file '{ap}'\n".encode("utf-8"))

    # â”€â”€ ë¹„ë””ì˜¤ concat
    temp_video = os.path.join(os.path.dirname(save_path) or ".", "_temp_video.mp4")
    try:
        subprocess.run(
            [ffmpeg_path, "-y", "-f", "concat", "-safe", "0", "-i", concat_txt,
             "-r", "24", "-c:v", "libx264", "-pix_fmt", "yuv420p", "-preset", "ultrafast", "-crf", "23", "-an", temp_video],
            check=True,
        )
    except subprocess.CalledProcessError:
        # fallback: filter_complex concat
        cmd = [ffmpeg_path, "-y"]
        for p in safe_paths:
            cmd += ["-i", p]
        n = len(safe_paths)
        filtergraph = "".join(f"[{i}:v]" for i in range(n)) + f"concat=n={n}:v=1:a=0[outv]"
        cmd += ["-filter_complex", filtergraph, "-map", "[outv]",
                "-r", "24", "-c:v", "libx264", "-pix_fmt", "yuv420p",
                "-preset", "ultrafast", "-crf", "23", "-an", temp_video]
        subprocess.run(cmd, check=True)

    # â”€â”€ ì˜¤ë””ì˜¤ ì¶”ì¶œ/ì¸ì½”ë“œ
    audio_mix = os.path.join(os.path.dirname(save_path) or ".", "_mix_audio.m4a")
    try:
        final_audio.write_audiofile(audio_mix, fps=44100, codec="aac", bitrate="128k", logger=None)
    except Exception:
        wav_tmp = os.path.join(os.path.dirname(save_path) or ".", "_mix_audio.wav")
        try:
            final_audio.write_audiofile(wav_tmp, fps=44100, logger=None)
            subprocess.run([ffmpeg_path, "-y", "-i", wav_tmp, "-c:a", "aac", "-b:a", "128k", audio_mix], check=True)
            os.remove(wav_tmp)
        except Exception as e:
            print(f"âš ï¸ ì˜¤ë””ì˜¤ ì¸ì½”ë”© í´ë°± ì‹¤íŒ¨: {e}")

    # â”€â”€ ë¹„ë””ì˜¤+ì˜¤ë””ì˜¤ mux
    subprocess.run(
        [ffmpeg_path, "-y",
         "-i", temp_video, "-i", audio_mix,
         "-map", "0:v:0", "-map", "1:a:0",
         "-c:v", "copy", "-c:a", "aac", "-b:a", "128k",
         save_path],
        check=True,
    )

    print(f"âœ… ì˜ìƒ(ë™ì˜ìƒ ì†ŒìŠ¤) ì €ì¥ ì™„ë£Œ: {save_path}")

    # â”€â”€ ì •ë¦¬
    for p in seg_files + [concat_txt, temp_video, audio_mix]:
        try:
            os.remove(p)
        except Exception:
            pass
    try:
        if final_audio is not narration:
            final_audio.close()
    except Exception:
        pass
    try:
        narration.close()
    except Exception:
        pass
    gc.collect()

    return save_path
