# rag_pipeline.py

import streamlit as st
import asyncio
from langchain_google_genai import ChatGoogleGenerativeAI
#from langchain_community.embeddings import SentenceTransformerEmbeddings
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain_community.document_loaders import SeleniumURLLoader
# [ìˆ˜ì • 1] RecursiveCharacterTextSplitterë¥¼ ë‹¤ì‹œ import í•©ë‹ˆë‹¤.
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.chains.combine_documents import create_stuff_documents_chain
from file_handler import get_documents_from_files
import faiss
from langchain.embeddings import HuggingFaceEmbeddings
import os

def get_retriever_from_source(source_type, source_input):
    documents = [] 
    with st.status("ë¬¸ì„œ ì²˜ë¦¬ ì¤‘...", expanded=True) as status:
        if source_type == "URL":
            status.update(label="URL ì»¨í…ì¸ ë¥¼ ë¡œë“œ ì¤‘ì…ë‹ˆë‹¤...")
            loader = SeleniumURLLoader(urls=[source_input])
            documents = loader.load()
        elif source_type == "Files":
            status.update(label="íŒŒì¼ì„ íŒŒì‹±í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
            documents = get_documents_from_files(source_input)
        elif source_type == "FAISS":
            embeddings = HuggingFaceEmbeddings(model_name="jhgan/ko-sbert-sts")
    
            if os.path.isdir(source_input):
                index_dir = source_input
                # ğŸ” FAISS í•„ìˆ˜ íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
                faiss_file = os.path.join(index_dir, "index.faiss")
                pkl_file = os.path.join(index_dir, "index.pkl")
        
                if not os.path.exists(faiss_file):
                    st.error("index.faiss íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                    print("[ERROR] index.faiss íŒŒì¼ ì—†ìŒ:", faiss_file)
                if not os.path.exists(pkl_file):
                    st.error("index.pkl íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                    print("[ERROR] index.pkl íŒŒì¼ ì—†ìŒ:", pkl_file)

            else:
                st.error(f"ìœ íš¨í•˜ì§€ ì•Šì€ ê²½ë¡œì…ë‹ˆë‹¤: {source_input}")
                return None

            return FAISS.load_local(index_dir, embeddings).as_retriever()
        if not documents:
            status.update(label="ë¬¸ì„œ ë¡œë”© ì‹¤íŒ¨.", state="error")
            return None

        status.update(label="ë¬¸ì„œë¥¼ ì²­í¬(chunk)ë¡œ ë¶„í•  ì¤‘ì…ë‹ˆë‹¤...")
        # [ìˆ˜ì • 2] í‘œì™€ ê°™ì€ êµ¬ì¡°ì  ë°ì´í„°ê°€ ê¹¨ì§€ì§€ ì•Šë„ë¡, Markdown êµ¬ì¡°ì— ìµœì í™”ëœ
        # RecursiveCharacterTextSplitterë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            separators=["\n\n", "\n", " ", ""], # Markdown êµ¬ì¡°ë¥¼ ìš°ì„ ì ìœ¼ë¡œ ê³ ë ¤
            is_separator_regex=False,
        )
        splits = text_splitter.split_documents(documents)
        
        status.update(label=f"ì„ë² ë”© ëª¨ë¸ì„ ë¡œì»¬ì— ë¡œë“œ ì¤‘ì…ë‹ˆë‹¤...")
        embeddings = HuggingFaceEmbeddings(model_name='jhgan/ko-sbert-sts')
        
        status.update(label=f"{len(splits)}ê°œì˜ ì²­í¬ë¥¼ ì„ë² ë”©í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
        vectorstore = FAISS.from_documents(splits, embeddings)
        
        # [ìˆ˜ì • 3] Contextual Compression Retriever ëŒ€ì‹ , ì¤‘ë³µì„ ì¤„ì—¬ì£¼ëŠ” MMR ê²€ìƒ‰ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
        # ì´ ë°©ì‹ì´ ë” ì•ˆì •ì ì´ê³  ì¶œì²˜ ëˆ„ë½ ë¬¸ì œê°€ ì—†ìŠµë‹ˆë‹¤.
        retriever = vectorstore.as_retriever(
            search_type="similarity",
            search_kwargs={'k': 3, 'fetch_k': 20}
        )
        status.update(label="ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ!", state="complete")
    
    return retriever

def get_document_chain(system_prompt):
    template = f"""{system_prompt}

Answer the user's question based on the context provided below and the conversation history.
The context may include text and tables in markdown format. You must be able to understand and answer based on them.
If you don't know the answer, just say that you don't know. Don't make up an answer.

Context:
{{context}}
"""
    rag_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", template),
            MessagesPlaceholder(variable_name="chat_history"),
            ("user", "{input}"),
        ]
    )
    llm = ChatGoogleGenerativeAI(model="models/gemini-1.5-flash", temperature=0)
    document_chain = create_stuff_documents_chain(llm, rag_prompt)
    return document_chain

def get_default_chain(system_prompt):
    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system_prompt),
            MessagesPlaceholder(variable_name="chat_history"),
            ("user", "{question}"),
        ]
    )
    llm = ChatGoogleGenerativeAI(model="models/gemini-1.5-flash", temperature=0)
    return prompt | llm | StrOutputParser()
