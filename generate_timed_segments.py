from deep_translator import GoogleTranslator
from ssml_converter import convert_line_to_ssml
from html import escape as _xml_escape
# generate_timed_segments.py
import os
import re
from elevenlabs_tts import generate_tts
from pydub import AudioSegment
import kss
import boto3, json
from elevenlabs_tts import TTS_POLLY_VOICES 

def _join_no_repeat(a: str, b: str) -> str:
    import re
    A = re.sub(r"\s+", " ", (a or "")).strip()
    B = re.sub(r"\s+", " ", (b or "")).strip()
    if not A: return B
    if not B: return A
    if B in A: return A             # bê°€ aì— ì™„ì „íˆ í¬í•¨ â†’ aë§Œ
    if A in B: return B             # aê°€ bì— ì™„ì „íˆ í¬í•¨ â†’ bë§Œ
    A_toks, B_toks = A.split(), B.split()
    k = min(len(A_toks), len(B_toks))
    for n in range(k, 0, -1):
        # aì˜ ì ‘ë¯¸ == bì˜ ì ‘ë‘ â†’ ê²¹ì¹œ ë¶€ë¶„ ë¹¼ê³  ë¶™ì´ê¸°
        if A_toks[-n:] == B_toks[:n]:
            return " ".join(A_toks + B_toks[n:])
    return A + " " + B

def dedupe_adjacent_texts(segs):
    out = []
    prev_clean = None
    for s in segs:
        t = (s.get("text") or "").strip()
        t_clean = re.sub(r"\s+", " ", strip_ssml_tags(t)).strip()
        if out and t_clean == prev_clean:
            # ë°”ë¡œ ì´ì „ê³¼ ë™ì¼í•˜ë©´ ì‹œê°„ë§Œ ì´ì–´ë¶™ì„
            out[-1]["end"] = max(out[-1]["end"], s["end"])
        else:
            out.append(dict(s))
            prev_clean = t_clean
    return out

TAG_RE = re.compile(r"<[^>]+>")
def strip_ssml_tags(s: str) -> str:
    return TAG_RE.sub(" ", s or "")

# --- SSML guard helpers (ì›ë¬¸â‰ SSML ë¶ˆì¼ì¹˜/ì¤‘ë³µ ë°©ì§€) ---
import re as _re_guard

def _plain_text_from_ssml(ssml: str) -> str:
    t = _re_guard.sub(r"<[^>]+>", " ", ssml)  # íƒœê·¸ ì œê±°
    return _re_guard.sub(r"\s+", " ", t).strip()

def _tokenize_ko_en(s: str):
    # í•œê¸€/ì˜ë¬¸/ìˆ«ìë§Œ í† í°í™”(ê¸°í˜¸/ê³µë°± ë¬´ì‹œ)
    return _re_guard.findall(r"[0-9A-Za-z\uac00-\ud7a3]+", s or "")

def _ssml_safe_or_fallback(orig_line: str, ssml_fragment: str):
    """ì›ë¬¸ê³¼ LLM-SSML ë¶ˆì¼ì¹˜/ì¤‘ë³µì´ë©´ ê²°ì •ì  SSMLë¡œ í´ë°±"""
    plain = _plain_text_from_ssml(ssml_fragment)
    tok_o = _tokenize_ko_en(orig_line)
    tok_s = _tokenize_ko_en(plain)

    # ìƒˆ ë‹¨ì–´ ì‚½ì… íƒì§€
    inserted = [t for t in tok_s if t not in tok_o]
    # ì—°ì† ì¤‘ë³µ íƒì§€(â€œì–´ë–»ê²Œ ì–´ë–»ê²Œâ€ ë“±)
    repeated = any(tok_s[i] == tok_s[i-1] for i in range(1, len(tok_s)))

    if inserted or repeated or len(tok_s) < max(1, len(tok_o)//3):
        safe = f'<prosody rate="150%" volume="medium">{_xml_escape(orig_line)}</prosody>'
        return safe, True
    return ssml_fragment, False

def get_polly_speechmarks(ssml: str, voice_id: str, region: str = "ap-northeast-2", types=("sentence",)):
    polly = boto3.client("polly", region_name=region)
    resp = polly.synthesize_speech(
        Text=ssml, TextType="ssml",
        VoiceId=voice_id,
        OutputFormat="json",
        SpeechMarkTypes=list(types)
    )
    # ìŠ¤íŠ¸ë¦¼ì€ JSONL í˜•íƒœ(ì¤„ë§ˆë‹¤ í•˜ë‚˜ì˜ JSON)
    payload = resp["AudioStream"].read().decode("utf-8", errors="ignore")
    marks = [json.loads(line) for line in payload.splitlines() if line.strip()]
    # ê° í•­ëª© ì˜ˆì‹œ: {"time": 1234, "type": "sentence", "value": "ë¬¸ì¥ í…ìŠ¤íŠ¸", ...}
    return marks

def resolve_polly_voice_id(polly_voice_key: str, tts_lang: str | None = "ko") -> str:
    """
    polly_voice_key: ì½”ë“œì—ì„œ ì“°ëŠ” í‚¤("korean_female1" ë“±)
    tts_lang: 'ko' | 'en' ë“±, í‚¤ê°€ ì—†ì„ ë•Œì˜ ê¸°ë³¸ê°’ ì„ íƒì—ë§Œ ì‚¬ìš©
    """
    v = TTS_POLLY_VOICES.get(polly_voice_key)
    if v:
        return v
    # í‚¤ê°€ ì—†ì„ ë•Œ ì•ˆì „ í´ë°±
    if tts_lang == "ko":
        return TTS_POLLY_VOICES.get("korean_female2", "Seoyeon")  # ko-KR
    return TTS_POLLY_VOICES.get("default_female", "Joanna")       # en-US

SUBTITLE_TEMPLATES = {
    "educational": {
        "Fontname": "NanumGothic",
        "Fontsize": 12,
        "PrimaryColour": "&H00FFFFFF",       # í°ìƒ‰ í…ìŠ¤íŠ¸
        "OutlineColour": "&H00000000",       # ê²€ì • ì™¸ê³½ì„ 
        "Outline": 2,
        "Alignment": 2,
        "MarginV": 40
    },
    "entertainer": {
        "Fontname": "NanumGothic",
        "Fontsize": 12,
        "PrimaryColour": "&H00FFFFFF",       # í°ìƒ‰ í…ìŠ¤íŠ¸
        "OutlineColour": "&H00000000",       # ê²€ì • ì™¸ê³½ì„ 
        "Outline": 2,
        "Alignment": 2,
        "MarginV": 40
    },
    "slow": {
        "Fontname": "NanumGothic",
        "Fontsize": 12,
        "PrimaryColour": "&H00FFFFFF",       # í°ìƒ‰ í…ìŠ¤íŠ¸
        "OutlineColour": "&H00000000",       # ê²€ì • ì™¸ê³½ì„ 
        "Outline": 2,
        "Alignment": 2,
        "MarginV": 40
    },
    "default": {
        "Fontname": "NanumGothic",
        "Fontsize": 12,
        "PrimaryColour": "&H00FFFFFF",       # í°ìƒ‰ í…ìŠ¤íŠ¸
        "OutlineColour": "&H00000000",       # ê²€ì • ì™¸ê³½ì„ 
        "Outline": 2,
        "Alignment": 2,
        "MarginV": 40
    },
    "korean_male": {
        "Fontname": "NanumGothic",
        "Fontsize": 12,
        "PrimaryColour": "&H00FFFFFF",       # í°ìƒ‰ í…ìŠ¤íŠ¸
        "OutlineColour": "&H00000000",       # ê²€ì • ì™¸ê³½ì„ 
        "Outline": 2,
        "Alignment": 2,
        "MarginV": 40
    },
    "korean_male2": {
        "Fontname": "NanumGothic",
        "Fontsize": 12,
        "PrimaryColour": "&H00FFFFFF",       # í°ìƒ‰ í…ìŠ¤íŠ¸
        "OutlineColour": "&H00000000",       # ê²€ì • ì™¸ê³½ì„ 
        "Outline": 2,
        "Alignment": 2,
        "MarginV": 40
    },
    "korean_female": {
        "Fontname": "NanumGothic",
        "Fontsize": 12,
        "PrimaryColour": "&H00FFFFFF",       # í°ìƒ‰ í…ìŠ¤íŠ¸
        "OutlineColour": "&H00000000",       # ê²€ì • ì™¸ê³½ì„ 
        "Outline": 2,
        "Alignment": 2,
        "MarginV": 40
    },
    "korean_female2": {
        "Fontname": "NanumGothic",
        "Fontsize": 12,
        "PrimaryColour": "&H00FFFFFF",       # í°ìƒ‰ í…ìŠ¤íŠ¸
        "OutlineColour": "&H00000000",       # ê²€ì • ì™¸ê³½ì„ 
        "Outline": 2,
        "Alignment": 2,
        "MarginV": 40
    }
}

def _looks_english(text: str) -> bool:
    # ë§¤ìš° ë‹¨ìˆœí•œ íœ´ë¦¬ìŠ¤í‹±: ì•ŒíŒŒë²³ì´ í•œê¸€ë³´ë‹¤ í™•ì‹¤íˆ ë§ìœ¼ë©´ ì˜ì–´ë¡œ ê°„ì£¼
    letters = len(re.findall(r'[A-Za-z]', text))
    hangul = len(re.findall(r'[\uac00-\ud7a3]', text))
    return letters >= max(3, hangul * 2)

def _detect_script_language(lines):
    eng = sum(_looks_english(x) for x in lines)
    kor = sum(bool(re.search(r'[\uac00-\ud7a3]', x)) for x in lines)
    return 'en' if eng > kor else 'ko'

def _maybe_translate_lines(lines, target='ko', only_if_src_is_english=True):
    if not lines:
        return lines
    try:
        src = _detect_script_language(lines)
        if only_if_src_is_english and src != 'en':
            # ì›ë¬¸ì´ ì˜ì–´ê°€ ì•„ë‹ ë•ŒëŠ” ê±´ë“œë¦¬ì§€ ì•ŠìŒ
            return lines
        if target is None or target == src:
            return lines
        tr = GoogleTranslator(source='auto', target=target)
        return [tr.translate(l) if l.strip() else l for l in lines]
    except Exception:
        # ë²ˆì—­ ì‹¤íŒ¨ ì‹œ ì›ë¬¸ ìœ ì§€ (í¬ë˜ì‹œ ë°©ì§€)
        return lines

def _validate_ssml(text: str) -> str:
    """
    Polly í˜¸ì¶œ ì „ SSML ì•ˆì „ì„± ê²€ì‚¬ ë° ë³´ì •
    - í—ˆìš© íƒœê·¸: speak, prosody, break
    - Neuralì—ì„œ ë¬¸ì œë˜ëŠ” pitch ì œê±°
    - ì†ì„±ì€ ìŒë”°ì˜´í‘œë¡œ í‘œì¤€í™”
    - ì—°ì† break/ê³¼ë„í•œ time ë³´ì •
    - ë¹ˆ prosody ì œê±°, íƒœê·¸ ë¶ˆì¼ì¹˜ ë³´ì •
    """
    if not text:
        return ""

    t = text.strip().replace("\ufeff", "")

    # 0) ë‚´ë¶€ì— <speak> ì¡°ê°ì´ ìˆìœ¼ë©´ ê±·ì–´ë‚´ê¸° (ë¼ì¸ ë‹¨ìœ„)
    t = re.sub(r"</?speak\s*>", "", t, flags=re.I)

    # 1) í—ˆìš© ì™¸ íƒœê·¸ëŠ” ì œê±° (speak/prosody/breakë§Œ ë‚¨ê¹€)
    t = re.sub(r"</?(?!prosody\b|break\b)[a-zA-Z0-9:_-]+\b[^>]*>", "", t)

    # 2) ë‹¨ì¼ì¸ìš© ì†ì„± â†’ ìŒë”°ì˜´í‘œ
    t = re.sub(r'(\b[a-zA-Z:-]+)=\'([^\']*)\'', r'\1="\2"', t)

    # 3) pitch ì†ì„± ì œê±° (Neural í˜¸í™˜)
    t = re.sub(r'\s+pitch="[^"]*"', "", t)

    # 4) break time ë³´ì •: ìˆ«ìmsë§Œ í—ˆìš©, 2000ms ì´ˆê³¼ì‹œ 2000msë¡œ clamp
    def _clamp_break(m):
        val = m.group(2)
        try:
            n = int(val)
        except Exception:
            n = 200  # ì´ìƒì¹˜ë©´ 200msë¡œ
        n = max(0, min(n, 2000))
        return f'{m.group(1)}{n}{m.group(3)}'
    t = re.sub(r'(<break\b[^>]*\btime=")(\d+)(ms"[^>]*/?>)', _clamp_break, t, flags=re.I)

    # 5) ì—°ì† break â†’ í•˜ë‚˜ë§Œ ë‚¨ê¹€
    t = re.sub(r'(?:<break\b[^>]*/>\s*){2,}', lambda m: re.findall(r'<break\b[^>]*/>', m.group(0))[0], t, flags=re.I)

    # 6) ë¹ˆ prosody ì œê±°
    t = re.sub(r"<prosody[^>]*>\s*</prosody>", "", t, flags=re.I)

    # 7) prosody ë‹«í˜ ë³´ì •
    open_count = len(re.findall(r"<prosody\b", t, flags=re.I))
    close_count = len(re.findall(r"</prosody>", t, flags=re.I))
    t += "</prosody>" * max(0, open_count - close_count)

    # 8) prosodyê°€ ì•„ì˜ˆ ì—†ìœ¼ë©´ ê¸°ë³¸ ë˜í•‘ (ì•ˆì „ ê¸°ë³¸ê°’)
    if "<prosody" not in t:
        t = f'<prosody rate="155%" volume="medium">{_xml_escape(t)}</prosody>'

    return t.strip()

def generate_tts_per_line(script_lines, provider, template, polly_voice_key="korean_female1"):
    audio_paths = []
    temp_audio_dir = "temp_line_audios"
    os.makedirs(temp_audio_dir, exist_ok=True)

    print(f"ë””ë²„ê·¸: ì´ {len(script_lines)}ê°œì˜ ìŠ¤í¬ë¦½íŠ¸ ë¼ì¸ì— ëŒ€í•´ TTS ìƒì„± ì‹œë„.")

    for i, line in enumerate(script_lines):
        line_audio_path = os.path.join(temp_audio_dir, f"line_{i}.mp3")
        try:
            # Pollyë©´ í•œ ë²ˆ ë” ì•ˆì „ ì²´í¬(ë¹ˆ prosody ì œê±° ë“±)
            line_ssml = _validate_ssml(line)

            # ì™„ì „ì²´ê°€ ì•„ë‹ˆë©´ <speak> ë˜í•‘(í˜¹ì‹œ ìƒìœ„ ë‹¨ê³„ì—ì„œ ëª» ê°ì‹¼ ê²½ìš° ëŒ€ë¹„)
            ls = line_ssml.strip()
            if provider == "polly" and not ls.startswith("<speak"):
                ls = f"<speak>{ls}</speak>"

            generate_tts(
                text=ls,
                save_path=line_audio_path,
                provider=provider,
                template_name=template,
                polly_voice_name_key=polly_voice_key
            )
            audio_paths.append(line_audio_path)
            print(f"ë””ë²„ê·¸: ë¼ì¸ {i+1} ('{line[:30]}...') TTS ìƒì„± ì„±ê³µ. íŒŒì¼: {line_audio_path}")
        except Exception as e:
            print(f"ì˜¤ë¥˜: ë¼ì¸ {i+1} ('{line[:30]}...') TTS ìƒì„± ì‹¤íŒ¨: {e}")
            continue
            
    print(f"ë””ë²„ê·¸: ìµœì¢… ìƒì„±ëœ ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ ìˆ˜: {len(audio_paths)}")
    if not audio_paths:
        raise RuntimeError("ë¼ì¸ë³„ TTSê°€ 0ê±´ ìƒì„±ë¨ (ê° ë¼ì¸ì˜ ì‹¤íŒ¨ ì‚¬ìœ ëŠ” ìœ„ ë¡œê·¸ ì°¸ì¡°)")
    
    return audio_paths

def merge_audio_files(audio_paths, output_path):
    merged = AudioSegment.empty()
    segments = []
    current_time = 0.0

    for path in audio_paths:
        a = AudioSegment.from_file(path)
        d = a.duration_seconds
        segments.append({"start": current_time, "end": current_time + d})
        merged += a
        current_time += d

    # âœ… ëŠê¹€ ë°©ì§€ìš© ê¼¬ë¦¬ ë¬´ìŒ
    tail = AudioSegment.silent(duration=120)
    merged += tail
    if segments:
        segments[-1]["end"] += 0.12

    merged.export(output_path, format="mp3")
    return segments

def get_segments_from_audio(audio_paths, script_lines):
    segments = []
    current_time = 0
    for i, audio_path in enumerate(audio_paths):
        try:
            audio = AudioSegment.from_file(audio_path)
            duration = audio.duration_seconds
            line = script_lines[i]
            segments.append({
                "start": current_time,
                "end": current_time + duration,
                "text": line
            })
            current_time += duration
        except Exception as e:
            print(f"ì˜¤ë¥˜: ì˜¤ë””ì˜¤ íŒŒì¼ {audio_path} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            continue
    return segments

# --- pitch í• ë‹¹ í•¨ìˆ˜ ---
def _assign_pitch(text: str) -> int:
    if text.endswith("?"):
        return 20     # ì§ˆë¬¸/ê²½ê³ 
    if text.endswith("ìŠµë‹ˆë‹¤") or text.endswith("í•©ë‹ˆë‹¤"):
        return -5     # ì¼ë°˜ ì„¤ëª…
    if text.endswith("ì´ë‹¤") or text.endswith("ì—†ìŠµë‹ˆë‹¤"):
        return -18    # ê²°ë¡ /ë‹¨ì •
    return 0

def generate_ass_subtitle(segments, ass_path, template_name="default",
                          strip_trailing_punct_last=True):
    settings = SUBTITLE_TEMPLATES.get(template_name, SUBTITLE_TEMPLATES["default"])

    def _escape_ass_text(s: str) -> str:
        s = s.replace("\\", r"\\")
        s = s.replace("\r", "")
        s = s.replace("\n", r"\N")
        return s

    with open(ass_path, "w", encoding="utf-8") as f:
        f.write("[Script Info]\n")
        f.write("ScriptType: v4.00+\n\n")

        f.write("[V4+ Styles]\n")
        f.write("Format: Name, Fontname, Fontsize, PrimaryColour, OutlineColour, BorderStyle, Outline, Shadow, Alignment, MarginL, MarginR, MarginV, Encoding\n")
        f.write(f"Style: Bottom,{settings['Fontname']},{settings['Fontsize']},{settings['PrimaryColour']},{settings['OutlineColour']},1,{settings['Outline']},0,2,10,10,{settings['MarginV']},1\n\n")

        f.write("[Events]\n")
        f.write("Format: Layer, Start, End, Style, Name, MarginL, MarginR, MarginV, Effect, Text\n")

        for i, seg in enumerate(segments):
            start, end = float(seg.get('start', 0.0)), float(seg.get('end', 0.0))

            # â‘  í…ìŠ¤íŠ¸ë¥¼ 'ë¨¼ì €' ì•ˆì „í•˜ê²Œ ë½‘ê³ 
            raw = seg.get('text') or ""
            # â‘¡ SSML íƒœê·¸ëŠ” ê³µë°±ìœ¼ë¡œ ëŒ€ì²´ â†’ ê²½ê³„ê°€ ë¶™ì§€ ì•Šë„ë¡
            txt = strip_ssml_tags(raw)          # ì˜ˆ: "<prosody>ì•ˆë…•</prosody>ì„¸ìƒ" â†’ "ì•ˆë…• ì„¸ìƒ"
            # â‘¢ ê³µë°± ì •ê·œí™”(2ì¹¸ ì´ìƒ â†’ 1ì¹¸)
            txt = re.sub(r"\s+", " ", txt).strip()

            # (ì„ íƒ) ë§ˆì§€ë§‰ ì¤„ ê¼¬ë¦¬ êµ¬ë‘ì  ë‹¤ë“¬ê¸°
            if strip_trailing_punct_last and i == len(segments) - 1:
                txt = _strip_last_punct_preserve_closers(txt)

            # ìƒ‰ìƒ(í”¼ì¹˜) íŒë‹¨ì€ ì •ê·œí™” ì´í›„ í…ìŠ¤íŠ¸ ê¸°ì¤€
            pitch_val = seg.get("pitch")
            if pitch_val is None:
                pitch_val = _assign_pitch(txt)
            colour_tag = "{\\c&H0000FF&}" if pitch_val <= -10 else ""

            # ASS ì´ìŠ¤ì¼€ì´í”„ëŠ” ë§ˆì§€ë§‰
            txt = txt.replace("\\", r"\\").replace("\r", "").replace("\n", r"\N")

            start_ts = format_ass_timestamp(start)
            end_ts   = format_ass_timestamp(end)

            f.write(f"Dialogue: 0,{start_ts},{end_ts},Bottom,,0,0,0,,{colour_tag}{txt}\n")

def format_ass_timestamp(seconds):
    h = int(seconds // 3600)
    m = int((seconds % 3600) // 60)
    s = int(seconds % 60)
    cs = int((seconds - int(seconds)) * 100)
    return f"{h:01}:{m:02}:{s:02}.{cs:02}"


def split_script_to_lines(script_text, mode="newline"):
    text = script_text or ""
    if mode == "punct":  # ì½¤ë§ˆ/ë§ˆì¹¨í‘œ ê¸°ì¤€
        parts = re.split(r'(?<=[,.])\s*', text.strip())
        return [p for p in map(str.strip, parts) if p]
    elif mode == "kss":  # í•œêµ­ì–´ ë¬¸ì¥ ë¶„í• ê¸°
        return [s.strip() for s in kss.split_sentences(text) if s.strip()]
    else:                # âœ… ì…ë ¥ ì¤„ë°”ê¿ˆ ê·¸ëŒ€ë¡œ(ì›í•˜ì‹œëŠ” ë™ì‘)
        return [ln.strip() for ln in text.splitlines() if ln.strip()]

# --- ë³€ê²½ 2: generate_subtitle_from_script ì‹œê·¸ë‹ˆì²˜/ë¡œì§ í™•ì¥ ---
def generate_subtitle_from_script(
    script_text: str,
    ass_path: str,
    full_audio_file_path: str,
    provider: str = "polly",
    template: str = "default",
    polly_voice_key: str = "default_male",
    subtitle_lang: str = "ko",
    translate_only_if_english: bool = False,
    tts_lang: str | None = None,
    split_mode: str = "newline",
    strip_trailing_punct_last: bool = True
):
    # 1) ë¼ì¸ ë¶„í•  (ì›ë¬¸/ìë§‰ ë¶„ë¦¬ ë³´ê´€)
    script_lines_raw = split_script_to_lines(script_text, mode=split_mode)
    script_lines_raw = [l for l in script_lines_raw if l.strip()]   # ì™„ì „ ë¹ˆ ì¤„ ì œê±°
    def _strip_punct_and_quotes(s: str) -> str:
        if not s: return ""
        s = s.translate(str.maketrans({
            "â€œ": "", "â€": "", "â€": "", "â€Ÿ": "", '"': "",
            "â€˜": "", "â€™": "", "â€š": "", "â€›": "", "'": "",
            "ï¼": "!", "ï¼Ÿ": "?"
        }))
        s = re.sub(r'[!?]+', '', s)
        s = re.sub(r'\s{2,}', ' ', s)
        return s.strip()

    script_lines_sub = [strip_ssml_tags(_strip_punct_and_quotes(l)) for l in script_lines_raw]
    if not script_lines_raw:
        return [], None, ass_path

    # 2) ì–¸ì–´-ë³´ì´ìŠ¤ í‚¤ ë™ê¸°í™” (Polly ì „ìš©)
    if provider == "polly":
        if tts_lang == "en" and polly_voice_key.startswith("korean_"):
            print(f"âš ï¸ ì˜ì–´ ëª¨ë“œì¸ë° í•œêµ­ì–´ ë³´ì´ìŠ¤({polly_voice_key}) ì„ íƒë¨ â†’ default_maleë¡œ êµì²´")
            polly_voice_key = "default_male"
        elif tts_lang == "ko" and polly_voice_key.startswith("default_"):
            print(f"âš ï¸ í•œêµ­ì–´ ëª¨ë“œì¸ë° ì˜ì–´ ë³´ì´ìŠ¤({polly_voice_key}) ì„ íƒë¨ â†’ korean_female1ìœ¼ë¡œ êµì²´")
            polly_voice_key = "korean_female1"

    # 3) TTS ë¼ì¸ ì¤€ë¹„ (PollyëŠ” ì›ë¬¸ ì‚¬ìš© + ì•ˆì „ ê°€ë“œ)
    if provider == "polly":
        tts_src_lines = script_lines_raw
        tts_lines = []
        for orig in tts_src_lines:
            try:
                frag = convert_line_to_ssml(orig)  # LLM ìƒì„±
            except Exception:
                frag = f'<prosody rate="150%" volume="medium">{_xml_escape(orig)}</prosody>'
            frag, _fell_back = _ssml_safe_or_fallback(orig, frag)  # âœ… ì›ë¬¸ ë¶ˆì¼ì¹˜/ì¤‘ë³µ ì°¨ë‹¨
            safe = _validate_ssml(frag)
            if not safe.strip().startswith("<speak"):
                safe = f"<speak>{safe}</speak>"
            tts_lines.append(safe)
    else:
        # (Polly ì™¸ ì—”ì§„ì´ë©´ ìë§‰ ì •ë¦¬ë³¸ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©)
        tts_lines = script_lines_sub[:]

    # (ì„ íƒ) TTS ì–¸ì–´ ê°•ì œ ë³€í™˜
    if tts_lang in ("en", "ko") and provider != "polly":
        tts_lines = _maybe_translate_lines(
            tts_lines,
            target=tts_lang,
            only_if_src_is_english=False
        )

    # 4) ìë§‰ ë¼ì¸ (í™”ë©´ í‘œì‹œìš©: ê¸°í˜¸ ì œê±°ë³¸ ê¸°ì¤€)
    if subtitle_lang in ("ko", "en"):
        subtitle_lines = _maybe_translate_lines(
            script_lines_sub,
            target=subtitle_lang,
            only_if_src_is_english=translate_only_if_english
        )
    else:
        subtitle_lines = script_lines_sub[:]

    # 5) ë¼ì¸ë³„ TTS ìƒì„±
    audio_paths = generate_tts_per_line(
        tts_lines,
        provider=provider,
        template=template,
        polly_voice_key=polly_voice_key   # âœ… ë³´ì •ëœ ë³´ì´ìŠ¤ í‚¤ ë°˜ì˜
    )
    if not audio_paths:
        return [], None, ass_path

    # 6) ë³‘í•©
    segments_raw = merge_audio_files(audio_paths, full_audio_file_path)

    segments = []
    for i, s in enumerate(segments_raw):
        # âœ… ìë§‰ìš© ê¸°í˜¸ ì œê±°ë³¸ìœ¼ë¡œ í´ë°±
        line_text = subtitle_lines[i] if i < len(subtitle_lines) else script_lines_sub[i]
        segments.append({
            "start": s["start"],
            "end": s["end"],
            "text": line_text,
            "pitch": _assign_pitch(line_text)
        })

    # === SpeechMarks ê¸°ë°˜ ì •í™• íƒ€ì´ë° ===
    MIN_SEG_DUR = 0.35  # ìµœì†Œ 0.35ì´ˆë¡œ í•©ë¦¬ì  í•©ì¹¨(í•„ìš”ì‹œ 0.3~0.5 ì‚¬ì´ë¡œ ì¡°ì •)

    def _merge_short_pieces(pieces, min_dur=MIN_SEG_DUR):
        if not pieces: return []
        merged = []
        cur = dict(pieces[0])
        for p in pieces[1:]:
            # cur ì¡°ê°ì´ ì•„ì§ ì§§ìœ¼ë©´ ë‹¤ìŒ ì¡°ê°ê³¼ í•©ì¹¨
            if (cur["end"] - cur["start"]) < min_dur:
                cur["end"]  = p["end"]
                cur["text"] = _join_no_repeat(cur["text"], p["text"])
            else:
                merged.append(cur)
                cur = dict(p)
        merged.append(cur)
        # ë§¨ ë§ˆì§€ë§‰ë„ ë„ˆë¬´ ì§§ìœ¼ë©´ ì•ê³¼ í•©ì¹˜ê¸° ì‹œë„
        if len(merged) >= 2 and (merged[-1]["end"] - merged[-1]["start"]) < min_dur:
            merged[-2]["end"]  = merged[-1]["end"]
            merged[-2]["text"] = _join_no_repeat(merged[-2]["text"], merged[-1]["text"])
            merged.pop()
        return merged

    exact_segments = []
    for i, s in enumerate(segments_raw):
        line_ssml = tts_lines[i]             # ì˜¤ë””ì˜¤ì— ì“´ SSML ê·¸ëŒ€ë¡œ
        voice_id  = resolve_polly_voice_id(polly_voice_key)
        marks     = get_polly_speechmarks(line_ssml, voice_id, types=("sentence",))
        if not marks:
            # âœ… ë¬¸ì¥ ë§ˆí¬ê°€ ì—†ìœ¼ë©´ ë‹¨ì–´ ë§ˆí¬ë¡œ í´ë°±
            marks = get_polly_speechmarks(line_ssml, voice_id, types=("word",))

        line_offset = s["start"]; line_end = s["end"]

        if marks:
            pieces = []
            prev_text = None  # â† ì¶”ê°€
            for j, mk in enumerate(marks):
                st = line_offset + (mk["time"] / 1000.0)
                en = line_end if j == len(marks)-1 else (line_offset + marks[j+1]["time"] / 1000.0)

                raw_val = mk.get("value", "")
                val = strip_ssml_tags(_strip_punct_and_quotes(raw_val))
                val_norm = re.sub(r"\s+", " ", val).strip()
                if not val_norm:
                    continue
                # ğŸ”’ ì¸ì ‘ ì¤‘ë³µ ë°©ì§€
                if val_norm == (prev_text or ""):
                    continue
                prev_text = val_norm

                st = max(line_offset, min(st, line_end))
                en = max(st,            min(en, line_end))

                if pieces:
                    prev_txt = pieces[-1]["text"]
                    prev_norm = re.sub(r"\s+", " ", prev_txt).strip()
                    # ì™„ì „ ë™ì¼, ì ‘ë¯¸/ì ‘ë‘ê°€ ê²¹ì¹˜ë©´ í˜„ì¬ ì¡°ê°ì€ ë¶™ì´ì§€ ì•Šê³  ì‹œê°„ë§Œ ëŠ˜ë¦¬ê±°ë‚˜ ëŒ€ì²´
                    if val_norm == prev_norm or prev_norm.endswith(val_norm) or val_norm.endswith(prev_norm):
                        if len(val_norm) <= len(prev_norm):
                            pieces[-1]["end"] = en     # ì´ì „ ì¡°ê° ì‹œê°„ë§Œ ëŠ˜ë¦¼
                            continue
                        else:
                            pieces[-1]["text"] = val_norm
                            pieces[-1]["end"]  = en
                            continue
                
                pieces.append({"start": st, "end": en, "text": val_norm, "pitch": _assign_pitch(val_norm)})

            pieces = _merge_short_pieces(pieces, MIN_SEG_DUR)
            exact_segments.extend(pieces)

        else:
            # í´ë°±: ë¼ì¸ í†µì§œ(ìë§‰ìš© ì •ë¦¬ë³¸ ì‚¬ìš©) â€” SSML ì ˆëŒ€ ì—†ìŒ
            line_text = subtitle_lines[i] if i < len(subtitle_lines) else script_lines_sub[i]
            line_text = strip_ssml_tags(line_text)
            exact_segments.append({
                "start": s["start"], "end": s["end"],
                "text": line_text, "pitch": _assign_pitch(line_text)
            })

    exact_segments = dedupe_adjacent_texts(exact_segments)
    
    # ì´ë¯¸ â€˜ì •í™• íƒ€ì„â€™ì´ë‹ˆ ì¡°ê°í™” ì—†ì´ ë°”ë¡œ ASS ìƒì„±
    generate_ass_subtitle(exact_segments, ass_path, template_name=template, strip_trailing_punct_last=True)
    
    return exact_segments, None, ass_path

# === Auto-paced subtitle densifier (ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ë§¥ ë¶„í•  ìš°ì„ ) ===
def _auto_split_for_tempo(text: str, tempo: str = "fast"):
    """
    tempo: fast(ì§§ê²Œ) | medium | slow
    ë¶„í•  ìš°ì„ ìˆœìœ„:
    1) ë‹´í™” í‘œì§€(ê·¸ë¦¬ê³ /í•˜ì§€ë§Œ/ê·¸ë˜ì„œ/ê·¸ëŸ°ë°/ê·¸ëŸ¬ë‹ˆê¹Œ/ì¦‰/íŠ¹íˆ/ë°˜ë©´ì—/ê²Œë‹¤ê°€/ë˜ëŠ”/í˜¹ì€ ë“±) 'ë’¤'ì—ì„œ ëŠê¸°
    2) ì—°ê²° ì–´ë¯¸(ê³ /ì§€ë§Œ/ëŠ”ë°/ë©´ì„œ/ë¼ë©´/ë©´/ë‹ˆê¹Œ/ë‹¤ê°€/ìœ¼ë©°/ë©° ë“±) 'ë’¤'ì—ì„œ ëŠê¸°
    3) ì‰¼í‘œ/ì„¸ë¯¸ì½œë¡ /ì¤‘ì  ë“± êµ¬ë‘ì  ë’¤ì—ì„œ ëŠê¸°
    4) ì—¬ì „íˆ ê¸¸ë©´ ê³µë°± ê·¼ì²˜ë¡œ ê¸¸ì´ ê¸°ë°˜ ë¶„í• 
    ì´í›„ ë„ˆë¬´ ì§§ì€ ì¡°ê°ì€ ì´ì›ƒê³¼ ë³‘í•©
    """
    import re

    # ê¸¸ì´ ëª©í‘œ(ìƒí™© ë§ê²Œ ì¡°ì ˆ)
    max_len_map = {"fast": 12, "medium": 18, "slow": 24}
    max_len = max_len_map.get(tempo, 18)
    min_piece_chars = 2  # ë„ˆë¬´ ì§§ì€ ì¡°ê° ë³‘í•© ê¸°ì¤€

    t = (text or "").strip()
    if not t:
        return []

    # 1) ë‹´í™” í‘œì§€ í›„ ë¶„í• (í† í°ì€ ì• ì¡°ê°ì— ë‘ )
    discourse = r"(ê·¸ë¦¬ê³ |í•˜ì§€ë§Œ|ê·¼ë°|ê·¸ëŸ°ë°|ê·¸ë˜ì„œ|ê·¸ëŸ¬ë‹ˆê¹Œ|ì¦‰|íŠ¹íˆ|ê²Œë‹¤ê°€|í•œí¸|ë°˜ë©´ì—|ë˜ëŠ”|í˜¹ì€|ë‹¤ë§Œ)"
    t = re.sub(rf"\b{discourse}\b\s*", r"\g<0>Â§", t)

    # 2) ì—°ê²° ì–´ë¯¸ í›„ ë¶„í• (ì–´ë¯¸ëŠ” ì• ì¡°ê°ì— ë‘ )
    eomi = r"(ê³ |ì§€ë§Œ|ëŠ”ë°ìš”?|ë©´ì„œ|ë©°|ë¼ë©´|ë©´|ë‹ˆê¹Œ|ë‹¤ê°€|ìœ¼ë©°|ê±°ë‚˜|ë“ ì§€)"
    t = re.sub(rf"({eomi})(?=\s|\Z)", r"\1Â§", t)

    # 3) ì‰¼í‘œÂ·ì„¸ë¯¸ì½œë¡ Â·ì¤‘ì  ë’¤ ë¶„í•  (êµ¬ë‘ì ì€ ì• ì¡°ê°ì—)
    t = re.sub(r"(?<=[,ï¼Œã€;:Â·])\s*", "Â§", t)

    # ì¼ì°¨ ë¶„í• 
    parts = [p.strip() for p in t.split("Â§") if p.strip()]
    chunks = []

    # 4) ê¸¸ì´ ë³´ì •: ë„ˆë¬´ ê¸´ ê±´ ê³µë°± ê·¼ì²˜ë¡œ ì¶”ê°€ ë¶„í• 
    for p in parts:
        if len(p) <= max_len:
            chunks.append(p)
            continue

        cur = p
        while len(cur) > max_len:
            window = cur[: max_len + 6]  # ì—¬ìœ 
            spaces = [m.start() for m in re.finditer(r"\s", window)]
            split_pos = spaces[-1] if spaces else max_len
            chunks.append(cur[:split_pos].strip())
            cur = cur[split_pos:].strip()
        if cur:
            chunks.append(cur)

    # 5) ë„ˆë¬´ ì§§ì€ ì¡°ê° ë³‘í•©(ì–‘ ì˜†ê³¼ ìì—°ìŠ¤ëŸ¬ìš´ ê³µë°± ì²˜ë¦¬)
    def _wordish(ch: str) -> bool:
        return ch.isalnum() or ('\uAC00' <= ch <= '\uD7A3')  # ì˜ë¬¸/ìˆ«ì/í•œê¸€
    i = 1
    while i < len(chunks):
        if len(chunks[i]) < min_piece_chars:
            prev = chunks[i-1].rstrip()
            cur  = chunks[i].lstrip()
            sep = " " if (prev and cur and _wordish(prev[-1]) and _wordish(cur[0])) else ""
            chunks[i-1] = (prev + sep + cur).strip()
            chunks.pop(i)
        else:
            i += 1

    return chunks

def auto_densify_for_subs(
    segments,
    tempo: str = "fast",
    strip_trailing_punct_each: bool = True,
    words_per_piece: int | None = 3,
    min_tail_words: int = 2,
    chunk_strategy: str | None = None,   # âœ… ì¶”ê°€: "period_2or3" ì‚¬ìš©
):
    dense = []
    for seg in segments:
        start, end = seg["start"], seg["end"]
        dur = max(0.01, end - start)

        final_pieces = []
        text = (seg.get("text") or "").strip()

        if chunk_strategy == "period_2or3":   # âœ… ìƒˆ ì „ëµ
            # 1) ë§ˆì¹¨í‘œ(.)ë¡œ ë¬¸ì¥ ë¶„ë¦¬
            sentences = _sentence_split_by_dot(text) or [text]

            for sent in sentences:
                tokens = re.findall(r'\S+', sent)
                wc = len(tokens)

                # 2) ë¬¸ì¥ ê¸¸ì´ì— ë”°ë¼ 1/2/3 ì¡°ê° ê²°ì •(ì˜ë¯¸ í•´ì¹˜ì§€ ì•Šê²Œ ë‹¨ì–´ ê²½ê³„ë§Œ ì‚¬ìš©)
                if wc <= 4:
                    chunks = [" ".join(tokens).strip()]
                elif wc <= 10:
                    chunks = _split_tokens_into_n(tokens, 2)   # 2ì¡°ê°
                else:
                    chunks = _split_tokens_into_n(tokens, 3)   # 3ì¡°ê°

                # 3) ê° ì¡°ê° ê¼¬ë¦¬ êµ¬ë‘ì  ì œê±°
                if strip_trailing_punct_each:
                    chunks = [_strip_last_punct_preserve_closers(c) for c in chunks]

                # 4) ë§¨ ë ì¡°ê°ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ ì•ê³¼ ë³‘í•©(ì˜ˆ: 'ë¼' ë‹¨ë… ë°©ì§€)
                if len(chunks) >= 2 and len(chunks[-1].split()) < 2:
                    chunks[-2] = (chunks[-2] + " " + chunks[-1]).strip()
                    chunks.pop()

                final_pieces.extend(chunks)

        else:
            # ê¸°ì¡´ ê²½ë¡œ(ë¬¸ë§¥â†’ê¸¸ì´â†’(ì˜µì…˜)ë‹¨ì–´ê°œìˆ˜ ê¸°ì¤€)
            pieces = _auto_split_for_tempo(text, tempo=tempo)
            if words_per_piece and words_per_piece > 0:
                tmp = []
                for p in pieces:
                    subs = _micro_split_by_words(p, words_per_piece, min_tail_words)
                    tmp.extend(subs if subs else [p.strip()])
                final_pieces = tmp
            else:
                final_pieces = [p.strip() for p in pieces]

            if strip_trailing_punct_each:
                final_pieces = [_strip_last_punct_preserve_closers(x) for x in final_pieces]
        
        final_pieces = _smooth_chunks_by_flow(
            final_pieces,
            target_words=3,
            min_words=2,
            max_words=5
        )
        
        if not final_pieces:
            dense.append(seg)
            continue

        # ì‹œê°„ ë°°ë¶„(ë¬¸ì ê¸¸ì´ ë¹„ìœ¨)
        total_chars = sum(len(x) for x in final_pieces) or 1
        t = start
        for i, txt in enumerate(final_pieces):
            t2 = end if i == len(final_pieces) - 1 else t + dur * (len(txt) / total_chars)
            dense.append({'start': t, 'end': t2, 'text': txt, 'pitch': seg.get('pitch')})
            t = t2

    return dense

def _strip_last_punct_preserve_closers(s: str) -> str:
    # ëì´ [. , ! ? â€¦] ì´ê³  ê·¸ ë’¤ì—ëŠ” ê³µë°±/ë‹«ëŠ” ë”°ì˜´í‘œ/ê´„í˜¸ë§Œ ì˜¤ëŠ” ê²½ìš° ê·¸ êµ¬ë‘ì ë§Œ ì œê±°
    return re.sub(r'([.,!?â€¦])(?=\s*(?:["\'â€â€™)\]\}]|$))', '', s.strip())

# --- helper: ë‹¨ì–´ ë‹¨ìœ„ ë§ˆì´í¬ë¡œ ë¶„í•  ---
def _micro_split_by_words(piece: str, target_words: int = 3, min_tail_words: int = 2):
    # ê³µë°± ê¸°ì¤€ í† í°í™”(í† í°ì— ë¶™ì€ ì‰¼í‘œ ë“±ì€ ê·¸ëŒ€ë¡œ ìœ ì§€ â†’ ì¤‘ê°„ì˜ êµ¬ë‘ì ì€ ì‚´ë¦¼)
    tokens = re.findall(r'\S+', piece.strip())
    if not tokens:
        return []
    chunks = [" ".join(tokens[i:i+target_words]).strip()
              for i in range(0, len(tokens), target_words)]
    # ë§ˆì§€ë§‰ ë©ì–´ë¦¬ê°€ ë„ˆë¬´ ì§§ìœ¼ë©´ ì• ë©ì–´ë¦¬ë¡œ í•©ì¹˜ê¸°
    if len(chunks) >= 2 and len(chunks[-1].split()) < min_tail_words:
        chunks[-2] = (chunks[-2] + " " + chunks[-1]).strip()
        chunks.pop()
    return chunks

def _sentence_split_by_dot(text: str):
    """'.' ë’¤ì—ì„œ ë¬¸ì¥ ë¶„ë¦¬(ê³µë°± ë¬´ì‹œ). ë§ˆì¹¨í‘œê°€ ì—†ë‹¤ë©´ ì „ì²´ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ."""
    if not text or not text.strip():
        return []
    parts = re.split(r'(?<=\.)\s*', text.strip())
    # ë¹ˆ ì¡°ê° ì œê±° + ì›ë¬¸ ìœ ì§€
    return [p.strip() for p in parts if p and p.strip()]

def _split_tokens_into_n(tokens, n, prefer_punct=True):
    """
    í† í° ë¦¬ìŠ¤íŠ¸ë¥¼ nê°œë¡œ ê· í˜• ìˆê²Œ ìë¦„.
    prefer_punct=Trueë©´ ê²½ê³„ ê·¼ì²˜ì—ì„œ ì‰¼í‘œ/ì„¸ë¯¸ì½œë¡  ë“± ë’¤ë¥¼ ìš°ì„  ê²½ê³„ë¡œ ì„ íƒ.
    """
    if n <= 1 or len(tokens) <= n:
        return [" ".join(tokens).strip()]

    desired = [round(len(tokens) * i / n) for i in range(1, n)]
    boundaries = []
    for idx in desired:
        # ìë¥´ê¸° ì¢‹ì€ ê·¼ì²˜ í›„ë³´ ì¸ë±ìŠ¤(í† í° ì‚¬ì´ ê²½ê³„)
        cand = list(range(max(1, idx - 2), min(len(tokens) - 1, idx + 2) + 1))
        pick = None
        if prefer_punct:
            for j in cand:
                if re.search(r'[,:;Â·â€¦]$', tokens[j - 1]):
                    pick = j
                    break
        if pick is None:
            pick = min(cand, key=lambda j: abs(j - idx)) if cand else idx
        # ë‹¨ì¡° ì¦ê°€ ë³´ì¥
        if boundaries and pick <= boundaries[-1]:
            pick = boundaries[-1] + 1
        pick = min(max(1, pick), len(tokens) - 1)
        boundaries.append(pick)

    # ê²½ê³„ë¡œ ë¶„í• 
    out = []
    start = 0
    for b in boundaries + [len(tokens)]:
        out.append(" ".join(tokens[start:b]).strip())
        start = b
    return [x for x in out if x]


def _smooth_chunks_by_flow(pieces, target_words=3, min_words=2, max_words=5):
    """
    pieces: ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸(ì´ë¯¸ ì¡°ê°ë‚œ ìë§‰)
    - 1ë‹¨ì–´/ë„ˆë¬´ì§§ì€ ì¡°ê°ì€ ì•/ë’¤ì™€ í•©ì¹¨
    - ë‹´í™”í‘œì§€(ê·¸ë¦¬ê³ /í•˜ì§€ë§Œ/ê·¼ë°/ê·¸ëŸ°ë°/ê·¸ëŸ¬ë‹ˆê¹Œ/ë˜/ë˜í•œ/ê²Œë‹¤ê°€/í•œí¸/ë°˜ë©´ì—/ì¦‰)ëŠ” ë’¤ ì¡°ê°ì— ë¶™ì´ëŠ” ê±¸ ìš°ì„ 
    - ë„ˆë¬´ ê¸¸ì–´ì§„ ì¡°ê°ì€ ë‹¨ì–´ ê²½ê³„ + êµ¬ë‘ì  ë’¤ë¥¼ ì„ í˜¸í•´ ë‹¤ì‹œ ë‚˜ëˆ”
    """
    discourse_heads = {"ê·¸ë¦¬ê³ ","í•˜ì§€ë§Œ","ê·¼ë°","ê·¸ëŸ°ë°","ê·¸ëŸ¬ë‹ˆê¹Œ","ë˜","ë˜í•œ","ê²Œë‹¤ê°€","í•œí¸","ë°˜ë©´ì—","ì¦‰"}

    # 1) í† í°í™”
    toks = [re.findall(r"\S+", p.strip()) for p in pieces if p.strip()]
    if not toks:
        return []

    # 2) 1ë‹¨ì–´/ì§§ì€ ì¡°ê° ë³‘í•©
    i = 0
    while i < len(toks):
        wc = len(toks[i])
        if wc >= min_words or len(toks) == 1:
            i += 1
            continue

        first_tok = toks[i][0] if toks[i] else ""
        # ë‹´í™”í‘œì§€ë§Œ ë‹¨ë…ì´ë©´ ë‹¤ìŒê³¼ í•©ì¹˜ê¸° ìš°ì„ 
        if first_tok in discourse_heads and i + 1 < len(toks):
            toks[i + 1] = toks[i] + toks[i + 1]
            toks.pop(i)
            continue

        # ê·¸ ì™¸: ì• ì¡°ê°ì´ ëª©í‘œë³´ë‹¤ ì§§ìœ¼ë©´ ì•ê³¼ í•©ì¹˜ê¸°, ì•„ë‹ˆë©´ ë’¤ì™€ í•©ì¹˜ê¸°
        prev_ok = (i > 0 and len(toks[i - 1]) < target_words)
        if prev_ok:
            toks[i - 1] = toks[i - 1] + toks[i]
            toks.pop(i)
        elif i + 1 < len(toks):
            toks[i] = toks[i] + toks[i + 1]
            toks.pop(i + 1)
        else:
            i += 1  # ë§ˆì§€ë§‰ í•˜ë‚˜ ë‚¨ì€ ì˜ˆì™¸
    # 3) ë„ˆë¬´ ê¸´ ì¡°ê°ì€ ìì—°ìŠ¤ëŸ½ê²Œ ì¬ë¶„í• (êµ¬ë‘ì  ì„ í˜¸)
    refined = []
    for tt in toks:
        wc = len(tt)
        if wc > max_words:
            n = max(2, round(wc / target_words))
            refined.extend(_split_tokens_into_n(tt, n, prefer_punct=True))
        else:
            refined.append(" ".join(tt).strip())

    # 4) ê° ì¡°ê° ëì˜ ê¼¬ë¦¬ êµ¬ë‘ì  ì •ë¦¬(ë”°ì˜´í‘œ/ê´„í˜¸ ë³´ì¡´)
    refined = [_strip_last_punct_preserve_closers(x) for x in refined if x.strip()]
    return refined