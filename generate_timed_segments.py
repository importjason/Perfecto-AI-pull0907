from deep_translator import GoogleTranslator
from ssml_converter import convert_line_to_ssml, breath_linebreaks, koreanize_if_english
from html import escape as _xml_escape
# generate_timed_segments.py
import os
import re, math
from elevenlabs_tts import generate_tts
from pydub import AudioSegment
import kss
import boto3, json
from elevenlabs_tts import TTS_POLLY_VOICES 
from botocore.exceptions import ClientError

def _split_script_by_llm_breath(script_text: str) -> list[str]:
    """
    ìŠ¤í¬ë¦½íŠ¸ë¥¼ ë¬¸ë‹¨ë³„ë¡œ ìª¼ê°  ë’¤, ê° ë¬¸ë‹¨ì„ LLM ë¸Œë ˆìŠ¤ ë¼ì¸ë“¤ë¡œ í™•ì¥.
    ë°˜í™˜: ìµœì¢… TTS/ì„¸ê·¸ë¨¼íŠ¸ìš© 'í•œ ì¤„ì”©' ë¦¬ìŠ¤íŠ¸(ë¹ˆ ì¤„ ì œê±°).
    """
    blocks = [b.strip() for b in re.split(r"\n{2,}", script_text or "") if b.strip()]
    lines: list[str] = []
    for b in blocks:
        try:
            parts = [ln for ln in breath_linebreaks(b) if ln.strip()]
        except Exception:
            parts = [b]
        lines.extend(parts)
    return lines or [script_text.strip()]

def split_script_to_lines(script_text, mode="llm"):
    """í•­ìƒ LLM ë¸Œë ˆìŠ¤ ë¶„ì ˆì„ ì‚¬ìš©í•œë‹¤."""
    text = script_text or ""
    lines = breath_linebreaks(text)  # LLM í˜¸ì¶œ
    return [ln.strip() for ln in lines if ln.strip()]

def _drop_special_except_q(text: str) -> str:
    # í•œê¸€/ì˜ë¬¸/ìˆ«ì/ê³µë°±/ë¬¼ìŒí‘œë§Œ ë‚¨ê¹€
    return re.sub(r"[^0-9A-Za-z\uac00-\ud7a3?\s]", "", text or "")

def _summarize_line_pitch(ssml: str) -> float | None:
    """ë¼ì¸ ì „ì²´ì˜ pitchë¥¼ ìš”ì•½. ë¹¨ê°•/íŒŒë‘ ì„ê³„ì— ì˜ë¯¸ ìˆê²Œ ë°˜ì‘í•˜ë„ë¡ ì„¤ê³„."""
    pieces = _parse_ssml_pieces(ssml or "")
    if not pieces:
        return None
    vals = [float(p.get("pitch_pct", 0)) for p in pieces if "pitch_pct" in p]
    if not vals:
        return None
    # ê·œì¹™: ë‚®ìŒ ê²½ê³  ìš°ì„ , ê·¸ë‹¤ìŒ ë†’ìŒ, ì•„ë‹ˆë©´ í‰ê· 
    low = min(vals)
    high = max(vals)
    if low <= -4:           # ë¹¨ê°• ì„ê³„ ì¶©ì¡± ì‹œ ê·¸ ê°’ì„ ì‚¬ìš©
        return low
    if high >= +8:          # íŒŒë‘ ì„ê³„ ì¶©ì¡± ì‹œ ê·¸ ê°’ì„ ì‚¬ìš©
        return high
    return sum(vals)/len(vals)

def _pitch_to_hex(p):
    """
    ASSëŠ” BGR ìˆœì„œ.
    - None/ë¯¸ë¯¸: ê¸°ë³¸(í•˜ì–‘)
    - ë‚®ìŒ(<= -6): ë¹¨ê°•  -> &H0000FF&
    - ë†’ìŒ(>= +6): íŒŒë‘ -> &HFF0000&
    """
    try:
        v = float(p)
    except Exception:
        return None
    if v <= -4:
        return "&H0000FF&"  # red
    if v >= +8:
        return "&HFF0000&"  # blue
    return None

def _strip_trailing_commas(s: str) -> str:
    return re.sub(r'[,\s]+$', '', (s or '').strip())

def harden_ko_sentence_boundaries(segments):
    """
    í•œêµ­ì–´ ë¬¸ì¥ ê²½ê³„ë¥¼ ë” ê°•í•˜ê²Œ ë³´ì •:
    - '?', 'â€¦ë‹¤/ìš”/ë‹ˆë‹¤/ìŠµë‹ˆë‹¤/ì…ë‹ˆë‹¤' ëì´ ì•„ë‹ˆë©´ ë’¤ ì¡°ê°ê³¼ ë³‘í•©
    - ë‹¤ìŒ ì¡°ê°ì´ 'ì´ë‹¤/ê²ƒì´ë‹¤/ìˆ˜ ìˆë‹¤/í•´ì•¼ í•œë‹¤/ì´ ìˆ«ìë©´/í•˜ì§€ë§Œ/ê·¸ë¦¬ê³ ' ë¥˜ ì ‘ì†ë¶€ë©´ ë³‘í•©
    - ë§¤ìš° ì§§ì€ ê¼¬ë¦¬(<=6ì)ëŠ” ì•ì— ë¶™ì„
    """
    END_STRONG_RE = re.compile(r'(?:\?|!|\.|ë‹¤|ìš”|ë‹ˆë‹¤|ìŠµë‹ˆë‹¤|ì…ë‹ˆë‹¤|ì˜ˆìš”|ì´ì—ìš”|ì˜€ë‹¤|ê² [ë‹¤ìŠµë‹ˆë‹¤])$')
    NEXT_TAIL_RE  = re.compile(r'^(?:ê·¸ë¦¬ê³ |í•˜ì§€ë§Œ|ê·¼ë°|ê·¸ëŸ°ë°|ê·¸ë˜ì„œ|ê·¸ëŸ¬ë‹ˆê¹Œ|ì¦‰|íŠ¹íˆ|ê²Œë‹¤ê°€|í•œí¸|ë°˜ë©´ì—|ë‹¤ë§Œ|'
                               r'ì´ë‹¤|ê²ƒì´ë‹¤|ê²ƒì…ë‹ˆë‹¤|ìˆ˜ ìˆë‹¤|ìˆ˜ ì—†ë‹¤|í•´ì•¼ í•œë‹¤|ì´ ìˆ«ìë©´)\b')

    out = []
    i = 0
    while i < len(segments):
        cur = dict(segments[i]); i += 1
        cur_text = (cur.get("text") or "").strip()

        while i < len(segments):
            nxt = segments[i]
            nxt_text = (nxt.get("text") or "").strip()

            # ì´ë¯¸ ê°•í•œ ëì´ë©´ ë©ˆì¶¤
            if END_STRONG_RE.search(cur_text):
                break
            # ë‹¤ìŒì´ ì ‘ì†/ë§ê¼¬ë¦¬ë¡œ ì‹œì‘í•˜ê±°ë‚˜ ë‹¤ìŒì´ ë„ˆë¬´ ì§§ì€ ê¼¬ë¦¬ë©´ ë³‘í•©
            if NEXT_TAIL_RE.match(nxt_text) or len(nxt_text) <= 6:
                cur["end"]  = float(nxt["end"])
                cur_text    = (cur_text.rstrip() + " " + nxt_text.lstrip()).strip()
                cur["text"] = cur_text
                i += 1
                continue
            break

        out.append(cur)
    return out

def _parse_ssml_pieces(ssml: str):
    """<prosody ...>text</prosody> (+ ì„ íƒì  <break>) ë¥¼ ìˆœì„œëŒ€ë¡œ ì¶”ì¶œ"""
    if not ssml: return []
    body = ssml
    if body.strip().startswith("<speak"):
        body = re.sub(r"^<speak[^>]*>|</speak>\s*$", "", body.strip(), flags=re.I)

    pieces = []
    pos = 0
    tag_re = re.compile(r'<prosody\b([^>]*)>(.*?)</prosody\s*>', re.I|re.S)
    brk_re = re.compile(r'<break\b[^>]*time="(\d+)ms"[^>]*/\s*>', re.I)

    for m in tag_re.finditer(body):
        attrs = m.group(1) or ""
        text  = (m.group(2) or "").strip()
        rate  = re.search(r'rate="([+\-]?\d+)%"', attrs)
        pitch = re.search(r'pitch="([+\-]?\d+)%"', attrs)
        rate_pct  = int(rate.group(1)) if rate else 150  # ê¸°ë³¸ 150%
        pitch_pct = int(pitch.group(1)) if pitch else 0

        # ì´ prosody ë’¤ì— ì¦‰ì‹œ ì˜¤ëŠ” break 1ê°œë¥¼ ì†Œë¹„(ìˆìœ¼ë©´)
        tail = body[m.end():]
        brk = brk_re.match(tail)
        brk_ms = int(brk.group(1)) if brk else 0

        if brk:  # ì†Œë¹„ëœ breakëŠ” ë³¸ë¬¸ì—ì„œ ì œê±°ëœë‹¤ê³  ê°€ì •
            pass

        pieces.append({
            "text": text,
            "rate_pct": rate_pct,
            "pitch_pct": pitch_pct,
            "break_ms": brk_ms,
        })
    return [p for p in pieces if p["text"]]

def _quantize_segments(segs, fps=24.0, clamp_start=None, clamp_end=None):
    """ASS/ë¹„ë””ì˜¤ íƒ€ì„ë¼ì¸(24fps)ì— ë§ì¶° ì‹œì‘/ëì„ í”„ë ˆì„ ë‹¨ìœ„ë¡œ ìŠ¤ëƒ…."""
    tick = 1.0 / float(fps)
    out, prev_end = [], None
    for s in segs:
        st = round(s["start"] / tick) * tick
        en = round(s["end"]   / tick) * tick
        if prev_end is not None and st < prev_end:
            st = prev_end
        if en <= st:  # ìµœì†Œ 1í”„ë ˆì„
            en = st + tick
        out.append({**s, "start": st, "end": en})
        prev_end = en
    if clamp_start is not None:
        out[0]["start"] = max(clamp_start, out[0]["start"])
    if clamp_end is not None:
        out[-1]["end"]  = min(clamp_end,  out[-1]["end"])
    return out

def _pitch_level_from_attr(pitch_str: str) -> str:
    # "+20%" / "-15%" / "+3st" ë“± â†’ ëŒ€ëµ í¼ì„¼íŠ¸/ì •ìˆ˜ë§Œ ì¶”ì¶œ
    import re
    m = re.search(r"(-?\d+)\s*%?", pitch_str or "")
    v = int(m.group(1)) if m else 0
    if v >= +10: return "high"
    if v <= -10: return "low"
    return "mid"

def _build_dense_from_ssml(line_ssml: str, seg_start: float, seg_end: float, fps: float = 24.0):
    """
    í•œ ì¤„(ì˜¤ë””ì˜¤ í•œ íŒŒì¼) SSMLì„ prosody ì¡°ê° ë‹¨ìœ„ë¡œ ì‹œê°„ ë¶„ë°° â†’ dense events ë°˜í™˜
    - ê° ì´ë²¤íŠ¸ì— pitch(ìˆ«ì %), pitch_level(high/mid/low) í¬í•¨
    """
    pcs = _parse_ssml_pieces(line_ssml)  # â† ê¸°ì¡´ í•¨ìˆ˜ ì‚¬ìš©
    if not pcs:
        return []

    # ì•ˆì „ ë””í´íŠ¸
    for p in pcs:
        p.setdefault("text", "")
        p.setdefault("rate_pct", 150)   # ë³´í†µ 100~200%, ë‚´ë¶€ ê°€ì¤‘ì¹˜ ê¸°ì¤€ 150ì„ ì¤‘ì‹¬ìœ¼ë¡œ
        p.setdefault("pitch_pct", 0)
        p.setdefault("break_ms", 0)

    dur = max(0.01, seg_end - seg_start)
    total_break = sum(p["break_ms"] for p in pcs) / 1000.0
    speech_dur  = max(0.0, dur - total_break)

    # rate ë°˜ì˜ ê°€ì¤‘ì¹˜ (rate ë†’ì„ìˆ˜ë¡ ê°™ì€ ê¸€ììˆ˜ë¼ë„ ë” ë¹¨ë¦¬ ì½ìœ¼ë‹ˆ ì‹œê°„ ì ê²Œ ë°°ë¶„)
    weights = []
    for p in pcs:
        char_len = max(1, len(p["text"]))
        rate_mul = max(0.1, float(p["rate_pct"]) / 150.0)  # 150%ë¥¼ ì¤‘ì‹¬ê°’ìœ¼ë¡œ
        w = char_len / rate_mul
        weights.append(w)
    W = sum(weights) or 1.0

    t = seg_start
    events = []
    for p, w in zip(pcs, weights):
        span = speech_dur * (w / W)
        t0 = t
        t1 = min(seg_end, t0 + span)

        pitch_pct = float(p.get("pitch_pct", 0))
        pitch_lvl = "high" if pitch_pct >= 10 else ("low" if pitch_pct <= -10 else "mid")

        events.append({
            "start": t0,
            "end":   t1,
            "text":  p["text"],
            "pitch": pitch_pct,        # ìˆ«ì % (ìƒ‰ìƒ ë§¤í•‘ ì‹œ ì‚¬ìš©)
            "pitch_level": pitch_lvl,  # í•„ìš”í•˜ë©´ ë¬¸ìì—´ ë ˆë²¨ë„ ì‚¬ìš© ê°€ëŠ¥
        })

        # prosody ì‚¬ì´ì˜ break ë°˜ì˜
        t = t1 + (p["break_ms"] / 1000.0)

    # í”„ë ˆì„ ê²©ì ìŠ¤ëƒ… + ë²”ìœ„ í´ë¨í”„ (í”„ë¡œì íŠ¸ì— ì´ë¯¸ ìˆëŠ” í—¬í¼ ì‚¬ìš©)
    try:
        return _quantize_segments(events, fps=fps, clamp_start=seg_start, clamp_end=seg_end)
    except NameError:
        # fallback: ì•„ì£¼ ì–•ì€ ìŠ¤ëƒ…
        tick = 1.0 / float(fps)
        out = []
        for ev in events:
            s = max(seg_start, round(ev["start"] / tick) * tick)
            e = min(seg_end,   round(ev["end"]   / tick) * tick)
            if e <= s: e = s + tick
            out.append({**ev, "start": round(s, 3), "end": round(e, 3)})
        # ê²¹ì¹¨ ë°©ì§€
        for i in range(len(out) - 1):
            if out[i]["end"] > out[i+1]["start"]:
                out[i]["end"] = max(out[i]["start"] + 0.02, out[i+1]["start"] - 0.001)
        return out

def _clean_for_align(s: str) -> str:
    return re.sub(r"[^0-9A-Za-z\uac00-\ud7a3]+", "", s or "").strip()

def _align_breath_to_wordmarks(breath_lines, marks, line_offset, line_end, min_piece_dur=0.35):
    """
    breath_lines: í˜¸í¡ ì¡°ê° ë¦¬ìŠ¤íŠ¸(ì›ë¬¸ ë³´ì¡´ ì¤„)
    marks: Polly speechmarks(type='word') for the line
    ë°˜í™˜: [{start, end, text, pitch}]
    """
    words = [(line_offset + (mk["time"]/1000.0), mk.get("value",""))
             for mk in marks if mk.get("type") == "word"]
    if not words:
        return []

    pieces, w_idx, w_n = [], 0, len(words)
    for breath in breath_lines:
        target_len = len(_clean_for_align(breath))
        if target_len == 0:  # ë¹ˆ í˜¸í¡ì¡°ê° ë°©ì§€
            continue

        st = words[w_idx][0] if w_idx < w_n else line_offset
        acc = ""
        while w_idx < w_n and len(_clean_for_align(acc)) < target_len:
            acc += words[w_idx][1]
            w_idx += 1

        en = words[w_idx][0] if w_idx < w_n else line_end

        st = max(line_offset, min(st, line_end))
        en = max(st, min(en, line_end))

        txt = re.sub(r"\s+", " ", breath).strip()
        if not txt:
            continue

        if (en - st) < min_piece_dur and pieces:
            pieces[-1]["end"]  = en
            pieces[-1]["text"] = _join_no_repeat(pieces[-1]["text"], txt)
        else:
            pieces.append({"start": st, "end": en, "text": txt, "pitch": _assign_pitch(txt)})

    if len(pieces) >= 2 and (pieces[-1]["end"] - pieces[-1]["start"]) < min_piece_dur:
        pieces[-2]["end"]  = pieces[-1]["end"]
        pieces[-2]["text"] = _join_no_repeat(pieces[-2]["text"], pieces[-1]["text"])
        pieces.pop()

    return pieces

def _join_no_repeat(a: str, b: str) -> str:
    import re
    A = re.sub(r"\s+", " ", (a or "")).strip()
    B = re.sub(r"\s+", " ", (b or "")).strip()
    if not A: return B
    if not B: return A
    if B in A: return A             # bê°€ aì— ì™„ì „íˆ í¬í•¨ â†’ aë§Œ
    if A in B: return B             # aê°€ bì— ì™„ì „íˆ í¬í•¨ â†’ bë§Œ
    A_toks, B_toks = A.split(), B.split()
    k = min(len(A_toks), len(B_toks))
    for n in range(k, 0, -1):
        # aì˜ ì ‘ë¯¸ == bì˜ ì ‘ë‘ â†’ ê²¹ì¹œ ë¶€ë¶„ ë¹¼ê³  ë¶™ì´ê¸°
        if A_toks[-n:] == B_toks[:n]:
            return " ".join(A_toks + B_toks[n:])
    return A + " " + B

def dedupe_adjacent_texts(segs):
    out = []
    prev_clean = None
    for s in segs:
        t = (s.get("text") or "").strip()
        t_clean = re.sub(r"\s+", " ", strip_ssml_tags(t)).strip()
        if out and t_clean == prev_clean:
            # ë°”ë¡œ ì´ì „ê³¼ ë™ì¼í•˜ë©´ ì‹œê°„ë§Œ ì´ì–´ë¶™ì„
            out[-1]["end"] = max(out[-1]["end"], s["end"])
        else:
            out.append(dict(s))
            prev_clean = t_clean
    return out

TAG_RE = re.compile(r"<[^>]+>")
def strip_ssml_tags(s: str) -> str:
    return TAG_RE.sub(" ", s or "")

# --- SSML guard helpers (ì›ë¬¸â‰ SSML ë¶ˆì¼ì¹˜/ì¤‘ë³µ ë°©ì§€) ---
import re as _re_guard

def _plain_text_from_ssml(ssml: str) -> str:
    t = _re_guard.sub(r"<[^>]+>", " ", ssml)  # íƒœê·¸ ì œê±°
    return _re_guard.sub(r"\s+", " ", t).strip()

def _tokenize_ko_en(s: str):
    # í•œê¸€/ì˜ë¬¸/ìˆ«ìë§Œ í† í°í™”(ê¸°í˜¸/ê³µë°± ë¬´ì‹œ)
    return _re_guard.findall(r"[0-9A-Za-z\uac00-\ud7a3]+", s or "")

def _ssml_safe_or_fallback(orig_line: str, ssml_fragment: str):
    """ì›ë¬¸ê³¼ LLM-SSML ë¶ˆì¼ì¹˜/ì¤‘ë³µì´ë©´ ê²°ì •ì  SSMLë¡œ í´ë°±"""
    plain = _plain_text_from_ssml(ssml_fragment)
    tok_o = _tokenize_ko_en(orig_line)
    tok_s = _tokenize_ko_en(plain)

    # ìƒˆ ë‹¨ì–´ ì‚½ì… íƒì§€
    inserted = [t for t in tok_s if t not in tok_o]
    # ì—°ì† ì¤‘ë³µ íƒì§€(â€œì–´ë–»ê²Œ ì–´ë–»ê²Œâ€ ë“±)
    repeated = any(tok_s[i] == tok_s[i-1] for i in range(1, len(tok_s)))

    if inserted or repeated or len(tok_s) < max(1, len(tok_o)//3):
        safe = f'<prosody rate="150%" volume="medium">{_xml_escape(orig_line)}</prosody>'
        return safe, True
    return ssml_fragment, False

def resolve_polly_voice_id(polly_voice_key: str, default="Seoyeon") -> str:
    return TTS_POLLY_VOICES.get(polly_voice_key, TTS_POLLY_VOICES.get("korean_female", default))

def _looks_ssml(s: str) -> bool:
    s = (s or "").strip()
    return s.startswith("<speak") or "<prosody" in s or "<break" in s

def _pick_engine_from_ssml(ssml: str) -> str:
    # ì˜¤ë””ì˜¤ í•©ì„±ê³¼ ë™ì¼ ê·œì¹™: pitch ìˆìœ¼ë©´ standard, ì—†ìœ¼ë©´ neural
    return "standard" if ' pitch="' in (ssml or "") else "neural"

def get_polly_speechmarks(text_or_ssml: str, voice_id: str,
                          types=("word",), region="ap-northeast-2"):
    """ì˜¤ë””ì˜¤ í•©ì„±ì— ì‚¬ìš©í•œ SSML/ì—”ì§„ê³¼ ë™ì¼ ì¡°ê±´ìœ¼ë¡œ SpeechMarksë¥¼ ë°›ì•„ì˜¨ë‹¤."""
    payload = (text_or_ssml or "").strip()
    if not payload:
        return []
    text_type = "ssml" if _looks_ssml(payload) else "text"
    engine = _pick_engine_from_ssml(payload)

    polly = boto3.client("polly", region_name=region)
    resp = polly.synthesize_speech(
        Text=payload,
        TextType=text_type,
        VoiceId=voice_id,
        OutputFormat="json",
        SpeechMarkTypes=list(types),
        Engine=engine,              # â˜… ì¤‘ìš”: ì˜¤ë””ì˜¤ì™€ ë™ì¼ ì—”ì§„
    )
    body = resp["AudioStream"].read().decode("utf-8", errors="ignore")
    return [json.loads(line) for line in body.splitlines() if line.strip()]

def resolve_polly_voice_id(polly_voice_key: str, tts_lang: str | None = "ko") -> str:
    """
    polly_voice_key: ì½”ë“œì—ì„œ ì“°ëŠ” í‚¤("korean_female1" ë“±)
    tts_lang: 'ko' | 'en' ë“±, í‚¤ê°€ ì—†ì„ ë•Œì˜ ê¸°ë³¸ê°’ ì„ íƒì—ë§Œ ì‚¬ìš©
    """
    v = TTS_POLLY_VOICES.get(polly_voice_key)
    if v:
        return v
    # í‚¤ê°€ ì—†ì„ ë•Œ ì•ˆì „ í´ë°±
    if tts_lang == "ko":
        return TTS_POLLY_VOICES.get("korean_female2", "Seoyeon")  # ko-KR
    return TTS_POLLY_VOICES.get("default_female", "Joanna")       # en-US

SUBTITLE_TEMPLATES = {
    "educational": {
        "Fontname": "NanumGothic",
        "Fontsize": 12,
        "PrimaryColour": "&H00FFFFFF",       # í°ìƒ‰ í…ìŠ¤íŠ¸
        "OutlineColour": "&H00000000",       # ê²€ì • ì™¸ê³½ì„ 
        "Outline": 2,
        "Alignment": 2,
        "MarginV": 40
    },
    "entertainer": {
        "Fontname": "NanumGothic",
        "Fontsize": 12,
        "PrimaryColour": "&H00FFFFFF",       # í°ìƒ‰ í…ìŠ¤íŠ¸
        "OutlineColour": "&H00000000",       # ê²€ì • ì™¸ê³½ì„ 
        "Outline": 2,
        "Alignment": 2,
        "MarginV": 40
    },
    "slow": {
        "Fontname": "NanumGothic",
        "Fontsize": 12,
        "PrimaryColour": "&H00FFFFFF",       # í°ìƒ‰ í…ìŠ¤íŠ¸
        "OutlineColour": "&H00000000",       # ê²€ì • ì™¸ê³½ì„ 
        "Outline": 2,
        "Alignment": 2,
        "MarginV": 40
    },
    "default": {
        "Fontname": "NanumGothic",
        "Fontsize": 12,
        "PrimaryColour": "&H00FFFFFF",       # í°ìƒ‰ í…ìŠ¤íŠ¸
        "OutlineColour": "&H00000000",       # ê²€ì • ì™¸ê³½ì„ 
        "Outline": 2,
        "Alignment": 2,
        "MarginV": 40
    },
    "korean_male": {
        "Fontname": "NanumGothic",
        "Fontsize": 12,
        "PrimaryColour": "&H00FFFFFF",       # í°ìƒ‰ í…ìŠ¤íŠ¸
        "OutlineColour": "&H00000000",       # ê²€ì • ì™¸ê³½ì„ 
        "Outline": 2,
        "Alignment": 2,
        "MarginV": 40
    },
    "korean_male2": {
        "Fontname": "NanumGothic",
        "Fontsize": 12,
        "PrimaryColour": "&H00FFFFFF",       # í°ìƒ‰ í…ìŠ¤íŠ¸
        "OutlineColour": "&H00000000",       # ê²€ì • ì™¸ê³½ì„ 
        "Outline": 2,
        "Alignment": 2,
        "MarginV": 40
    },
    "korean_female": {
        "Fontname": "NanumGothic",
        "Fontsize": 12,
        "PrimaryColour": "&H00FFFFFF",       # í°ìƒ‰ í…ìŠ¤íŠ¸
        "OutlineColour": "&H00000000",       # ê²€ì • ì™¸ê³½ì„ 
        "Outline": 2,
        "Alignment": 2,
        "MarginV": 40
    },
    "korean_female2": {
        "Fontname": "NanumGothic",
        "Fontsize": 12,
        "PrimaryColour": "&H00FFFFFF",       # í°ìƒ‰ í…ìŠ¤íŠ¸
        "OutlineColour": "&H00000000",       # ê²€ì • ì™¸ê³½ì„ 
        "Outline": 2,
        "Alignment": 2,
        "MarginV": 40
    }
}

def _looks_english(text: str) -> bool:
    # ë§¤ìš° ë‹¨ìˆœí•œ íœ´ë¦¬ìŠ¤í‹±: ì•ŒíŒŒë²³ì´ í•œê¸€ë³´ë‹¤ í™•ì‹¤íˆ ë§ìœ¼ë©´ ì˜ì–´ë¡œ ê°„ì£¼
    letters = len(re.findall(r'[A-Za-z]', text))
    hangul = len(re.findall(r'[\uac00-\ud7a3]', text))
    return letters >= max(3, hangul * 2)

def _detect_script_language(lines):
    eng = sum(_looks_english(x) for x in lines)
    kor = sum(bool(re.search(r'[\uac00-\ud7a3]', x)) for x in lines)
    return 'en' if eng > kor else 'ko'

def _maybe_translate_lines(lines, target='ko', only_if_src_is_english=True):
    if not lines:
        return lines
    try:
        src = _detect_script_language(lines)
        if only_if_src_is_english and src != 'en':
            # ì›ë¬¸ì´ ì˜ì–´ê°€ ì•„ë‹ ë•ŒëŠ” ê±´ë“œë¦¬ì§€ ì•ŠìŒ
            return lines
        if target is None or target == src:
            return lines
        tr = GoogleTranslator(source='auto', target=target)
        return [tr.translate(l) if l.strip() else l for l in lines]
    except Exception:
        # ë²ˆì—­ ì‹¤íŒ¨ ì‹œ ì›ë¬¸ ìœ ì§€ (í¬ë˜ì‹œ ë°©ì§€)
        return lines

def _validate_ssml(text: str) -> str:
    """
    Polly í˜¸ì¶œ ì „ SSML ì•ˆì „ì„± ê²€ì‚¬ ë° ë³´ì •
    - í—ˆìš© íƒœê·¸: speak, prosody, break
    - Neuralì—ì„œ ë¬¸ì œë˜ëŠ” pitch ì œê±°
    - ì†ì„±ì€ ìŒë”°ì˜´í‘œë¡œ í‘œì¤€í™”
    - ì—°ì† break/ê³¼ë„í•œ time ë³´ì •
    - ë¹ˆ prosody ì œê±°, íƒœê·¸ ë¶ˆì¼ì¹˜ ë³´ì •
    """
    if not text:
        return ""

    t = text.strip().replace("\ufeff", "")

    # 0) ë‚´ë¶€ì— <speak> ì¡°ê°ì´ ìˆìœ¼ë©´ ê±·ì–´ë‚´ê¸° (ë¼ì¸ ë‹¨ìœ„)
    t = re.sub(r"</?speak\s*>", "", t, flags=re.I)

    # 1) í—ˆìš© ì™¸ íƒœê·¸ëŠ” ì œê±° (speak/prosody/breakë§Œ ë‚¨ê¹€)
    t = re.sub(r"</?(?!prosody\b|break\b)[a-zA-Z0-9:_-]+\b[^>]*>", "", t)

    # 2) ë‹¨ì¼ì¸ìš© ì†ì„± â†’ ìŒë”°ì˜´í‘œ
    t = re.sub(r'(\b[a-zA-Z:-]+)=\'([^\']*)\'', r'\1="\2"', t)

    ## 3) pitch ì†ì„± ì œê±° (Neural í˜¸í™˜)
    #t = re.sub(r'\s+pitch="[^"]*"', "", t)

    # 4) break time ë³´ì •: ìˆ«ìmsë§Œ í—ˆìš©, 2000ms ì´ˆê³¼ì‹œ 2000msë¡œ clamp
    def _clamp_break(m):
        val = m.group(2)
        try:
            n = int(val)
        except Exception:
            n = 200  # ì´ìƒì¹˜ë©´ 200msë¡œ
        n = max(0, min(n, 2000))
        return f'{m.group(1)}{n}{m.group(3)}'
    t = re.sub(r'(<break\b[^>]*\btime=")(\d+)(ms"[^>]*/?>)', _clamp_break, t, flags=re.I)

    # 5) ì—°ì† break â†’ í•˜ë‚˜ë§Œ ë‚¨ê¹€
    t = re.sub(r'(?:<break\b[^>]*/>\s*){2,}', lambda m: re.findall(r'<break\b[^>]*/>', m.group(0))[0], t, flags=re.I)

    # 6) ë¹ˆ prosody ì œê±°
    t = re.sub(r"<prosody[^>]*>\s*</prosody>", "", t, flags=re.I)

    # 7) prosody ë‹«í˜ ë³´ì •
    open_count = len(re.findall(r"<prosody\b", t, flags=re.I))
    close_count = len(re.findall(r"</prosody>", t, flags=re.I))
    t += "</prosody>" * max(0, open_count - close_count)

    # 8) prosodyê°€ ì•„ì˜ˆ ì—†ìœ¼ë©´ ê¸°ë³¸ ë˜í•‘ (ì•ˆì „ ê¸°ë³¸ê°’)
    if "<prosody" not in t:
        t = f'<prosody rate="155%" volume="medium">{_xml_escape(t)}</prosody>'

    return t.strip()

def generate_tts_per_line(script_lines, provider, template, polly_voice_key="korean_female1"):
    audio_paths = []
    temp_audio_dir = "temp_line_audios"
    os.makedirs(temp_audio_dir, exist_ok=True)

    print(f"ë””ë²„ê·¸: ì´ {len(script_lines)}ê°œì˜ ìŠ¤í¬ë¦½íŠ¸ ë¼ì¸ì— ëŒ€í•´ TTS ìƒì„± ì‹œë„.")

    for i, line in enumerate(script_lines):
        line_audio_path = os.path.join(temp_audio_dir, f"line_{i}.mp3")
        try:
            # Pollyë©´ í•œ ë²ˆ ë” ì•ˆì „ ì²´í¬(ë¹ˆ prosody ì œê±° ë“±)
            line_ssml = _validate_ssml(line)

            # ì™„ì „ì²´ê°€ ì•„ë‹ˆë©´ <speak> ë˜í•‘(í˜¹ì‹œ ìƒìœ„ ë‹¨ê³„ì—ì„œ ëª» ê°ì‹¼ ê²½ìš° ëŒ€ë¹„)
            ls = line_ssml.strip()
            if provider == "polly" and not ls.startswith("<speak"):
                ls = f"<speak>{ls}</speak>"

            generate_tts(
                text=ls,
                save_path=line_audio_path,
                provider=provider,
                template_name=template,
                polly_voice_name_key=polly_voice_key
            )
            audio_paths.append(line_audio_path)
            print(f"ë””ë²„ê·¸: ë¼ì¸ {i+1} ('{line[:30]}...') TTS ìƒì„± ì„±ê³µ. íŒŒì¼: {line_audio_path}")
        except Exception as e:
            print(f"ì˜¤ë¥˜: ë¼ì¸ {i+1} ('{line[:30]}...') TTS ìƒì„± ì‹¤íŒ¨: {e}")
            continue
            
    print(f"ë””ë²„ê·¸: ìµœì¢… ìƒì„±ëœ ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ ìˆ˜: {len(audio_paths)}")
    if not audio_paths:
        raise RuntimeError("ë¼ì¸ë³„ TTSê°€ 0ê±´ ìƒì„±ë¨ (ê° ë¼ì¸ì˜ ì‹¤íŒ¨ ì‚¬ìœ ëŠ” ìœ„ ë¡œê·¸ ì°¸ì¡°)")
    
    return audio_paths

def merge_audio_files(audio_paths, output_path):
    merged = AudioSegment.empty()
    segments = []
    current_time = 0.0

    for path in audio_paths:
        a = AudioSegment.from_file(path)
        d = a.duration_seconds
        segments.append({"start": current_time, "end": current_time + d})
        merged += a
        current_time += d

    # âœ… ëŠê¹€ ë°©ì§€ìš© ê¼¬ë¦¬ ë¬´ìŒ
    tail = AudioSegment.silent(duration=120)
    merged += tail
    if segments:
        segments[-1]["end"] += 0.12

    merged.export(output_path, format="mp3")
    return segments

def get_segments_from_audio(audio_paths, script_lines):
    segments = []
    current_time = 0
    for i, audio_path in enumerate(audio_paths):
        try:
            audio = AudioSegment.from_file(audio_path)
            duration = audio.duration_seconds
            line = script_lines[i]
            segments.append({
                "start": current_time,
                "end": current_time + duration,
                "text": line
            })
            current_time += duration
        except Exception as e:
            print(f"ì˜¤ë¥˜: ì˜¤ë””ì˜¤ íŒŒì¼ {audio_path} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            continue
    return segments

# --- pitch í• ë‹¹ í•¨ìˆ˜ ---
def _assign_pitch(text: str) -> int:
    if text.endswith("?"):
        return 20     # ì§ˆë¬¸/ê²½ê³ 
    if text.endswith("ìŠµë‹ˆë‹¤") or text.endswith("í•©ë‹ˆë‹¤"):
        return -5     # ì¼ë°˜ ì„¤ëª…
    if text.endswith("ì´ë‹¤") or text.endswith("ì—†ìŠµë‹ˆë‹¤"):
        return -18    # ê²°ë¡ /ë‹¨ì •
    return 0

import os, re, math, unicodedata
from typing import List, Dict

NBSP = "\u00A0"

def _ass_time(t: float) -> str:
    """float ì´ˆ -> ASS ì‹œê°„ H:MM:SS.cs (centi-second, 2ìë¦¬)"""
    if t < 0: t = 0.0
    h = int(t // 3600)
    m = int((t % 3600) // 60)
    s = int(t % 60)
    cs = int(round((t - math.floor(t)) * 100))
    if cs == 100:
        s += 1
        cs = 0
    if s == 60:
        m += 1
        s = 0
    if m == 60:
        h += 1
        m = 0
    return f"{h:d}:{m:02d}:{s:02d}.{cs:02d}"

ASS_NL = r"\N"
NBSP   = "\u00A0"

def _sanitize_ass_text_for_dialog(text: str) -> str:
    """
    - ì—­ìŠ¬ë˜ì‹œ(\)ëŠ” ì ˆëŒ€ ì†ëŒ€ì§€ ì•ŠìŒ (ì¤„ë°”ê¿ˆ ASS_NL ë³´ì¡´)
    - { } ëŠ” ì „ê°ìœ¼ë¡œ ë°”ê¿” override íƒœê·¸ ì£¼ì… ë°©ì§€
    - ì“¸ë°ì—†ëŠ” íƒ­/ë‹¤ì¤‘ ê³µë°±ë§Œ ì •ë¦¬
    """
    import re
    t = (text or "").replace("\r", "")
    t = t.replace("{", "ï½›").replace("}", "ï½")
    t = re.sub(r"[ \t]{2,}", " ", t)
    return t.strip() or NBSP

def _best_two_line_break(text: str, max_len: int, min_each: int = 3) -> str:
    raw = text
    # ì¤„ë°”ê¿ˆ/ì œì–´ë¬¸ì ì œê±°(ë¼ì¸ ê³„ì‚°ìš©)
    plain = raw.replace(r"\N", " ").replace("\n", " ")

    # ì´ë¯¸ 1ì¤„ë¡œ ì¶©ë¶„í•˜ë©´ ê·¸ëŒ€ë¡œ
    if len(plain) <= max_len:
        return raw

    # NBSPëŠ” non-breakë¡œ ì·¨ê¸‰í•˜ë¯€ë¡œ, í›„ë³´ íƒìƒ‰ ì‹œ ì œì™¸
    # ì´ìƒì ì¸ ë¶„ê¸°ì : ëŒ€ëµ ì ˆë°˜ ê·¼ì²˜
    tgt = max_len  # 2ì¤„ ëª©í‘œë¼ë©´ ëŒ€ëµ ì²« ì¤„ max_len ë¶€ê·¼ì´ ìì—°ìŠ¤ëŸ¬ì›€

    # í›„ë³´ ìˆ˜ì§‘(ìš°ì„ ìˆœìœ„ ê·¸ë£¹)
    def _cands(chars):
        idx = []
        for m in re.finditer(chars, plain):
            i = m.start()
            # NBSP ê·¼ì²˜ ê¸ˆì§€: 'ë‹¨ì–´ ë¬¶ìŒ ë³´í˜¸'
            if i > 0 and plain[i-1] == NBSP: 
                continue
            if i < len(plain)-1 and plain[i+1] == NBSP:
                continue
            idx.append(i)
        return idx

    spaces = _cands(r" ")
    mild   = _cands(r"[,Â·:\-\/]")
    strong = _cands(r"[?ï¼Ÿ]")

    groups = [spaces, mild, strong]
    best = None
    best_score = 10**9

    for cand_group, weight in zip(groups, [0, 1, 2]):
        for i in cand_group:
            left  = plain[:i].rstrip()
            right = plain[i+1:].lstrip()
            if len(left) < min_each or len(right) < min_each:
                continue
            if len(left) > max_len or len(right) > max_len:
                continue
            # ìŠ¤ì½”ì–´: ëª©í‘œì ê³¼ ê·¼ì ‘ + ê·¸ë£¹ ê°€ì¤‘ì¹˜(ê³µë°±ì´ ê°€ì¥ ì„ í˜¸)
            score = abs(len(left) - tgt) * 2 + weight * 10
            if score < best_score:
                best_score, best = (score, i)

        if best is not None:
            break  # ë” ì¢‹ì€ ê·¸ë£¹ íƒìƒ‰ ì „ ì¢…ë£Œ(ê³µë°±ì—ì„œ ì„±ê³µ ì‹œ ê³ ì •)

    if best is None:
        # í´ë°±: max_lenì— ê°€ì¥ ê°€ê¹Œìš´ ê³µë°± ë˜ëŠ” ì•ˆì „ ìœ„ì¹˜
        cut = None
        for i in range(min_each, len(plain) - min_each):
            if plain[i] == " " and len(plain[:i]) <= max_len:
                cut = i
        if cut is None:
            cut = min(max_len, len(plain)-min_each)
        left, right = plain[:cut].rstrip(), plain[cut:].lstrip()
        return f"{left}\\N{right}"

    left, right = plain[:best].rstrip(), plain[best+1:].lstrip()
    return f"{left}\\N{right}"

def _prepare_text_for_lines(text: str, max_chars_per_line: int, max_lines: int) -> str:
    if not text:
        return NBSP

    # ì´ë¯¸ \Nì´ ìˆìœ¼ë©´(ì‚¬ìš©ì/ìƒìœ„ ë¡œì§ì—ì„œ ê°•ì œ ë¶„í•´) ê·¸ëŒ€ë¡œ ë‘ 
    if r"\N" in text:
        return text

    # 1ì¤„ë¡œ ì¶©ë¶„í•˜ë©´ ê·¸ëŒ€ë¡œ
    if len(text) <= max_chars_per_line or max_lines <= 1:
        return text

    # 2ì¤„ í—ˆìš©: ì¢‹ì€ ì§€ì ì—ì„œë§Œ ë¶„ë¦¬
    text2 = _best_two_line_break(text, max_chars_per_line, min_each=max(3, int(max_chars_per_line*0.3)))
    # í˜¹ì‹œë¼ë„ ê²°ê³¼ê°€ ê³¼ë„í•˜ê²Œ ê¸¸ë©´ ë§ˆì§€ë§‰ í´ë°±(í•˜ë“œ ì»·)
    if any(len(x) > max_chars_per_line for x in text2.split(r"\N")) and max_lines >= 2:
        raw = text.replace(r"\N", " ")
        cut = max_chars_per_line
        text2 = raw[:cut].rstrip() + r"\N" + raw[cut:].lstrip()

    # ë¼ì¸ ê°œìˆ˜ ì œí•œ
    parts = text2.split(r"\N")
    if len(parts) > max_lines:
        text2 = r"\N".join(parts[:max_lines-1] + [" ".join(parts[max_lines-1:])])

    # ë¹„ì–´ìˆëŠ” ë¼ì¸ì´ ìƒê¸°ì§€ ì•Šê²Œ NBSP ë³´ê°•
    fixed = []
    for p in text2.split(r"\N"):
        pp = p if p.strip().replace(NBSP, "") else NBSP
        fixed.append(pp)
    return r"\N".join(fixed)

def _strip_trailing_punct_last_line(text: str) -> str:
    """
    ë§ˆì§€ë§‰ ì¤„ ëì˜ 'ë³´ì´ëŠ” ë§ˆì¹¨í‘œ/ì—¬ë¶„ ê³µë°±'ë§Œ ì‚´ì§ ì œê±°(ë Œë” ì•ˆì •ì„±).
    ë¬¼ìŒí‘œ/ëŠë‚Œí‘œ/ì¢…ê²°ì–´ë¯¸ ë“±ì€ ë³´ì¡´.
    """
    if not text:
        return text
    lines = text.split(r"\N")
    last = lines[-1]
    # ì™„ì „ ê³µë€ ë³´í˜¸
    if not last.strip().replace(NBSP, ""):
        last = NBSP
    # ì•„ì£¼ ì•½í•œ ë§ˆì¹¨í‘œ/ì¤‘ë³µ ê³µë°±ë§Œ ì •ë¦¬
    last = re.sub(r"[ \t]+$", "", last)
    last = re.sub(r"[.]{2,}$", ".", last)   # "..." -> "."
    # í•œ ê¸€ìì§œë¦¬ ì¤„ ë°©ì§€ìš© NBSP
    if len(last.strip()) == 0:
        last = NBSP
    lines[-1] = last
    return r"\N".join(lines)

ASS_NL = r"\N"

# --- í…œí”Œë¦¿ ì„¹ì…˜ ë¡œë”(ì—¬ëŸ¬ë¶„ íŒŒì¼ì— ì´ë¯¸ ìˆìœ¼ë©´ ê·¸ê±¸ ì“°ì„¸ìš”) ---
def _resolve_template_blocks(template_name: str):
    script_info = [
        "[Script Info]","ScriptType: v4.00+","WrapStyle: 2",
        "ScaledBorderAndShadow: yes","PlayResX: 720","PlayResY: 1080",""
    ]
    styles = [
        "[V4+ Styles]",
        "Format: Name, Fontname, Fontsize, PrimaryColour, SecondaryColour, OutlineColour, BackColour, "
        "Bold, Italic, Underline, StrikeOut, ScaleX, ScaleY, Spacing, Angle, BorderStyle, Outline, Shadow, "
        "Alignment, MarginL, MarginR, MarginV, Encoding",
        "Style: Default, Arial, 50, &H00FFFFFF, &H000000FF, &H00000000, &H64000000, "
        "-1, 0, 0, 0, 100, 100, 0, 0, 1, 2, 0, 2, 30, 30, 40, 1",
        ""
    ]
    events_header = ["[Events]","Format: Layer, Start, End, Style, Name, MarginL, MarginR, MarginV, Effect, Text"]
    try:
        if 'SUBTITLE_TEMPLATES' in globals() and template_name in SUBTITLE_TEMPLATES:
            tpl = SUBTITLE_TEMPLATES[template_name]
            if isinstance(tpl, dict):
                if "script_info" in tpl:
                    si = tpl["script_info"]
                    if isinstance(si, str):  script_info = ["[Script Info]"]+[ln for ln in si.splitlines() if ln.strip()]+[""]
                    elif isinstance(si, (list,tuple)): script_info = ["[Script Info]"]+[str(x) for x in si if str(x).strip()]+[""]
                if "styles" in tpl:
                    st = tpl["styles"]
                    if isinstance(st, str):  styles = ["[V4+ Styles]"]+[ln for ln in st.splitlines() if ln.strip()]+[""]
                    elif isinstance(st, (list,tuple)): styles = ["[V4+ Styles]"]+[str(x) for x in st if str(x).strip()]+[""]
                if "events_header" in tpl:
                    eh = tpl["events_header"]
                    if isinstance(eh, str):  events_header = [ln for ln in eh.splitlines() if ln.strip()]
                    elif isinstance(eh, (list,tuple)): events_header = [str(x) for x in eh if str(x).strip()]
    except Exception:
        pass
    return script_info, styles, events_header

# --- ì—¬ê¸°ì„œë¶€í„°: BMJUA ê³ ì • ë¶€ë¶„ ---
FORCE_FONT_FAMILY = "BMJUA_ttf"                     # í°íŠ¸ íŒ¨ë°€ë¦¬ëª…(íŒŒì¼ëª…ê³¼ ë§ì¶° ê³ ì •)
FORCE_FONT_DIR    = os.path.join("assets","fonts")  # ì‹¤ì œ íŒŒì¼ ìœ„ì¹˜: assets/fonts/BMJUA_ttf.ttf

def _force_font_in_styles(styles_lines, family: str = FORCE_FONT_FAMILY):
    out = []
    fmt_fields = None
    for ln in styles_lines:
        s = ln.strip()
        if s.startswith("Format:"):
            # í•„ë“œ ì¸ë±ìŠ¤ íŒŒì•…(ë³´í†µ Name, Fontname, Fontsize, ...)
            fmt_fields = [x.strip() for x in ln.split(":",1)[1].split(",")]
            out.append(ln); continue
        if s.startswith("Style:"):
            try:
                prefix, rest = ln.split(":", 1)
                vals = [v.strip() for v in rest.split(",")]
                idx = 1  # ê¸°ë³¸: ë‘ ë²ˆì§¸ê°€ Fontname
                if fmt_fields and "Fontname" in fmt_fields:
                    idx = fmt_fields.index("Fontname")
                if idx < len(vals):
                    vals[idx] = family
                ln = f"{prefix}: {', '.join(vals)}"
            except Exception:
                pass
        out.append(ln)
    return out

def _first_style_name(styles_lines, default="Default"):
    for ln in styles_lines:
        m = re.match(r"\s*Style:\s*([^,]+)\s*,", ln)
        if m:
            return m.group(1).strip()
    return default

# íŒŒì¼ ìƒë‹¨ ì–´ë”˜ê°€(ì „ì—­)ì— ê³ ì • ìƒìˆ˜ ì¶”ê°€
ASS_FONT_FILE = os.path.abspath(os.path.join("assets", "fonts", "BMJUA_ttf.ttf"))
ASS_FONT_FAMILY = "BM JUA"  # BMJUA_ttf.ttfì˜ ë‚´ë¶€ íŒ¨ë°€ë¦¬ëª…(ì¼ë°˜ì ìœ¼ë¡œ ì´ ì´ë¦„)

def _ensure_styles_with_bmjua(styles_block_lines: list[str]) -> list[str]:
    """
    í…œí”Œë¦¿ì˜ [V4+ Styles] ë¸”ë¡ê³¼ ë¬´ê´€í•˜ê²Œ, BM JUA ì „ìš© ìŠ¤íƒ€ì¼ì„ ì¶”ê°€í•´ë‘¡ë‹ˆë‹¤.
    ì´ë²¤íŠ¸ëŠ” ì´ ìŠ¤íƒ€ì¼ ì´ë¦„ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    out = []
    seen_header = False
    for ln in styles_block_lines:
        out.append(ln)
        if ln.strip().lower().startswith("format:"):
            seen_header = True
    if not seen_header:
        # í˜¹ì‹œ í…œí”Œë¦¿ì´ í˜•ì‹ì„ ì•ˆì¼ìœ¼ë©´ ì•ˆì „ ê¸°ë³¸ í—¤ë” ì¶”ê°€
        out = [
            "[V4+ Styles]",
            "Format: Name, Fontname, Fontsize, PrimaryColour, SecondaryColour, OutlineColour, BackColour, "
            "Bold, Italic, Underline, StrikeOut, ScaleX, ScaleY, Spacing, Angle, BorderStyle, Outline, Shadow, "
            "Alignment, MarginL, MarginR, MarginV, Encoding"
        ] + out

    # â˜… BMJUA ì „ìš© ìŠ¤íƒ€ì¼ì„ 'Style: BMJua'ë¡œ ì¶”ê°€
    #  - Fontsize/Outline/MarginVëŠ” ì›ë˜ ì“°ì‹œë˜ ê°’ ë²”ìœ„ì— ë§ì¶° ì ë‹¹íˆ; í•„ìš”ì‹œ ì¡°ì •
    out.append(
        f"Style: BMJua, {ASS_FONT_FAMILY}, 58, &H00FFFFFF, &H000000FF, &H00000000, &H64000000, " #ê¸€ìí¬ê¸° 58ë¡œ ìˆ˜ì •
        f"-1, 0, 0, 0, 100, 100, 0, 0, 1, 4, 0, 2, 30, 30, 60, 1"
    )
    out.append("")  # ë§ˆì§€ë§‰ ê³µë°± ì¤„
    return out

def generate_ass_subtitle(
    segments,
    ass_path: str,
    template_name: str = "educational",
    strip_trailing_punct_last: bool = True,
    max_chars_per_line: int | None = None,
    max_lines: int | None = None,
    wrap_mode: str = "preserve"  # "preserve" | "smart"
) -> str:
    """
    wrap_mode:
      - "preserve": í…ìŠ¤íŠ¸ë¥¼ ì ˆëŒ€ ë˜í•‘í•˜ì§€ ì•Šê³  í•œ ì¤„ ê·¸ëŒ€ë¡œ ê¸°ë¡ (ì¶”ì²œ)
      - "smart": ê¸¸ì´ ê¸°ì¤€ 2ì¤„ ë¶„í•  ë“± ê¸°ì¡´ ë™ì‘ ìœ ì§€
    """
    if not segments:
        segments = [{"start": 0.00, "end": 0.02, "text": NBSP}]

    script_info, styles, events_header = _resolve_template_blocks(template_name)
    styles = _ensure_styles_with_bmjua(styles)

    lines = []
    for ev in segments:
        s = float(ev.get("start", 0.0))
        e = float(ev.get("end", s + 0.02))
        if e <= s:
            e = s + 0.02
        if ev is segments[-1]:
            e = max(e, s + 0.35)

        raw_text = (ev.get("text") or "")

        def _line_clean(one: str) -> str:
            t = one or ""
            if strip_trailing_punct_last:
                t = _strip_last_punct_preserve_closers(t)
            t = _drop_special_keep_units(t)  # '?', %, Â°, â„ƒ, Â°C, Â°F, km/h, ã¦ ë“± ë‹¨ìœ„ ë³´ì¡´
            t = _sanitize_ass_text_for_dialog(t)
            return t

        if wrap_mode == "preserve":
            # ğŸš« ì ˆëŒ€ ì¤„ë°”ê¿ˆ/ë˜í•‘ ì•ˆ í•¨ â†’ ì›ë¬¸ ê·¸ëŒ€ë¡œ í•œ ì¤„
            plan_text = _line_clean(raw_text).strip() or NBSP
        else:
            # ğŸ”€ ê¸°ì¡´ì²˜ëŸ¼ \N ì¡´ì¤‘ + ê°€ê³µ
            normalized = _line_clean(raw_text)
            plan_text = normalized

        # â‘¢ pitch â†’ ìƒ‰ìƒ
        col_hex = _pitch_to_hex(ev.get("pitch"))
        if col_hex:
            plan_text = "{\\c" + col_hex + "}" + plan_text

        dlg = f"Dialogue: 0,{_ass_time(s)},{_ass_time(e)},BMJua,,0,0,0,,{plan_text}"
        lines.append(dlg)

    os.makedirs(os.path.dirname(ass_path) or ".", exist_ok=True)
    with open(ass_path, "w", encoding="utf-8") as f:
        f.write("\n".join(script_info) + "\n")
        f.write("\n".join(styles) + "\n")
        f.write("\n".join(events_header) + "\n")
        f.write("\n".join(lines) + "\n")

    return ass_path

def format_ass_timestamp(seconds):
    h = int(seconds // 3600)
    m = int((seconds % 3600) // 60)
    s = int(seconds % 60)
    cs = int((seconds - int(seconds)) * 100)
    return f"{h:01}:{m:02}:{s:02}.{cs:02}"

def _drop_special_keep_units(s: str) -> str:
    """
    ìë§‰ í…ìŠ¤íŠ¸ì—ì„œ ì¥ì‹ì„± íŠ¹ìˆ˜ë¬¸ìë¥¼ ì§€ìš°ë˜,
    ìˆ«ì/ë‹¨ìœ„/ì§ˆì˜ë¶€í˜¸ëŠ” ë³´ì¡´:
    - ë³´ì¡´: í•œê¸€/ì˜ë¬¸/ìˆ«ì/ê³µë°±/NBSP, ë¬¼ìŒí‘œ(?),
            %, Â°, â„ƒ, â„‰, 'Â°C','Â°F', ìŠ¬ë˜ì‹œ(/),
            CJK í˜¸í™˜ ë‹¨ìœ„(ã, ã¦, ã¥ ë“± U+3300~U+33FF)
    - ì†Œìˆ˜ì ì€ 'ìˆ«ì.ìˆ«ì' ì•ˆì—ì„œë§Œ ë³´ì¡´
    - ê·¸ ì™¸ ê¸°í˜¸(ë”°ì˜´í‘œ, ë§ˆì¹¨í‘œ, ì½œë¡ , ì„¸ë¯¸ì½œë¡  ë“±) ì œê±°
    """
    NBSP = "\u00A0"
    if not s:
        return s

    # ì†Œìˆ˜ì  ë³´í˜¸
    s = re.sub(r'(?<=\d)\.(?=\d)', 'Â§DECIMALÂ§', s)

    # 'Â°C' / 'Â°F'ëŠ” ê·¸ëŒ€ë¡œ ë‘ 
    s = s.replace("Â°C", "Â°C").replace("Â°F", "Â°F")

    # í—ˆìš© ë¬¸ì ì™¸ ì œê±°
    allowed = r"[A-Za-z\uAC00-\uD7A30-9 \t" + NBSP + r"\?%Â°/â„ƒâ„‰\u3300-\u33FF]"
    s = re.sub(rf"[^{allowed}]+", "", s)

    # ë³´í˜¸ ì†Œìˆ˜ì  ë³µì›
    s = s.replace('Â§DECIMALÂ§', '.')

    # ì½¤ë§ˆëŠ” ìˆ«ì ì‚¬ì´ê°€ ì•„ë‹ˆë©´ ì œê±°(ì›í•˜ì‹œë©´ ì „ë¶€ ì œê±°í•´ë„ ë¬´ë°©)
    s = re.sub(r'(?<!\d),(?!\d)', '', s)

    # ì•ë’¤ ê³µë°± ì •ë¦¬
    return re.sub(r"\s{2,}", " ", s).strip()

def split_script_to_lines(script_text: str, mode="llm") -> list[str]:
    text = (script_text or "").strip()
    if not text:
        return []

    # âœ¨ ê°œí–‰ì´ ìˆìœ¼ë©´ ìš°ì„  í•˜ë“œ ê²½ê³„ë¡œ ìª¼ê°¬
    if "\n" in text:
        base_lines = [ln.strip() for ln in text.splitlines() if ln.strip()]
    else:
        base_lines = [text]

    if mode == "newline":
        return base_lines

    # mode == "llm": ê° ì¤„ì„ breath_linebreaksì— ë„£ë˜, honor_newlines=Trueë¡œ ì¶”ê°€ ë¶„ì ˆ ë°©ì§€
    out = []
    for line in base_lines:
        out.extend(breath_linebreaks(line, honor_newlines=True))
    return out

# --- ë³€ê²½ 2: generate_subtitle_from_script ì‹œê·¸ë‹ˆì²˜/ë¡œì§ í™•ì¥ ---
def generate_subtitle_from_script(
    script_text: str,
    ass_path: str,
    full_audio_file_path: str,
    provider: str = "polly",
    template: str = "default",
    polly_voice_key: str = "default_male",
    subtitle_lang: str = "ko",
    translate_only_if_english: bool = False,
    tts_lang: str | None = None,
    split_mode = "llm",
    strip_trailing_punct_last: bool = True,
):
    """
    ëª©ì : 'ë¼ì¸ ë‹¨ìœ„ ì„¸ê·¸ë¨¼íŠ¸(base)'ë§Œ ë°˜í™˜í•˜ê³ , ê° ì„¸ê·¸ë¨¼íŠ¸ì— SSMLì„ ì‹¤ì–´ ë©”ì¸ì—ì„œ densify í•˜ë„ë¡ í•œë‹¤.
    - ì—¬ê¸°ì„œëŠ” ASS ìƒì„±/ìë§‰ ìª¼ê°œê¸°/ë³‘í•©ì„ í•˜ì§€ ì•ŠëŠ”ë‹¤.
    - ë©”ì¸ì—ì„œ auto_densify_for_subs(...)ê°€ SSML( rate/pitch/break )ì„ ì½ì–´ SpeechMarks ê¸°ë°˜ìœ¼ë¡œ ì •í™•íˆ ìª¼ê°¤ ìˆ˜ ìˆê²Œ í•¨.
    ë°˜í™˜: (segments_base, audio_clips, ass_path)
    """

    # --- 0) ë³´ì¡°
    def _strip_punct_and_quotes(s: str) -> str:
        if not s: return ""
        s = s.translate(str.maketrans({
            "â€œ": "", "â€": "", "â€": "", "â€Ÿ": "", '"': "",
            "â€˜": "", "â€™": "", "â€š": "", "â€›": "", "'": "",
            "ï¼": "!", "ï¼Ÿ": "?"
        }))
        s = re.sub(r'\s{2,}', ' ', s)
        return s.strip()

    # --- 1) ìŠ¤í¬ë¦½íŠ¸ â†’ ë¼ì¸
    base_lines = split_script_to_lines(script_text or "", mode=split_mode)
    base_lines = [ln for ln in base_lines if ln.strip()]
    if not base_lines:
        return [], None, ass_path

    # SSML íƒœê·¸ ì œê±°í•œ í´ë¦° í…ìŠ¤íŠ¸(SSML ìƒì„±ì„ ìœ„í•´)
    clean_lines = split_script_to_lines(script_text, mode=split_mode)

    # --- 2) Polly ë³´ì´ìŠ¤/ì–¸ì–´ ì •í•©
    if provider.lower() == "polly":
        if tts_lang == "en" and polly_voice_key.startswith("korean_"):
            polly_voice_key = "default_male"
        elif tts_lang == "ko" and polly_voice_key.startswith("default_"):
            polly_voice_key = "korean_female1"

    # --- 3) ë¼ì¸ë³„ SSML ìƒì„±(Polly) ë˜ëŠ” ì›ë¬¸(íƒ€ ê³µê¸‰ì)
    prov = provider.lower()
    tts_lines = []
    ssml_meta_lines = []          # â˜… ëª¨ë“  ê³µê¸‰ì ê³µí†µ: ë©”íƒ€ íŒŒì‹±ìš© SSML

    for ln in clean_lines:
        ln_for_ssml = koreanize_if_english(ln)   # â˜… ì˜ë¬¸ì´ë©´ ì˜ë¯¸ ë™ì¼ í•œêµ­ì–´ë¡œ
        try:
            frag = convert_line_to_ssml(ln_for_ssml)  # <prosody>...</prosody> (+ <break/>)
        except Exception:
            frag = f'<prosody rate="150%" volume="medium">{_xml_escape(ln_for_ssml)}</prosody>'

        safe = _validate_ssml(frag)
        if not safe.strip().startswith("<speak"):
            safe = f"<speak>{safe}</speak>"
        ssml_meta_lines.append(safe)

    # TTSì— ë„˜ê¸¸ ë¼ì¸: Pollyë©´ SSML, ì•„ë‹ˆë©´ í‰ë¬¸
    if prov == "polly":
        tts_lines = ssml_meta_lines[:]
    else:
        tts_lines = clean_lines[:]

    # --- 4) ë¼ì¸ë³„ TTS ìƒì„± â†’ ë³‘í•©
    audio_paths = generate_tts_per_line(
        tts_lines, provider=provider, template=template, polly_voice_key=polly_voice_key
    )
    if not audio_paths:
        return [], None, ass_path

    segments_raw = merge_audio_files(audio_paths, full_audio_file_path)
    # segments_raw: [{"start":..., "end":...}, ...]

    # --- 5) â˜…â˜…â˜… ë¼ì¸ ë‹¨ìœ„ 'base ì„¸ê·¸ë¨¼íŠ¸' êµ¬ì„±: SSMLì„ ì‹¬ëŠ”ë‹¤
    # text: ì›ë¬¸(ë˜ëŠ” í‘œì‹œìš© ìë§‰ ê¸°ë³¸ê°’), ssml: Pollyì— ì‹¤ì œ ë³´ë‚¸ SSML
    if len(clean_lines) != len(base_lines):
        try:
            import streamlit as st
            st.warning(f"ë¼ì¸ ë¶ˆì¼ì¹˜: base={len(base_lines)} vs clean={len(clean_lines)} â†’ base ê¸°ì¤€ìœ¼ë¡œ ê°•ì œ ì •ë ¬")
        except Exception:
            print(f"[warn] ë¼ì¸ ë¶ˆì¼ì¹˜: base={len(base_lines)} vs clean={len(clean_lines)}")
        clean_lines = base_lines[:]
        ssml_meta_lines = ssml_meta_lines[:len(base_lines)]

    # ì˜¤ë””ì˜¤ ë³‘í•© í›„, ì„¸ê·¸ë¨¼íŠ¸/í…ìŠ¤íŠ¸/SSML ìµœì†Œê¸¸ì´ë¡œ ë™ê¸°í™”
    segments_raw = merge_audio_files(audio_paths, full_audio_file_path)
    n = min(len(segments_raw), len(base_lines), len(ssml_meta_lines))
    if n != len(segments_raw):
        try:
            st.warning(f"TTS ì„¸ê·¸ë¨¼íŠ¸ ìˆ˜ ë¶ˆì¼ì¹˜: audio={len(segments_raw)} vs base={len(base_lines)} â†’ min={n}ë¡œ ë§ì¶¤")
        except Exception:
            print(f"[warn] ì„¸ê·¸ë¨¼íŠ¸ ìˆ˜ ë¶ˆì¼ì¹˜: audio={len(segments_raw)} base={len(base_lines)} â†’ {n}")

    segments_base = []
    for i in range(n):
        s = segments_raw[i]
        line_text = base_lines[i]
        ssml_meta = ssml_meta_lines[i]
        pitch_sum = _summarize_line_pitch(ssml_meta)
        segments_base.append({
            "start": float(s["start"]),
            "end":   float(s["end"]),
            "text":  re.sub(r"\s+", " ", line_text).strip(),
            "ssml":  ssml_meta,
            "pitch": pitch_sum,
        })


    # --- 6) ì—¬ê¸°ì„œëŠ” ASS/ìë§‰ ë¶„í•´ë¥¼ í•˜ì§€ ì•ŠëŠ”ë‹¤(ë©”ì¸ì—ì„œ ì²˜ë¦¬)
    # generate_ass_subtitle(...) í˜¸ì¶œ ê¸ˆì§€

    # audio_clipsëŠ” ì´ í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ ì—´ì—ˆë‹¤ê°€ ë‹«ì§€ ì•Šìœ¼ë‹ˆ None ë°˜í™˜(ê¸°ì¡´ ê´€ë¡€ ìœ ì§€)
    return segments_base, None, ass_path

# === Auto-paced subtitle densifier (ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ë§¥ ë¶„í•  ìš°ì„ ) ===
def _auto_split_for_tempo(text: str, tempo: str = "fast"):
    """
    tempo: fast(ì§§ê²Œ) | medium | slow
    ë¶„í•  ìš°ì„ ìˆœìœ„:
    1) ë‹´í™” í‘œì§€(ê·¸ë¦¬ê³ /í•˜ì§€ë§Œ/ê·¸ë˜ì„œ/ê·¸ëŸ°ë°/ê·¸ëŸ¬ë‹ˆê¹Œ/ì¦‰/íŠ¹íˆ/ë°˜ë©´ì—/ê²Œë‹¤ê°€/ë˜ëŠ”/í˜¹ì€ ë“±) 'ë’¤'ì—ì„œ ëŠê¸°
    2) ì—°ê²° ì–´ë¯¸(ê³ /ì§€ë§Œ/ëŠ”ë°/ë©´ì„œ/ë¼ë©´/ë©´/ë‹ˆê¹Œ/ë‹¤ê°€/ìœ¼ë©°/ë©° ë“±) 'ë’¤'ì—ì„œ ëŠê¸°
    3) ì‰¼í‘œ/ì„¸ë¯¸ì½œë¡ /ì¤‘ì  ë“± êµ¬ë‘ì  ë’¤ì—ì„œ ëŠê¸°
    4) ì—¬ì „íˆ ê¸¸ë©´ ê³µë°± ê·¼ì²˜ë¡œ ê¸¸ì´ ê¸°ë°˜ ë¶„í• 
    ì´í›„ ë„ˆë¬´ ì§§ì€ ì¡°ê°ì€ ì´ì›ƒê³¼ ë³‘í•©
    """
    import re

    # ê¸¸ì´ ëª©í‘œ(ìƒí™© ë§ê²Œ ì¡°ì ˆ)
    max_len_map = {"fast": 12, "medium": 18, "slow": 24}
    max_len = max_len_map.get(tempo, 18)
    min_piece_chars = 2  # ë„ˆë¬´ ì§§ì€ ì¡°ê° ë³‘í•© ê¸°ì¤€

    t = (text or "").strip()
    if not t:
        return []

    # 1) ë‹´í™” í‘œì§€ í›„ ë¶„í• (í† í°ì€ ì• ì¡°ê°ì— ë‘ )
    discourse = r"(ê·¸ë¦¬ê³ |í•˜ì§€ë§Œ|ê·¼ë°|ê·¸ëŸ°ë°|ê·¸ë˜ì„œ|ê·¸ëŸ¬ë‹ˆê¹Œ|ì¦‰|íŠ¹íˆ|ê²Œë‹¤ê°€|í•œí¸|ë°˜ë©´ì—|ë˜ëŠ”|í˜¹ì€|ë‹¤ë§Œ)"
    t = re.sub(rf"\b{discourse}\b\s*", r"\g<0>Â§", t)

    # 2) ì—°ê²° ì–´ë¯¸ í›„ ë¶„í• (ì–´ë¯¸ëŠ” ì• ì¡°ê°ì— ë‘ )
    eomi = r"(ê³ |ì§€ë§Œ|ëŠ”ë°ìš”?|ë©´ì„œ|ë©°|ë¼ë©´|ë©´|ë‹ˆê¹Œ|ë‹¤ê°€|ìœ¼ë©°|ê±°ë‚˜|ë“ ì§€)"
    t = re.sub(rf"({eomi})(?=\s|\Z)", r"\1Â§", t)

    # 3) ì‰¼í‘œÂ·ì„¸ë¯¸ì½œë¡ Â·ì¤‘ì  ë’¤ ë¶„í•  (êµ¬ë‘ì ì€ ì• ì¡°ê°ì—)
    t = re.sub(r"(?<=[,ï¼Œã€;:Â·])\s*", "Â§", t)

    # ì¼ì°¨ ë¶„í• 
    parts = [p.strip() for p in t.split("Â§") if p.strip()]
    chunks = []

    # 4) ê¸¸ì´ ë³´ì •: ë„ˆë¬´ ê¸´ ê±´ ê³µë°± ê·¼ì²˜ë¡œ ì¶”ê°€ ë¶„í• 
    for p in parts:
        if len(p) <= max_len:
            chunks.append(p)
            continue

        cur = p
        while len(cur) > max_len:
            window = cur[: max_len + 6]  # ì—¬ìœ 
            spaces = [m.start() for m in re.finditer(r"\s", window)]
            split_pos = spaces[-1] if spaces else max_len
            chunks.append(cur[:split_pos].strip())
            cur = cur[split_pos:].strip()
        if cur:
            chunks.append(cur)

    # 5) ë„ˆë¬´ ì§§ì€ ì¡°ê° ë³‘í•©(ì–‘ ì˜†ê³¼ ìì—°ìŠ¤ëŸ¬ìš´ ê³µë°± ì²˜ë¦¬)
    def _wordish(ch: str) -> bool:
        return ch.isalnum() or ('\uAC00' <= ch <= '\uD7A3')  # ì˜ë¬¸/ìˆ«ì/í•œê¸€
    i = 1
    while i < len(chunks):
        if len(chunks[i]) < min_piece_chars:
            prev = chunks[i-1].rstrip()
            cur  = chunks[i].lstrip()
            sep = " " if (prev and cur and _wordish(prev[-1]) and _wordish(cur[0])) else ""
            chunks[i-1] = (prev + sep + cur).strip()
            chunks.pop(i)
        else:
            i += 1

    return chunks

def auto_densify_for_subs(
    segments,
    tempo: str = "fast",
    words_per_piece: int = 3,
    min_tail_words: int = 2,
    chunk_strategy: str | None = None,
    marks_voice_key: str | None = None,
    max_chars_per_piece: int = 16,
    min_piece_dur: float = 0.42,
):
    """
    ë¼ì¸(=TTS í•œ ë²ˆ) ê¸°ì¤€ ì„¸ê·¸ë¨¼íŠ¸ë¥¼ 'ë¹ ë¥¸ í…œí¬'ë¡œ ì˜ê²Œ ìª¼ê°­ë‹ˆë‹¤.
    - ê° ì…ë ¥ ì„¸ê·¸ë¨¼íŠ¸ëŠ” {"start","end","text","ssml"} êµ¬ì¡°ë¼ê³  ê°€ì •
    - ë§ê¼¬ë¦¬/ì¢…ê²°ì–´ë¯¸ë¥¼ ë³´í˜¸í•˜ê³ , ë„ˆë¬´ ì§§ì€ ê¼¬ë¦¬ëŠ” ì• ì¡°ê°ì— ë¶™ì…ë‹ˆë‹¤.
    - piecesì˜ ì‹œê°„ì€ ì› ì„¸ê·¸ë¨¼íŠ¸ êµ¬ê°„ ë‚´ì—ì„œë§Œ ë°°ë¶„ë˜ë©°, ë‹¤ìŒ ì„¸ê·¸ë¨¼íŠ¸ì™€ ê²¹ì¹˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    - (ì„ íƒ) max_chars_per_pieceë¡œ ì¡°ê° ê¸¸ì´ë¥¼ í•˜ë“œìº¡í•©ë‹ˆë‹¤(í•œêµ­ì–´ 12~18 ì¶”ì²œ).
    """
    out = []
    if not segments:
        return out

    # í…œí¬ë³„ ê¸°ë³¸ íŒŒë¼ë¯¸í„°
    if tempo == "fast":
        base_words = max(1, min(5, words_per_piece))
        min_dur = float(min_piece_dur)
    elif tempo == "normal":
        base_words = max(2, min(6, words_per_piece + 1))
        min_dur = max(0.5, float(min_piece_dur))
    else:
        base_words = max(3, min(7, words_per_piece + 2))
        min_dur = max(0.6, float(min_piece_dur))

    # ì¢…ê²°/ë§ê¼¬ë¦¬ ë³´í˜¸
    END_STRONG_RE = re.compile(r'(?:\?|â€¦|ì´ë‹¤|ë‹¤|ìš”|ì£ |ë‹ˆë‹¤|ìŠµë‹ˆë‹¤|ì…ë‹ˆë‹¤|ì˜ˆìš”|ì´ì—ìš”|ì˜€(?:ë‹¤|ìŠµë‹ˆë‹¤)|ê² (?:ë‹¤|ì£ )|ë§(?:ì£ |ë‹¤))$')

    def _tokenize_for_chunks(text: str):
        # í•œêµ­ì–´/ì˜ì–´/ìˆ«ì/ë¶€í˜¸ ë¶„ë¦¬ (ê³µë°± í¬í•¨ ìœ ì§€)
        toks = re.findall(r'[\uAC00-\uD7A3A-Za-z0-9]+|[^\s]', text or "")
        # ê³µë°± ë³µì›
        merged, prev_is_word = [], False
        for t in toks:
            if re.match(r'^\s+$', t):
                merged.append(t)
                prev_is_word = False
            elif re.match(r'^[\uAC00-\uD7A3A-Za-z0-9]+$', t):
                # ë‹¨ì–´
                if merged and not re.match(r'^\s+$', merged[-1]):
                    merged.append(' ')
                merged.append(t)
                prev_is_word = True
            else:
                # êµ¬ë‘ì /ê¸°í˜¸ëŠ” ë¶™ì—¬ì“°ê¸°
                merged.append(t)
                prev_is_word = False
        s = ''.join(merged).strip()
        parts = re.findall(r'\S+|\s+', s)
        return parts

    def _join_parts(parts):
        return ''.join(parts).strip()

    for seg in segments:
        s0 = float(seg["start"])
        e0 = float(seg["end"])
        t  = (seg.get("text") or "").strip()
        if not t:
            continue
        parts = _tokenize_for_chunks(t)

        # ê¸¸ì´ ê¸°ì¤€ìœ¼ë¡œ ì¡°ê°ë‚´ê¸°
        pieces = []
        cur, cur_chars, cur_words = [], 0, 0
        def _flush():
            nonlocal cur, cur_chars, cur_words
            if not cur:
                return
            txt = _join_parts(cur)
            pieces.append(txt)
            cur, cur_chars, cur_words = [], 0, 0

        for p in parts:
            if p.isspace():
                cur.append(p)
                cur_chars += len(p)
                continue
            cur.append(p)
            cur_chars += len(p)
            if re.match(r'^[\uAC00-\uD7A3A-Za-z0-9]+$', p):
                cur_words += 1
            # í•˜ë“œìº¡: ê¸€ììˆ˜ ì´ˆê³¼ ë˜ëŠ” ë‹¨ì–´ìˆ˜ ì´ˆê³¼ì¼ ë•Œ ëŠê¸°
            if cur_chars >= max_chars_per_piece or cur_words >= base_words:
                # ì¢…ê²° ê¼¬ë¦¬ ë³´í˜¸ëŠ” harden ë‹¨ê³„ì—ì„œ ì¶”ê°€ë¡œ ì²˜ë¦¬
                _flush()

        _flush()
        # ë³‘í•© ê·œì¹™: ì•„ì£¼ ì§§ì€ ê¼¬ë¦¬(<=4ì) ë‹¨ë…ì´ë©´ ì•ì— í•©ì¹¨
        merged = []
        for s in pieces:
            if merged and len(s) <= 4 and not END_STRONG_RE.search(merged[-1]):
                merged[-1] = (merged[-1] + ' ' + s).strip()
            else:
                merged.append(s)

        # ì‹œê°„ ë°°ë¶„ (ê¸€ì ë¹„ìœ¨)
        total_chars = sum(len(x) for x in merged) or 1
        dur_total   = max(0.01, e0 - s0)
        t_cursor    = s0
        for i, txt in enumerate(merged):
            if i == len(merged) - 1:
                t1 = e0
            else:
                frac = max(1, len(txt)) / total_chars
                t1   = t_cursor + dur_total * frac
            # ìµœì†Œ í‘œì‹œ ì‹œê°„ í™•ë³´ (ë‹¤ìŒ ì„¸ê·¸ë¨¼íŠ¸ ì¹¨ë²” ê¸ˆì§€)
            if (t1 - t_cursor) < min_dur:
                t1 = min(e0, t_cursor + min_dur)
            out.append({"start": round(t_cursor, 3), "end": round(t1, 3), "text": txt})
            t_cursor = t1

    return out

def _strip_last_punct_preserve_closers(s: str) -> str:
    # ëì´ [. , ! ? â€¦] ì´ê³  ê·¸ ë’¤ì—ëŠ” ê³µë°±/ë‹«ëŠ” ë”°ì˜´í‘œ/ê´„í˜¸ë§Œ ì˜¤ëŠ” ê²½ìš° ê·¸ êµ¬ë‘ì ë§Œ ì œê±°
    return re.sub(r'([.,!?â€¦])(?=\s*(?:["\'â€â€™)\]\}]|$))', '', s.strip())

# --- helper: ë‹¨ì–´ ë‹¨ìœ„ ë§ˆì´í¬ë¡œ ë¶„í•  ---
def _micro_split_by_words(piece: str, target_words: int = 3, min_tail_words: int = 2):
    # ê³µë°± ê¸°ì¤€ í† í°í™”(í† í°ì— ë¶™ì€ ì‰¼í‘œ ë“±ì€ ê·¸ëŒ€ë¡œ ìœ ì§€ â†’ ì¤‘ê°„ì˜ êµ¬ë‘ì ì€ ì‚´ë¦¼)
    tokens = re.findall(r'\S+', piece.strip())
    if not tokens:
        return []
    chunks = [" ".join(tokens[i:i+target_words]).strip()
              for i in range(0, len(tokens), target_words)]
    # ë§ˆì§€ë§‰ ë©ì–´ë¦¬ê°€ ë„ˆë¬´ ì§§ìœ¼ë©´ ì• ë©ì–´ë¦¬ë¡œ í•©ì¹˜ê¸°
    if len(chunks) >= 2 and len(chunks[-1].split()) < min_tail_words:
        chunks[-2] = (chunks[-2] + " " + chunks[-1]).strip()
        chunks.pop()
    return chunks

def _sentence_split_by_dot(text: str):
    """'.' ë’¤ì—ì„œ ë¬¸ì¥ ë¶„ë¦¬(ê³µë°± ë¬´ì‹œ). ë§ˆì¹¨í‘œê°€ ì—†ë‹¤ë©´ ì „ì²´ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ."""
    if not text or not text.strip():
        return []
    parts = re.split(r'(?<=\.)\s*', text.strip())
    # ë¹ˆ ì¡°ê° ì œê±° + ì›ë¬¸ ìœ ì§€
    return [p.strip() for p in parts if p and p.strip()]

def _split_tokens_into_n(tokens, n, prefer_punct=True):
    """
    í† í° ë¦¬ìŠ¤íŠ¸ë¥¼ nê°œë¡œ ê· í˜• ìˆê²Œ ìë¦„.
    prefer_punct=Trueë©´ ê²½ê³„ ê·¼ì²˜ì—ì„œ ì‰¼í‘œ/ì„¸ë¯¸ì½œë¡  ë“± ë’¤ë¥¼ ìš°ì„  ê²½ê³„ë¡œ ì„ íƒ.
    """
    if n <= 1 or len(tokens) <= n:
        return [" ".join(tokens).strip()]

    desired = [round(len(tokens) * i / n) for i in range(1, n)]
    boundaries = []
    for idx in desired:
        # ìë¥´ê¸° ì¢‹ì€ ê·¼ì²˜ í›„ë³´ ì¸ë±ìŠ¤(í† í° ì‚¬ì´ ê²½ê³„)
        cand = list(range(max(1, idx - 2), min(len(tokens) - 1, idx + 2) + 1))
        pick = None
        if prefer_punct:
            for j in cand:
                if re.search(r'[,:;Â·â€¦]$', tokens[j - 1]):
                    pick = j
                    break
        if pick is None:
            pick = min(cand, key=lambda j: abs(j - idx)) if cand else idx
        # ë‹¨ì¡° ì¦ê°€ ë³´ì¥
        if boundaries and pick <= boundaries[-1]:
            pick = boundaries[-1] + 1
        pick = min(max(1, pick), len(tokens) - 1)
        boundaries.append(pick)

    # ê²½ê³„ë¡œ ë¶„í• 
    out = []
    start = 0
    for b in boundaries + [len(tokens)]:
        out.append(" ".join(tokens[start:b]).strip())
        start = b
    return [x for x in out if x]


def _smooth_chunks_by_flow(pieces, target_words=3, min_words=2, max_words=5):
    """
    pieces: ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸(ì´ë¯¸ ì¡°ê°ë‚œ ìë§‰)
    - 1ë‹¨ì–´/ë„ˆë¬´ì§§ì€ ì¡°ê°ì€ ì•/ë’¤ì™€ í•©ì¹¨
    - ë‹´í™”í‘œì§€(ê·¸ë¦¬ê³ /í•˜ì§€ë§Œ/ê·¼ë°/ê·¸ëŸ°ë°/ê·¸ëŸ¬ë‹ˆê¹Œ/ë˜/ë˜í•œ/ê²Œë‹¤ê°€/í•œí¸/ë°˜ë©´ì—/ì¦‰)ëŠ” ë’¤ ì¡°ê°ì— ë¶™ì´ëŠ” ê±¸ ìš°ì„ 
    - ë„ˆë¬´ ê¸¸ì–´ì§„ ì¡°ê°ì€ ë‹¨ì–´ ê²½ê³„ + êµ¬ë‘ì  ë’¤ë¥¼ ì„ í˜¸í•´ ë‹¤ì‹œ ë‚˜ëˆ”
    """
    discourse_heads = {"ê·¸ë¦¬ê³ ","í•˜ì§€ë§Œ","ê·¼ë°","ê·¸ëŸ°ë°","ê·¸ëŸ¬ë‹ˆê¹Œ","ë˜","ë˜í•œ","ê²Œë‹¤ê°€","í•œí¸","ë°˜ë©´ì—","ì¦‰"}

    # 1) í† í°í™”
    toks = [re.findall(r"\S+", p.strip()) for p in pieces if p.strip()]
    if not toks:
        return []

    # 2) 1ë‹¨ì–´/ì§§ì€ ì¡°ê° ë³‘í•©
    i = 0
    while i < len(toks):
        wc = len(toks[i])
        if wc >= min_words or len(toks) == 1:
            i += 1
            continue

        first_tok = toks[i][0] if toks[i] else ""
        # ë‹´í™”í‘œì§€ë§Œ ë‹¨ë…ì´ë©´ ë‹¤ìŒê³¼ í•©ì¹˜ê¸° ìš°ì„ 
        if first_tok in discourse_heads and i + 1 < len(toks):
            toks[i + 1] = toks[i] + toks[i + 1]
            toks.pop(i)
            continue

        # ê·¸ ì™¸: ì• ì¡°ê°ì´ ëª©í‘œë³´ë‹¤ ì§§ìœ¼ë©´ ì•ê³¼ í•©ì¹˜ê¸°, ì•„ë‹ˆë©´ ë’¤ì™€ í•©ì¹˜ê¸°
        prev_ok = (i > 0 and len(toks[i - 1]) < target_words)
        if prev_ok:
            toks[i - 1] = toks[i - 1] + toks[i]
            toks.pop(i)
        elif i + 1 < len(toks):
            toks[i] = toks[i] + toks[i + 1]
            toks.pop(i + 1)
        else:
            i += 1  # ë§ˆì§€ë§‰ í•˜ë‚˜ ë‚¨ì€ ì˜ˆì™¸
    # 3) ë„ˆë¬´ ê¸´ ì¡°ê°ì€ ìì—°ìŠ¤ëŸ½ê²Œ ì¬ë¶„í• (êµ¬ë‘ì  ì„ í˜¸)
    refined = []
    for tt in toks:
        wc = len(tt)
        if wc > max_words:
            n = max(2, round(wc / target_words))
            refined.extend(_split_tokens_into_n(tt, n, prefer_punct=True))
        else:
            refined.append(" ".join(tt).strip())

    # 4) ê° ì¡°ê° ëì˜ ê¼¬ë¦¬ êµ¬ë‘ì  ì •ë¦¬(ë”°ì˜´í‘œ/ê´„í˜¸ ë³´ì¡´)
    refined = [_strip_last_punct_preserve_closers(x) for x in refined if x.strip()]
    return refined